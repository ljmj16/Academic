{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xHSx8l1tUYN"
   },
   "source": [
    "<a name=\"outline\"></a>\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [Task 1](#task-1): Regression\n",
    "  - [1.1](#q11) Linear regression\n",
    "  - [1.2](#q12) Ridge regression\n",
    "  - [1.3](#q13) Relaxation of Lasso regression\n",
    "- [Task 2](#task-2): Classification\n",
    "  - [2.1](#q21) kNN classifier\n",
    "  - [2.2](#q22) Random forest \n",
    "  - [2.3](#q23) Support vector machine (SVM) \n",
    "- [Task 3](#task-3): Mastery component \n",
    "  - [3.1](#q31) Logistic regression and bagging \n",
    "  - [3.2](#q32) Kernelised SVM classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1712,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I built this notebook in order so some functions in some exercises may use other functions defined in the previous exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3sSxTA3yzhN"
   },
   "source": [
    "<a name=\"task-1\"></a>\n",
    "# Task 1: Regression [^](#outline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gY8lFgnyzsF"
   },
   "source": [
    "<a name=\"q11\"></a>\n",
    "\n",
    "## 1.1: Linear regression  [^](#outline) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1\n",
    "We upload the data from $\\texttt{chemistry_samples.csv}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIC0</th>\n",
       "      <th>SM1_Dz(Z)</th>\n",
       "      <th>GATS1i</th>\n",
       "      <th>NdsCH</th>\n",
       "      <th>NdssC</th>\n",
       "      <th>MLOGP</th>\n",
       "      <th>FV1</th>\n",
       "      <th>VFV</th>\n",
       "      <th>FV2</th>\n",
       "      <th>FV3</th>\n",
       "      <th>LC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.661280</td>\n",
       "      <td>0.658363</td>\n",
       "      <td>1.602232</td>\n",
       "      <td>1.994272</td>\n",
       "      <td>0.836488</td>\n",
       "      <td>3.153623</td>\n",
       "      <td>15.893033</td>\n",
       "      <td>-27.724370</td>\n",
       "      <td>0.059355</td>\n",
       "      <td>0.756698</td>\n",
       "      <td>5.506249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.936362</td>\n",
       "      <td>1.154287</td>\n",
       "      <td>1.146997</td>\n",
       "      <td>0.904295</td>\n",
       "      <td>2.948308</td>\n",
       "      <td>5.141095</td>\n",
       "      <td>13.590177</td>\n",
       "      <td>-31.821521</td>\n",
       "      <td>-13.408855</td>\n",
       "      <td>1.161298</td>\n",
       "      <td>6.636791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.964144</td>\n",
       "      <td>0.415485</td>\n",
       "      <td>1.481028</td>\n",
       "      <td>2.136585</td>\n",
       "      <td>0.043679</td>\n",
       "      <td>-1.156783</td>\n",
       "      <td>15.989419</td>\n",
       "      <td>-3.699312</td>\n",
       "      <td>2.561525</td>\n",
       "      <td>0.500115</td>\n",
       "      <td>1.563388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.074617</td>\n",
       "      <td>1.417296</td>\n",
       "      <td>0.486216</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>-0.066980</td>\n",
       "      <td>2.610960</td>\n",
       "      <td>7.962046</td>\n",
       "      <td>-16.374439</td>\n",
       "      <td>2.448975</td>\n",
       "      <td>1.481888</td>\n",
       "      <td>6.248432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.448569</td>\n",
       "      <td>0.836892</td>\n",
       "      <td>1.951012</td>\n",
       "      <td>0.028318</td>\n",
       "      <td>-0.039121</td>\n",
       "      <td>1.851095</td>\n",
       "      <td>22.285266</td>\n",
       "      <td>-9.526361</td>\n",
       "      <td>2.870400</td>\n",
       "      <td>0.649234</td>\n",
       "      <td>3.676796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>2.762548</td>\n",
       "      <td>0.139194</td>\n",
       "      <td>1.630518</td>\n",
       "      <td>0.141917</td>\n",
       "      <td>0.360599</td>\n",
       "      <td>0.539647</td>\n",
       "      <td>18.861590</td>\n",
       "      <td>-21.369570</td>\n",
       "      <td>1.470872</td>\n",
       "      <td>0.277810</td>\n",
       "      <td>2.526064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>3.806224</td>\n",
       "      <td>0.546949</td>\n",
       "      <td>0.853502</td>\n",
       "      <td>0.045760</td>\n",
       "      <td>-0.045528</td>\n",
       "      <td>4.691096</td>\n",
       "      <td>11.109325</td>\n",
       "      <td>-26.873982</td>\n",
       "      <td>3.592650</td>\n",
       "      <td>0.649017</td>\n",
       "      <td>5.876928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>2.512367</td>\n",
       "      <td>0.324823</td>\n",
       "      <td>1.920923</td>\n",
       "      <td>0.125953</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>0.599234</td>\n",
       "      <td>18.817749</td>\n",
       "      <td>-18.689915</td>\n",
       "      <td>3.200544</td>\n",
       "      <td>0.431711</td>\n",
       "      <td>2.691485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4109</th>\n",
       "      <td>2.597895</td>\n",
       "      <td>0.335068</td>\n",
       "      <td>1.608763</td>\n",
       "      <td>0.107764</td>\n",
       "      <td>-0.206002</td>\n",
       "      <td>-0.393034</td>\n",
       "      <td>18.070758</td>\n",
       "      <td>-20.050566</td>\n",
       "      <td>4.586632</td>\n",
       "      <td>0.222328</td>\n",
       "      <td>2.096450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4110</th>\n",
       "      <td>3.053333</td>\n",
       "      <td>0.416593</td>\n",
       "      <td>1.646663</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.975002</td>\n",
       "      <td>1.013529</td>\n",
       "      <td>18.443987</td>\n",
       "      <td>-22.755477</td>\n",
       "      <td>-2.822340</td>\n",
       "      <td>0.548011</td>\n",
       "      <td>3.542785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4111 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CIC0  SM1_Dz(Z)    GATS1i     NdsCH     NdssC     MLOGP        FV1  \\\n",
       "0     3.661280   0.658363  1.602232  1.994272  0.836488  3.153623  15.893033   \n",
       "1     3.936362   1.154287  1.146997  0.904295  2.948308  5.141095  13.590177   \n",
       "2     0.964144   0.415485  1.481028  2.136585  0.043679 -1.156783  15.989419   \n",
       "3     2.074617   1.417296  0.486216  0.000908 -0.066980  2.610960   7.962046   \n",
       "4     1.448569   0.836892  1.951012  0.028318 -0.039121  1.851095  22.285266   \n",
       "...        ...        ...       ...       ...       ...       ...        ...   \n",
       "4106  2.762548   0.139194  1.630518  0.141917  0.360599  0.539647  18.861590   \n",
       "4107  3.806224   0.546949  0.853502  0.045760 -0.045528  4.691096  11.109325   \n",
       "4108  2.512367   0.324823  1.920923  0.125953  0.020476  0.599234  18.817749   \n",
       "4109  2.597895   0.335068  1.608763  0.107764 -0.206002 -0.393034  18.070758   \n",
       "4110  3.053333   0.416593  1.646663  0.001029  0.975002  1.013529  18.443987   \n",
       "\n",
       "            VFV        FV2       FV3      LC50  \n",
       "0    -27.724370   0.059355  0.756698  5.506249  \n",
       "1    -31.821521 -13.408855  1.161298  6.636791  \n",
       "2     -3.699312   2.561525  0.500115  1.563388  \n",
       "3    -16.374439   2.448975  1.481888  6.248432  \n",
       "4     -9.526361   2.870400  0.649234  3.676796  \n",
       "...         ...        ...       ...       ...  \n",
       "4106 -21.369570   1.470872  0.277810  2.526064  \n",
       "4107 -26.873982   3.592650  0.649017  5.876928  \n",
       "4108 -18.689915   3.200544  0.431711  2.691485  \n",
       "4109 -20.050566   4.586632  0.222328  2.096450  \n",
       "4110 -22.755477  -2.822340  0.548011  3.542785  \n",
       "\n",
       "[4111 rows x 11 columns]"
      ]
     },
     "execution_count": 1206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chemistry_samples = pd.read_csv(\"chemistry_samples.csv\") #reading from th csv file\n",
    "df_chemistry_samples #showing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposing that the data is already clean we can proceed looking for a linear regression model. We are looking for $\\boldsymbol \\beta$ such that we can built a model $y = \\boldsymbol X\\boldsymbol\\beta + \\epsilon$, where $\\boldsymbol X\\in\\mathbb{R}^{NxD} $ represents the inputs ($N$ is the number of rows in the dataset and $D=10$ is the number of features) and $y\\in\\mathbb{R}^N$ the noisy observations (corresponding to $\\texttt{LC50}$). The parameter vector $\\boldsymbol\\beta\\in\\mathbb{R}^D$ is used to parametrize the function. It is important to recall that the parameter vector $\\boldsymbol \\beta$ can be obtained solving a least-squares optimisation problem. In particular at the end we will have $\\boldsymbol \\beta = (\\boldsymbol X^T \\boldsymbol X)^{-1} \\boldsymbol X^T y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the matrices and the arrays from the data set\n",
    "X_1 = df_chemistry_samples.to_numpy()\n",
    "y = X_1[:,10] # creating the array y with the observation (LC50)\n",
    "N = len(y)\n",
    "X = X_1[:,:10] #creating a matrix storing the input data\n",
    "X_aug = np.hstack([1.0 * np.ones((len(y),1)), X]) #augmented training matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaobtainer(X_aug, y):\n",
    "    \"\"\"This function is used to obtain the parameters beta0, beta1, ...\n",
    "    Inputs:\n",
    "    \n",
    "    X_aug: numpy matrix with the input data\n",
    "    y: numpy array with the observed LC50\n",
    "    \n",
    "    Output:\n",
    "    beta: array of parameters of the model\"\"\"\n",
    "    \n",
    "    beta = np.linalg.solve(X_aug.T@X_aug,X_aug.T@y)\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we print the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta = [ 2.61638041e+00  4.47138333e-02  1.25871884e+00 -3.80092766e-02\n",
      "  3.63073448e-01  4.66534885e-03  3.90510052e-01 -7.46028629e-02\n",
      " -3.57069460e-02 -1.52588258e-02 -1.80315940e-03]\n"
     ]
    }
   ],
   "source": [
    "beta = betaobtainer(X_aug,y)\n",
    "print('beta =' ,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute the coefficient $R^2$ to determine how correct our model is. We remember that $R^2 = 1-\\frac{\\sum_i^N (y_i-(\\tilde y_i))^2}{\\sum_i^N (y_i-(\\hat y ))^2}$, where $\\hat y$ is the mean of the values of $y$, and $\\tilde y$, is the vector of the predicted $\\texttt{LC50}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I define a general R_square calcolator functions\n",
    "\n",
    "def R_square_calculator(X,y,beta):\n",
    "    '''Calculates the coefficient Rsquare'''\n",
    "    \n",
    "    N = len(y)\n",
    "    y_hat = np.mean(y) # computing the mean of the given y (vector of the values of ùôªùô≤ùüªùü∂)\n",
    "    # I will build an array from this in order to exploit numpy's functionalities\n",
    "    y_hat_vectorized = y_hat*np.ones(N) #N is the dimension of y, y_hat_vectorized is a vector with all values equal to the mean\n",
    "    y_tilde = X@beta #expected ùôªùô≤ùüªùü∂, for given beta\n",
    "    R_square = 1-np.linalg.norm(y-y_tilde)**2/np.linalg.norm(y-y_hat_vectorized)**2 #calculation of the coefficient R_square, using the formula \n",
    "\n",
    "    return R_square\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8718259975718016\n"
     ]
    }
   ],
   "source": [
    "#We calculare R_square score for the data set \n",
    "R_square = R_square_calculator(X_aug,y,beta)\n",
    "print(R_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2\n",
    "We upload from $\\texttt{chemistry_test.csv}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIC0</th>\n",
       "      <th>SM1_Dz(Z)</th>\n",
       "      <th>GATS1i</th>\n",
       "      <th>NdsCH</th>\n",
       "      <th>NdssC</th>\n",
       "      <th>MLOGP</th>\n",
       "      <th>FV1</th>\n",
       "      <th>VFV</th>\n",
       "      <th>FV2</th>\n",
       "      <th>FV3</th>\n",
       "      <th>LC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.617579</td>\n",
       "      <td>0.376167</td>\n",
       "      <td>1.224281</td>\n",
       "      <td>0.849464</td>\n",
       "      <td>1.101738</td>\n",
       "      <td>-0.448372</td>\n",
       "      <td>14.913614</td>\n",
       "      <td>-9.091450</td>\n",
       "      <td>-1.953849</td>\n",
       "      <td>0.328298</td>\n",
       "      <td>1.791786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.278766</td>\n",
       "      <td>0.514788</td>\n",
       "      <td>1.259734</td>\n",
       "      <td>0.210436</td>\n",
       "      <td>0.819626</td>\n",
       "      <td>4.446118</td>\n",
       "      <td>12.904817</td>\n",
       "      <td>-37.986185</td>\n",
       "      <td>-2.804426</td>\n",
       "      <td>0.452758</td>\n",
       "      <td>6.125609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.578652</td>\n",
       "      <td>0.221018</td>\n",
       "      <td>1.552583</td>\n",
       "      <td>1.007153</td>\n",
       "      <td>-0.013073</td>\n",
       "      <td>1.960720</td>\n",
       "      <td>17.393050</td>\n",
       "      <td>-27.188863</td>\n",
       "      <td>3.565159</td>\n",
       "      <td>0.341665</td>\n",
       "      <td>3.953270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.021762</td>\n",
       "      <td>1.602774</td>\n",
       "      <td>1.044233</td>\n",
       "      <td>0.054776</td>\n",
       "      <td>2.060890</td>\n",
       "      <td>4.510903</td>\n",
       "      <td>12.777434</td>\n",
       "      <td>-22.710306</td>\n",
       "      <td>-7.966119</td>\n",
       "      <td>1.729511</td>\n",
       "      <td>6.995314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.487163</td>\n",
       "      <td>0.799948</td>\n",
       "      <td>1.005727</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>0.146542</td>\n",
       "      <td>2.298082</td>\n",
       "      <td>13.336721</td>\n",
       "      <td>-16.839870</td>\n",
       "      <td>2.607198</td>\n",
       "      <td>0.904353</td>\n",
       "      <td>5.253633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>2.484857</td>\n",
       "      <td>0.281358</td>\n",
       "      <td>1.763974</td>\n",
       "      <td>0.051496</td>\n",
       "      <td>-0.247010</td>\n",
       "      <td>0.555638</td>\n",
       "      <td>20.134148</td>\n",
       "      <td>-15.019887</td>\n",
       "      <td>3.323621</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>2.693712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>1.732649</td>\n",
       "      <td>0.241483</td>\n",
       "      <td>1.332879</td>\n",
       "      <td>0.044737</td>\n",
       "      <td>-0.087352</td>\n",
       "      <td>0.478807</td>\n",
       "      <td>15.717319</td>\n",
       "      <td>-12.796126</td>\n",
       "      <td>2.762564</td>\n",
       "      <td>0.238815</td>\n",
       "      <td>2.690946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>3.478114</td>\n",
       "      <td>0.316327</td>\n",
       "      <td>1.196378</td>\n",
       "      <td>0.161062</td>\n",
       "      <td>0.021289</td>\n",
       "      <td>2.905319</td>\n",
       "      <td>13.947115</td>\n",
       "      <td>-29.829123</td>\n",
       "      <td>2.334150</td>\n",
       "      <td>0.157853</td>\n",
       "      <td>4.631151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>3.661736</td>\n",
       "      <td>0.388453</td>\n",
       "      <td>1.497032</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>0.246858</td>\n",
       "      <td>3.307575</td>\n",
       "      <td>14.560784</td>\n",
       "      <td>-29.526837</td>\n",
       "      <td>1.566413</td>\n",
       "      <td>0.474895</td>\n",
       "      <td>3.846075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>1.522468</td>\n",
       "      <td>0.066783</td>\n",
       "      <td>1.100577</td>\n",
       "      <td>0.146540</td>\n",
       "      <td>-0.259380</td>\n",
       "      <td>0.583446</td>\n",
       "      <td>12.796534</td>\n",
       "      <td>-11.206991</td>\n",
       "      <td>3.167980</td>\n",
       "      <td>0.299786</td>\n",
       "      <td>2.180131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1028 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CIC0  SM1_Dz(Z)    GATS1i     NdsCH     NdssC     MLOGP        FV1  \\\n",
       "0     1.617579   0.376167  1.224281  0.849464  1.101738 -0.448372  14.913614   \n",
       "1     4.278766   0.514788  1.259734  0.210436  0.819626  4.446118  12.904817   \n",
       "2     3.578652   0.221018  1.552583  1.007153 -0.013073  1.960720  17.393050   \n",
       "3     3.021762   1.602774  1.044233  0.054776  2.060890  4.510903  12.777434   \n",
       "4     2.487163   0.799948  1.005727  0.094923  0.146542  2.298082  13.336721   \n",
       "...        ...        ...       ...       ...       ...       ...        ...   \n",
       "1023  2.484857   0.281358  1.763974  0.051496 -0.247010  0.555638  20.134148   \n",
       "1024  1.732649   0.241483  1.332879  0.044737 -0.087352  0.478807  15.717319   \n",
       "1025  3.478114   0.316327  1.196378  0.161062  0.021289  2.905319  13.947115   \n",
       "1026  3.661736   0.388453  1.497032  0.018769  0.246858  3.307575  14.560784   \n",
       "1027  1.522468   0.066783  1.100577  0.146540 -0.259380  0.583446  12.796534   \n",
       "\n",
       "            VFV       FV2       FV3      LC50  \n",
       "0     -9.091450 -1.953849  0.328298  1.791786  \n",
       "1    -37.986185 -2.804426  0.452758  6.125609  \n",
       "2    -27.188863  3.565159  0.341665  3.953270  \n",
       "3    -22.710306 -7.966119  1.729511  6.995314  \n",
       "4    -16.839870  2.607198  0.904353  5.253633  \n",
       "...         ...       ...       ...       ...  \n",
       "1023 -15.019887  3.323621  0.207371  2.693712  \n",
       "1024 -12.796126  2.762564  0.238815  2.690946  \n",
       "1025 -29.829123  2.334150  0.157853  4.631151  \n",
       "1026 -29.526837  1.566413  0.474895  3.846075  \n",
       "1027 -11.206991  3.167980  0.299786  2.180131  \n",
       "\n",
       "[1028 rows x 11 columns]"
      ]
     },
     "execution_count": 1212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chemistry_test = pd.read_csv(\"chemistry_test.csv\") #reading from th csv file\n",
    "df_chemistry_test #showing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1364,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the matrices and the arrays from the testing data set \n",
    "X_test_1 = df_chemistry_test.to_numpy()\n",
    "y_test = X_test_1[:,10] # creating the array y with the observation (LC50)\n",
    "N_test = len(y_test)\n",
    "X_test = X_test_1[:,:10] #creating a matrix storing the input data\n",
    "X_aug_test = np.hstack([np.ones((len(y_test),1)), X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the predicted values from the model starting from the data of our test dataset. We will calculate the coefficient $R^2$, to analyse if the model works well with this testing dataset. (Note that we are going to use the beta obtained from exercise 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8642933369927281\n"
     ]
    }
   ],
   "source": [
    "#We compute the R_square score for the testing data set\n",
    "R_square_test = R_square_calculator(X_aug_test,y_test,beta)\n",
    "print(R_square_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the difference between the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference = 0.007532660579073425\n",
      "rel difference in percentage = 0.86\n"
     ]
    }
   ],
   "source": [
    "#difference\n",
    "diff_R_square_score = np.abs(R_square-R_square_test) \n",
    "#relative difference in percentage\n",
    "rel_diff_R_square_score = (diff_R_square_score/R_square)*100\n",
    "\n",
    "print ('difference =',diff_R_square_score)\n",
    "print ('rel difference in percentage =',round(rel_diff_R_square_score,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the $R^2$ score for the testing data is smaller than the one for the training data-set, but this is understandable since the model was trained on this data. Then we can see that our model used for the testing data set has a 'comparable accuracy' to the 'accuracy' that this model provides for the training data set. In particular the $R^2$ score is only $0.86\\%$ smaller for the training data set.\n",
    "We can say that the model we build can be used for predictions since it's not to 'overfitted' to the training data.\n",
    "\n",
    "However this analysis always depends from the requested precision. In fact what said before gives as a relationship between the two $R^2$ score, but in some real life cases we may need higher precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"q12\"></a>\n",
    "## 1.2: Ridge regression [^](#outline)\n",
    "\n",
    "### 1.2.1  \n",
    "Ridge regretion can be computed with the introduction of a parameter $\\lambda >0$, that we call 'penalty term', and then minimizing the loss function $L_{RIDGE}(\\beta)=||y-X\\beta||^2+\\lambda ||\\beta||^2$.\n",
    "The parameter $\\lambda$ has the duty to penalize high components of $\\beta$.\n",
    "\n",
    "First of all we code a function that computes $\\beta$ for the ridge regression for a given dataset and a given parameter $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_estimate(X, y, lamb):\n",
    "    \"\"\"Implements the ridge regression, with given parameter lambda and returns the parameter beta_ridge (which we need to build the regression model)\n",
    "    Inputs:\n",
    "    \n",
    "    X: NxD matrix containing the training inputs\n",
    "    y: Nx1 array containing the training observations\n",
    "    lamb:\n",
    "     \n",
    "    returns: beta_ridge\"\"\"\n",
    "    \n",
    "    N, D = X.shape\n",
    "    I = np.identity(D)\n",
    "    I[0,0] = 0\n",
    "    beta_ridge = np.linalg.solve(X.T@X+lamb*I,X.T@y) \n",
    "    return beta_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this point we will use 5-fold cross-validation in order to look for the hyper-parameter of the model. This approach consists in the division of the dataset in 5 sub-sets. Fixed one of this sub-sets, say $S_t$, we call it 'validation set', and the remaining 4 sub-sets are the training set. Now we build a model from the training set and use it to predict over the set $S_t$. We compute the errors (measuring them with $MSE_t = \\frac{1}{N}\\sum_{i\\in S_t}[f(x^{(i)})-y^{(i)}]^2$, where$f(x^{(i)})$ is the prediction we get). We repeat this process for all the 5 possible sub-sets (choosing each time one of them as validation set and the other ones as training set) and compute the average MSE.\n",
    "\n",
    "We can build the average MSE dependant from $\\lambda$ and use it to determine a good parameter $\\lambda$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we divide the indices in 5 sets, and write a function that computes the average MSE with given $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2038,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the matrices and the arrays from the data set\n",
    "X_1 = df_chemistry_samples.to_numpy()\n",
    "y = X_1[:,10] # creating the array y with the observation (LC50)\n",
    "N = len(y)\n",
    "X = X_1[:,:10] #creating a matrix storing the input data\n",
    "X_aug = np.hstack([np.ones((len(y),1)), X]) #augmented training matrix\n",
    "\n",
    "\n",
    "\n",
    "X_aug_ridge = 1.0*X_aug\n",
    "y_ridge = 1.0*y\n",
    "\n",
    "# Now we divide the indices in 5 sets\n",
    "folds_indexes = np.split(np.arange(len(y)-1), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to calculate the MSE for a given $\\beta$ and a function which implements cross validation and computes the average MSE over all the 5 validation-sets MSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSEcalculator(X_val,y_val,beta):\n",
    "    \"\"\"Calculates the MSE\"\"\"\n",
    "    N = len(y_val)\n",
    "    values = []\n",
    "    for i in range(N):\n",
    "        calc = (y_val[i]-(X_val@beta)[i])**2\n",
    "        values.append(calc)\n",
    "    MSE = sum(values)/N\n",
    "    return MSE\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def cross_validation_score(X, y, folds, lamb):\n",
    "   #Calculates the average MSE computing cross validation\n",
    "    #Inputs: \n",
    "\n",
    "    #X: matrix \n",
    "    #y: vector\n",
    "    #folds: folds with the indices\n",
    "    #lamb: given parameter\n",
    "    \n",
    "    #returns: MSE\n",
    "\n",
    "  scores = []\n",
    "  for i in range(5):  #part of the code for this loop is taken from the functions provided in the weekly coding tasks of this module\n",
    "    \n",
    "    val_indexes = folds[i]\n",
    "    train_indexes = list(set(range(y.shape[0]-1)) - set(val_indexes))\n",
    "    \n",
    "    #defining the training X and y\n",
    "    X_train_i = X[train_indexes, :]\n",
    "    y_train_i = y[train_indexes]\n",
    "\n",
    "    #defining the validation X and y\n",
    "    X_val_i = X[val_indexes, :] \n",
    "    y_val_i = y[val_indexes]\n",
    "    \n",
    "    beta_i = ridge_estimate(X_train_i,y_train_i,lamb=lamb) #calculating beta (with the ridge regression) for each one of the 5 training sets\n",
    "    \n",
    "    scores_i = MSEcalculator(X_val_i,y_val_i,beta_i) #calculating MSE \n",
    "    scores.append(scores_i)\n",
    "\n",
    "  # Computing the average MSE\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the behaviour of the average MSE for evolving parameter $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe02a095a10>]"
      ]
     },
     "execution_count": 1307,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEDCAYAAAAsr19QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAq20lEQVR4nO3dd3xV9f3H8dcngxF2IGEGwt47gCiK4kJwb6XiQC22ttqfWket1bZordVWi4s6ceKeKKKCqMgIAoGQAAEChGASViaZ9/P7I4c2TQO5gXvvuffm83w87iPnfs9653L43JPvWaKqGGOMCT0RbgcwxhhzdKyAG2NMiLICbowxIcoKuDHGhCgr4MYYE6KsgBtjTIgKeAEXkRdEJFdE1vtoed1F5AsRSRORDSKS6OV8A0TkBxEpE5HbjzDdqSLyo4isEZHvRKSP036H07ZGRNaLSJWIxDrjbnHaUkXk1hrLihWRhSKy2fnZrsa4u0UkQ0Q2isiZNdpHi8g6Z9wTIiJOe1MRmee0L6/5e4vI1c46NovI1TXaezrTbnbmbeK0i7PsDBFJEZFR3nyG9Xy+JzmfW6WIXHysyzPG1EFVA/oCTgJGAet9tLzFwOnOcEsgpo5pMutoiwfGALOA24+w/E3AQGf4F8BLdUxzDvC1MzwEWA/EAFHAl0BfZ9xfgbuc4buAh53hQcBaoCnQE9gCRDrjVgDjAQE+A86qkeUZZ/hyYJ4zHAtsdX62c4bbOePeAi53hp8BbnKGpzjLFuA4YLkP/l0SgWHAXODiQG9n9rJXY3gFfA9cVZcA+2q2iUhvEflcRFaJyLciMsCbZYnIICBKVRc6yy5S1RIvc+Sq6kqgor5JgdbOcBsgu45prgDecIYHAstUtURVK4FvgAuccecBLzvDLwPn12h/U1XLVHUbkAGMFZHOQGtV/UFVlepieH4dy3oHONXZOz8TWKiq+1R1P7AQmOyMm+RMW9f652q1ZUBbZ92IyM9EZIXzl8azIhJZz+dV/aGpZqpqCuDxZnpjTMMFSx/4HOBXqjoauB14ysv5+gEHROQ9EVktIo94W2Aa4HpgvohkAVcBf6k5UkRigMnAu07TeuAkEWnvjJsCJDjjOqrqbgDnZ7zT3hXYWWOxWU5bV2e4dvt/zeN8UeQD7Y+wrPbAAWfawy6r5jgRGQhcBpygqiOAKmBaXR+SMSbwotwOICItgeOBt53uXajuSkBELgT+WMdsu1T1TKrznwiMBHYA84BrgOdF5EngBGf6LiKyxhl+W1VnNSDib4ApqrpcRO4AHqO6qB9yDvC9qu4DUNU0EXmY6j3fIqq7Rio5MqmjTY/QfjTzHM2yTgVGAyudf5vmQC6AiMyluiustqdU1dsvYGPMMXC9gFP9V8ABZw/vv6jqe8B7R5g3C1itqlsBROQDqvtwn1fVXx6aSEQy61p+fUQkDhiuqsudpnnA57Umu5z/dJ8cyv088LyzjAf5z150joh0VtXdThdFbo3fI6HGIrpR3VWT5QzXbq85T5aIRFHdvbPPaT+51jyLgT1Ud41EOXvhdS2r9noEeFlV76792ajq9NptxpjAcr0LRVULgG0icgn8+4yI4V7OvhJo5xRaqO7j3eDDePuBNiLSz3l/OpB2aKSItAEmAh/WnElE4p2f3YEL+U+B/wg4dFbI1TXm+wi43DmzpCfQF1jhdLMUishxTh/29FrzHFrWxVQfRFVgAXCGiLRzznI5A1jgjFvkTFvX+qc7n/1xQL6z7q+Ai2v8PrEi0qMBn58xxp8CfdSU6mK2m+qDh1nADKrPvPic6u6GDcB9DVje6UAKsA54CWhSxzSZdbR1ctZfABxwhls74+YDXZzhC5xlr6V6T7ZXjWVcQ/XBx9rL/tb5PdYCp9Zob091Udzs/IytMe53VJ99shHnTBOnPYnqfvUtwGxAnPZmwNtUH/BcUSvXdU57BnBtjfZezrQZzrxNnXYBnnTWsQ5IqjHPZcAa5zNeBRzn5b/LGOczLQb2AqmB3tbsZa9wfx0qBsYYY0KM610oxhhjjk5AD2J26NBBExMTA7lKY4wJeatWrdqjqnG12wNawBMTE0lOTg7kKo0xJuSJyPa62q0LxRhjQpQVcGOMCVFWwI0xJkRZATfGmBBlBdwYY0KUFXBjjAlRVsCNMSZEWQE3xhg/Kq2o4v6PUsna79WzZhrECrgxxvjRq8u289LSTHbtP+jzZVsBN8YYPykuq+TpxVuY0KcD43q19/nyrYAbY4yfzP1hO3uLy/nN6f3qn/goWAE3xhg/KCyt4NklWzi5fxyje7TzyzqsgBtjjB+89H0mB0oq+M1p/tn7Bivgxhjjc/kHK/jXt1s5bWBHhie09dt6rIAbY4yPPfftVgpKK7n1tL5+XY8VcGOM8aE9RWU8/902pg7rzJCubfy6LivgxhjjQ08t2kJpRRX/56czT2qyAm6MMT6y68BBXl22nYtHd6N3XEu/r6/eAi4izURkhYisFZFUEXnAaf+TiKSIyBoR+UJEuvg9rTHGBLF/frUZgF+f6t++70O82QMvAyap6nBgBDBZRI4DHlHVYao6AvgEuM9vKY0xJshtzSvi7VVZXDmuO93axQRknfU+1FhVFShy3kY7L1XVghqTtQDU9/GMMSY0/P3LzTSJjOCXp/QJ2Dq96gMXkUgRWQPkAgtVdbnTPktEdgLTOMweuIjcKCLJIpKcl5fno9jGGBM8NmQX8PHabK6bkEhcq6YBW69XBVxVq5yukm7AWBEZ4rT/TlUTgNeAmw8z7xxVTVLVpLi4OB/FNsaY4PHoFxtp3SyKG0/sHdD1NugsFFU9ACwGJtca9TpwkW8iGWNM6Fi1fR9fpefy84m9aRMTHdB1e3MWSpyItHWGmwOnAekiUvMw67lAul8SGmNMkFJV/vr5Rjq0bMK1JyQGfP31HsQEOgMvi0gk1QX/LVX9RETeFZH+gAfYDsz0Y05jjAk632XsYfm2fdx/ziBimnhTTn3Lm7NQUoCRdbRbl4kxptFSVR5ZsJGubZtzxbjurmSwKzGNMeYoLEjNISUrn1tO60vTqEhXMlgBN8aYBqryKI9+sZFecS24cGRX13JYATfGmAZ6d1UWm3OLuP2M/kRFuldGrYAbY0wDHCyv4rGFmxiR0JazhnRyNYsVcGOMaYAXvt/GTwWl3H3WAETE1SxWwI0xxkv7ist5ZvEWThsYz7he7d2OYwXcGGO8NfvrDIrLK7lz8gC3owBWwI0xxis79pbwyrJMLk1KoG/HVm7HAayAG2OMV/72xUYiI4TfBOBRad6yAm6MMfVYl5XPR2uzmTGhJx1bN3M7zr9ZATfGmCNQVR76LI3YFk34+cTA3i62PlbAjTHmCL7ZlMfSLXv51aQ+tG4W2NvF1scKuDHGHEaVR/nLZ+l0j41h2rgebsf5H1bAjTHmMN5fvYv0nwq548z+NIkKvnIZfImMMSYIlFZU8dgXGxnWrQ1Th3Z2O06drIAbY0wdXlqaSXZ+KXedNYCICHcvmT8cK+DGGFPLnqIynvw6g0kD4jm+dwe34xyWFXBjjKnlH19uoqSiinumDHQ7yhFZATfGmBo25RTy+vId/Gxcd/rEt3Q7zhF581T6ZiKyQkTWikiqiDzgtD8iIukikiIi7x96cr0xxoSyWZ+m0bJpFLeeFjyXzB+ON3vgZcAkVR0OjAAmi8hxwEJgiKoOAzYBd/stpTHGBMDijbl8symPX5/al3Ytmrgdp171FnCtVuS8jXZeqqpfqGql074M6OanjMYY43eVVR5mfZpGYvsYpo9PdDuOV7zqAxeRSBFZA+QCC1V1ea1JrgM+O8y8N4pIsogk5+XlHVNYY4zxlzdW7mRzbhF3TxkYlBft1MWrlKpapaojqN7LHisiQw6NE5HfAZXAa4eZd46qJqlqUlxcnA8iG2OMb+UfrODvCzcxrmcsZwzq6HYcrzXoa0ZVDwCLgckAInI1cDYwTVXV1+GMMSYQnlqUwf6Scn5/9iDXn3PZEN6chRJ36AwTEWkOnAaki8hk4E7gXFUt8WtKY4zxkx17S3jx+0wuGtWNIV3buB2nQaK8mKYz8LKIRFJd8N9S1U9EJANoCix0vrGWqepM/0U1xhjfe3B+GlGRwh1n9nc7SoPVW8BVNQUYWUd7H78kMsaYAFmasYfPU3/ittP7BdWTdrwVGodajTHGxyqrPNz/cSoJsc254aRebsc5KlbAjTGN0qvLtrMpp4h7pw6iWXSk23GOihVwY0yjs6+4nMcWbmJCnw4hddpgbVbAjTGNzt++2EhxeRV/OCe0ThuszQq4MaZRWb8rnzdW7GD6+B707djK7TjHxAq4MabRUFUe+DiVdjFNQuJug/WxAm6MaTQ+WpvNysz9/PbM/rRpHu12nGNmBdwY0yiUlFfy0Px0hnRtzSVJCW7H8QlvrsQ0xpiQ99SiLfxUUMrsK0cSGaQPKW4o2wM3xoS9bXuKmbNkK+eP6EJSYqzbcXzGCrgxJqypKn/4KJWmURFB/5DihrICbowJa5+v/4klm/L4zen9iA/B+50ciRVwY0zYKimv5I+fbGBg59ZMH9/D7Tg+ZwXcGBO2nvgqg935pfzpvMFERYZfuQu/38gYY4CM3EKe+3YrF4/uFlYHLmuyAm6MCTuqyu8/SCWmSSR3nTXA7Th+YwXcGBN2PlqbzQ9b93LH5AF0aNnU7Th+YwXcGBNWCksrmPVpGkO7tuHKsd3djuNXdiWmMSas/OPLzeQVlTFnelLYXHF5OLYHbowJG+t35fPS0kwuH9OdEQlt3Y7jd/UWcBFpJiIrRGStiKSKyANO+yXOe4+IJPk/qjHGHF6VR7nn/XW0i4nmrsnhe+CyJm+6UMqASapaJCLRwHci8hmwHrgQeNafAY0xxhtzf8gkJSufxy8fQZuY0L9VrDfqLeCqqkCR8zbaeamqpgEh/TgiY0x4yD5wkL8t2MhJ/eI4d3gXt+MEjFd94CISKSJrgFxgoaou93YFInKjiCSLSHJeXt5RxjTGmMO7/6NUqlSZdf6QRrVT6VUBV9UqVR0BdAPGisgQb1egqnNUNUlVk+Li4o4ypjHG1G1B6k98sSGHW07tR0JsjNtxAqpBZ6Go6gFgMTDZH2GMMaYhCksr+MOHqQzo1IrrT+zpdpyA8+YslDgRaesMNwdOA9L9nMsYY+r16BebyCks5aELhxIdhjerqo83v3FnYJGIpAArqe4D/0RELhCRLGA88KmILPBnUGOMqWntzgO8/EMmVx3Xg5Hd27kdxxXenIWSAoyso/194H1/hDLGmCOpqPJw93vriGvZlNvP7O92HNfYpfTGmJAzZ8lWNuwu4JmfjaJ1s8ZxznddGl+nkTEmpGXkFvH4V5s5a0gnJg/p7HYcV1kBN8aEDI9HufPdFJpHR/LAeYPdjuM6K+DGmJAx94dMVm3fz31nDyK+VXg9oPhoWAE3xoSEnftK+OuCjUzsF8eFo7q6HScoWAE3xgQ9VeXu99YhwIMXDm1Ul8sfiRVwY0zQezs5i+8y9nDXWQPo2ra523GChhVwY0xQyyko5U+fbmBsz1imjevhdpygYgXcGBO0VJV7P1hPeaWHhy8aRkSYPyKtoayAG2OC1kdrs1m4IYf/O70fPTu0cDtO0LECbowJSjkFpdz3YSoju7fl+hN7uR0nKFkBN8YEnUNnnZRVVvHoJcPD/unyR8sKuDEm6LydnMXX6bncOXkAveJauh0naFkBN8YElaz9Jfzxkw0c1yuWq8cnuh0nqFkBN8YEDY9H+e07Kagqj1w83M46qYcVcGNM0Hh1+XaWbtnLvWcPanTPtzwaVsCNMUFh255iHpqfzsR+cVw+JsHtOCHBCrgxxnVVHuX2t9cSHSk8fNEwu9eJl+yJPMYY1z27ZAurtu/n75cNp1Mbu02st2wP3BjjqnVZ+Tz2xSamDu3M+SPsNrENUW8BF5FmIrJCRNaKSKqIPOC0x4rIQhHZ7PxsnI+FNsYctYPlVdwybzUdWjZl1gVDrOukgbzZAy8DJqnqcGAEMFlEjgPuAr5S1b7AV857Y4zx2p8/3cC2PcU8dulw2sY0cTtOyKm3gGu1IudttPNS4DzgZaf9ZeB8fwQ0xoSnLzfk8NryHdxwYi+O79PB7Tghyas+cBGJFJE1QC6wUFWXAx1VdTeA8zP+MPPeKCLJIpKcl5fno9jGmFCWV1jGne+mMKhza247o5/bcUKWVwVcVatUdQTQDRgrIkO8XYGqzlHVJFVNiouLO8qYxphwoar89p21FJVV8vjlI2gaFel2pJDVoLNQVPUAsBiYDOSISGcA52eur8MZY8LPK8u2s2hjHvdMGUjfjq3cjhPSvDkLJU5E2jrDzYHTgHTgI+BqZ7KrgQ/9lNEYEyY25xQy69M0Tu4fx/Tx9ni0Y+XNhTydgZdFJJLqgv+Wqn4iIj8Ab4nIDGAHcIkfcxpjQlxpRRU3v76alk2j+OvFdrWlL9RbwFU1BRhZR/te4FR/hDLGhJ8HPt7AxpxCXr5uLPGt7GpLX7ArMY0xfvdJSjZvrNjBzIm9mdjPTmbwFSvgxhi/2rG3hLvfXceo7m3tlEEfswJujPGb8koPN7/xIyLwxBUjiY60kuNLdjdCY4zf/PXzdFKy8nnmZ6Pp1s4e0OBr9nVojPGLr9NzeO67bVw9vgeTh3RyO05YsgJujPG53fkHue2ttQzq3Jq7pwx0O07YsgJujPGpiioPN7++mrJKD7OvHEmzaLtU3l+sD9wY41MPzU9n1fb9/POKkfSKa+l2nLBme+DGGJ/5JCWbF77fxrUnJHLO8C5uxwl7VsCNMT6RkVvIne+kMLpHO+4+y/q9A8EKuDHmmBWXVTLz1R9pFh3Jk1eOokmUlZZAsD5wY8wxUVXufm8dW/OKeGXGOHuqfADZ16Qx5pjM/WE7H63N5rYz+nOCPRotoKyAG2OO2qrt+/nzpxs4dUA8N03s7XacRscKuDHmqOQUlHLTq6vo1KYZj106gogIu793oFkBN8Y0WGlFFT9/ZRVFZZX8a3oSbWKi3Y7UKNlBTGNMg6gq936wnjU7D/D0tFEM6NTa7UiNlu2BG2Ma5KWlmbyzKotfT+rDWUM7ux2nUbMCbozx2vcZe/jzp2mcNrAjt55mD2dwmxVwY4xXdu4r4Zev/0ivDi34+2XD7aBlEKi3gItIgogsEpE0EUkVkVuc9uEi8oOIrBORj0XEOsKMCVPFZZXcMDcZj0f51/QkWjWzg5bBwJs98ErgNlUdCBwH/FJEBgHPAXep6lDgfeAO/8U0xrjF41Fue2stm3IK+eeVo0js0MLtSMZRbwFX1d2q+qMzXAikAV2B/sASZ7KFwEX+CmmMcc/DC9L5PPUn7pky0J4oH2Qa1AcuIonASGA5sB441xl1CZBwmHluFJFkEUnOy8s7hqjGmEB7c8UOnv1mK9PGdWfGhJ5uxzG1eF3ARaQl8C5wq6oWANdR3Z2yCmgFlNc1n6rOUdUkVU2Ki7Nvb2NCxfcZe7j3g/Wc2LcD9587GBE7aBlsvLqQR0SiqS7er6nqewCqmg6c4YzvB0z1V0hjTGBl5BYy89VV9IprwZPTRhEdaSesBSNvzkIR4HkgTVUfq9Ee7/yMAO4FnvFXSGNM4OwtKuPal1bSNCqCF64ZQ2s74yRoefO1egJwFTBJRNY4rynAFSKyCUgHsoEX/ZjTGBMApRVV3PjKKnILyvjX9CS6tYtxO5I5gnq7UFT1O+BwnV+P+zZO3ao8Sl5hmd0o3hg/8niUO95JYdX2/Tw1bRQju7dzO5KpR0h0bP3+w/Vc9PRScgtL3Y5iTNh6cH4aH6/N5s7JA5hi9zgJCSFRwC8fk8C+4nJmvJRMSXml23GMCTv/WrKV577bxjXHJzJzYi+34xgvhUQBH9atLbOvHElqdj6/fmM1VR51O5IxYeP91VnMmp/G1GGdue/sQXa6YAgJiQIOcOrAjjxw7mC+TMvlgY9TUbUibsyxWrIpjzveTmF8r/Y8dqndoCrUhNQDHa4an8jO/QeZs2QrCe1iuOEk+1PPmKOVknWAma+uom/HVjw7fTRNoyLdjmQaKKQKOMBdkweQtb+EWfPT6NquuR1sMeYoZO4p5toXVxLbogkvX2vneoeqkOlCOSQiQnjs0hGM6t6WW+etYdX2fW5HMiak5BSUMv2FFSgw97qxxLe203NDVcgVcIBm0ZE8d/UYurRpxvUvJ5ORW+R2JGNCwt6iMqY9t5y9RWW8cM0YesW1dDuSOQYhWcABYls04aVrxxIZEcFVzy9n14GDbkcyJqjlH6zgqudXkLW/hBeuGcOIhLZuRzLHKGQLOEBihxbMvW4sRWWVXPV89V6FMeZ/FZdVcs2LK9icW8izVyUxrld7tyMZHwjpAg4wqEtrXrhmDNkHDnL1iysoLK1wO5IxQaW0oorrX04mJSuff14xyh7KEEZCvoADjEmM5elpo0nfXcgNc5MprahyO5IxQaG80sNNr65i2ba9PHrJcCYP6eR2JONDYVHAAU4ZEM+jlw5n+bZ93Pz6aiqrPG5HMsZVlVUebp23mkUb83jwgqGcP7Kr25GMj4VNAQc4b0RX/njuYL5My+HOd9fhsUvuTSNVUeXhljfXMH/dT9w7dSBXjO3udiTjByF3IU99rhqfyP6SCh5buImm0RH8+bwhdnmwaVTKKz38+o3VfJ5aXbyvP9GuWA5XYVfAAX41qQ+lFVU8tXgLEQJ/Om+I3aDHNArllR5++fqPLNyQw+/PHmQPIg5zYVnARYQ7zuxPlSrPfrOVCBEesIeymjBXVlnFL1/7kS/Tcrn/nEFcc4IV73AXlgUcqov4XZMH4PEo//p2GxEi/OEcu1WmCU+lFVX84rUf+To9lz+dN5irxie6HckEQNgWcKgu4vdMGYhH4fnvthEZIdw7daAVcRNWSiuqmPnqKhZvzGPWBUOYNq6H25FMgIR1AYfqIn7v1IF4VHn+u21ECNwzxYq4CQ8FpRVc/3IyKzP38dCFQ+1sk0am3gIuIgnAXKAT4AHmqOrjIjICeAZoBlQCv1DVFX7MetREhPvOHvTv7pRKj9qTR0zI21NUxtUvrGDjT4X847IRnDfCzvNubLzZA68EblPVH0WkFbBKRBYCfwUeUNXPRGSK8/5k/0U9NiLC/ecOJjIighe+30ZJWRUPXjiUSDvF0ISgXQcOctVzy8nOP8i/pidxyoB4tyMZF9RbwFV1N7DbGS4UkTSgK6BAa2eyNkC2v0L6iojw+7MH0rJpJE98nUFJRRWPXTqc6Miwup7JhLmM3CKuen45RWWVvDJjHGMSY92OZFzSoD5wEUkERgLLgVuBBSLyN6qv6Dz+MPPcCNwI0L27+/1zIsL/ndGfmKZR/OWzdA6WVzH7ypE0i7bHSZngl5J1gGteXEmECPNuHM+gLq3rn8mELa93PUWkJfAucKuqFgA3Ab9R1QTgN8Dzdc2nqnNUNUlVk+LigucuaDMn9uZP51Vfdj/j5ZV2F0MT9L7bvIcr5iwjpkkk78y04m28LOAiEk118X5NVd9zmq8GDg2/DYz1fTz/ump8Io9eMpxlW/dx+Zxl5BXa/cRNcJq3cgfXvLiChNgY3pl5PIkdWrgdyQSBegu4VJ+q8TyQpqqP1RiVDUx0hicBm30fz/8uGt2N565OYmteMRc9vZTMPcVuRzLm3zwe5ZEF6dz57jqO79OBt2eOp1Mbe4alqebNHvgJwFXAJBFZ47ymADcAj4rIWuBBnH7uUHRK/3hev2EchaUVXPT0UlKyDrgdyRhKK6q4Zd4anly0hSvGJvD81Um0sqfHmxpENXC3XE1KStLk5OSAra+htuQVcfULK9hXXM5T00Zxcn87Ncu4Y39xOTe+kszKzP3cOXkAMyf2susWGjERWaWqSbXb7fy5GnrHteS9m44nsX0LrntpJXN/yHQ7kmmEtu0p5sKnl7I2K59/XjGSm07ubcXb1MkKeC3xrZvx9szxTBoQz30fpnL/R6n2dB8TMIs35nLe7O84UFLO69eP45zhXdyOZIKYFfA6tGgaxbNXJTFjQk9eWprJDXOTKSqrdDuWCWOqyrPfbOG6l1bSpW1zPrp5Akl2gY6phxXww4iMEH5/9iBmXTCEJZv3cNFTS9mxt8TtWCYMHSyv4pY31/DQZ+mcNaQz7/3ieBJiY9yOZUKAFfB6TBvXg5euHcPu/IOcM/s7lmzKczuSCSM795VwybNL+TglmzvO7M/sK0cS0yTsbxJqfMQKuBdO7BvHx7+aQOc2zbjmxRU8vXgLgTx7x4SnhRtymPrEt+zYW8Jz05P45Sl97GClaRAr4F7q0b4F7/3ieM4a2pmHP0/n5tdXU2z94uYoVFR5eGh+GjfMTaZ7+xg+/fWJnDqwo9uxTAiyv9UaIKZJFLOvGMmwrm14+PN0NuYU8tS0UfTr2MrtaCZE5BSU8qvXV7Micx/TxnXn92cPshupmaNme+ANJCL8fGJvXp0xjgMlFZw7+zveWrnTulRMvRZuyGHyP5awblc+j18+glkXDLXibY6JFfCjdHyfDsy/ZQKjurfjt++m8H9vrbUuFVOng+VV3PvBOm6Ym0yXts35+FcT7Ok5xiesC+UYxLdqxiszxjH76wwe/2oTa7MO8MTlIxnStY3b0UyQSNtdwK/fWM3m3CJuOLEnt5/Zn6ZRttdtfMP2wI9RZIRwy2l9ee364yguq+T8J7/nyUUZVHmsS6Uxq/Ioz3yzhfOe/J78gxW8MmMsv5s6yIq38Skr4D4yvnd7Ftx6EmcO6cQjCzZy6bM/2IU/jVRGbhEXPb2Uv3yWzqT+8Xx2y4mc2Dd4HmZiwocVcB9qG9OE2VeM5B+XjWBTTiFnPb6EN1bssAOcjUSVp/py+ClPfEvm3mKeuGIkT/9sFO1bNnU7mglT1gfuYyLC+SO7MqZnLLe/tZa731vHx2uzeejCofRob09RCVcbsgu45/11rNl5gDMGdeTPFwwhvpU9eMH4l90P3I88HuWNlTv4y/x0Kjwebju9P9eekEhUpP3hEy5Kyiv5x5ebef67bbRtHs195wzi3OFd7IpK41OHux+4FfAA2J1/kN9/sJ4v03IZ1q0ND14w1M5UCQNfpeVw34ep7DpwkMvHJHDXWQNoG9PE7VgmDFkBd5mq8knKbh74OJW9xeVMG9ed28/ob//hQ9DWvCJmfZrGV+m59OvYklkXDGWM3frV+NHhCrj1gQeIiHDO8C6c1C+Ovy/cxNwfMvk0ZTe/nTyAS5MSiIywP7mDXf7BCmZ/vZmXlmbSJDKCu84awHUn9KRJlHWJGXfUuwcuIgnAXKAT4AHmqOrjIjIP6O9M1hY4oKojjrSsxrwHXlva7gL+8GEqKzL3MbRrG+6ZMpDxvdu7HcvUobLKw7zknTz2xSb2lZRzyehu3H5mfztIaQLmqLtQRKQz0FlVfxSRVsAq4HxV3VBjmkeBfFX945GWZQX8v6kqH67J5uHP09mdX8qkAfHcOXkA/TvZzbGCgcejzF+/m0e/2MS2PcUk9WjHH84ZzNBudvzCBJbP+sBF5ENgtqoudN4LsAOYpKqbjzSvFfC6lVZU8eL3mTy1OIPiskouHt2NW0/rR5e2zd2O1iipKt9syuORBRtJzS6gX8eW3H5Gf04f1NHOLjGu8EkfuIgkAiOB5TWaTwRy6ive5vCaRUdy08m9uXxMAk8uymDuD9t5f/UuLk1K4Ben9KGrFfKAUFUWb8xj9qIMVm3fT7d2zXns0uGcN6KrHaMwQcnrPXARaQl8A8xS1fdqtD8NZKjqo4eZ70bgRoDu3buP3r59+zGHDndZ+0t4evEW3kreCcDFoxP4xcm97TmJfuLxKJ+n/sSTizJIzS6ga9vmzJzYi8vGdLcDlCYoHFMXiohEA58AC1T1sRrtUcAuYLSqZtW3HOtCaZjsAwd5evEW5q3ciUeVqcM6M2NCT4Z1a+t2tLBQUl7J+6t38cJ329iSV0zPDi246eTenD+iqxVuE1SO5SCmAC8D+1T11lrjJgN3q+pEb0JYAT86u/MP8ty325i3cidFZZWM7RnLjAk9OW1gR/vT/ihk7S/hlR+28+bKneQfrGBwl9b8fGJvpg7tbJ+nCUrHUsAnAN8C66g+jRDgHlWdLyIvActU9RlvQlgBPzaFpRXMW7mTF7/PZNeBg3Rr15zLkhK4dEwCHVvbKW1HUlnl4ZtNecxbuZMv03IAmDykE9ee0JOkHu3s4KQJanYlZhiprPKwIDWHV5dt54ete4mMEE7pH88VYxOY2C/O7rVSw7Y9xbyVvJN3V2WRW1hGh5ZNuGh0N6aPT7SDwyZk2JWYYSQqMoKpwzozdVhnMvcU8+bKnbyzKosv03KIbdGEKUM7cc6wLoxJjCWiEXYJ7DpwkPkpu/lk3W7W7jxAhMAp/eO5dEwCkwbEE21fcCZM2B54mKio8vB1ei4fr83my7QcSis8dGrdjClDO3PawHiSEmPD9sCcqrJ1TzGL0nP5dN1uVu84AMDQrm2YOqwzF4zsal1MJqRZF0ojUlxWyZdpOXy8Npslm/ZQXuWhZdMoTurXgUkDOjKhTwc6tQntglZSXsnybftYnJ7Loo157NhX/fSjgZ1bc/awzpw9rLPdf92EDSvgjVRxWSXfZ+zh6/Rcvk7PJbewDIDE9jGM69mecb1iGdszlq5tmwf1gbz8kgqSt+9jxbZ9LN+2j/W78qn0KM2iIzihdwdOHhDPyf3i7Fx5E5asgBtUldTsApZt3cuyrftYsW0vBaWVALRv0YTBXdswpEtrhnZtQ/9OrUiIjQl4f7HHo/xUUMrGnEI2ZBewflc+qdkF/97DbhIZwbBubRjbM5ZxvdozrmcszaLtQcEmvFkBN/+jyqOk/1TAqu37Wb8rn/W7CtiUU0ilp3qbiIwQEto1p2eHFiR2aEGn1s3o2LoZ8a2aEt+6KbEtmtKiaaTXT1qvrPJQXF7F/uJycgvLyC0sJa+wjJ8KSsncU0zmnhK27yumtMLz73l6tI9hcJfWDO7ShtE92jEioa0VbNPo2Fko5n9ERgiDu7RhcJf/3F2vtKKKjT8VkpFbxLY9xWzbW8y2vGKWb9tHSXlVncuJjhRaNI2iRZOof18Ic6g3psqjlJRXUVxWSVmlp875m0RGkBBb/UVxYt8OJHZoQZ/4lgzq0prWzaJ9+0sbE0asgJv/0iw6kuEJbRme0Pa/2lWVorLK6j3nguq9533F5ZSUV1FUVklxWSXFZVV4VKn5V12ECDFNI2nRJIoWTaOIaRJJu5gmxLduSnyrZsS1akrb5tGN8nRHY46VFXDjFRGhVbNoWjWLpndcS7fjGGOA8Dwx2BhjGgEr4MYYE6KsgBtjTIiyAm6MMSHKCrgxxoQoK+DGGBOirIAbY0yIsgJujDEhKqD3QhGRPOBoH0vfAdjjwzi+Eqy5IHizWa6GCdZcELzZwi1XD1WNq90Y0AJ+LEQkua6bubgtWHNB8GazXA0TrLkgeLM1llzWhWKMMSHKCrgxxoSoUCrgc9wOcBjBmguCN5vlaphgzQXBm61R5AqZPnBjjDH/LZT2wI0xxtRgBdwYY0JUUBRwEZksIhtFJENE7qpjvIjIE874FBEZ5e28fs41zcmTIiJLRWR4jXGZIrJORNaIiE8fBOpFrpNFJN9Z9xoRuc/bef2c644amdaLSJWIxDrj/Pl5vSAiuSKy/jDj3dq+6svlyvblZTa3trH6crm1jSWIyCIRSRORVBG5pY5pfL+dqfMILLdeQCSwBegFNAHWAoNqTTMF+AwQ4Dhgubfz+jnX8UA7Z/isQ7mc95lAB5c+r5OBT45mXn/mqjX9OcDX/v68nGWfBIwC1h9mfMC3Ly9zBXz7akC2gG9j3uRycRvrDIxyhlsBmwJRx4JhD3wskKGqW1W1HHgTOK/WNOcBc7XaMqCtiHT2cl6/5VLVpaq633m7DOjmo3UfUy4/zevrZV8BvOGjdR+Rqi4B9h1hEje2r3pzubR9HVp3fZ/Z4bj6mdUSyG1st6r+6AwXAmlA11qT+Xw7C4YC3hXYWeN9Fv/7ix9uGm/m9WeummZQ/e16iAJfiMgqEbnRR5kakmu8iKwVkc9EZHAD5/VnLkQkBpgMvFuj2V+flzfc2L4aKlDbV0MEehvzmpvbmIgkAiOB5bVG+Xw7C4aHGtf1OPLa5zYebhpv5j1aXi9bRE6h+j/YhBrNJ6hqtojEAwtFJN3ZewhErh+pvndCkYhMAT4A+no5rz9zHXIO8L2q1tyT8tfn5Q03ti+vBXj78pYb21hDuLKNiUhLqr80blXVgtqj65jlmLazYNgDzwISarzvBmR7OY038/ozFyIyDHgOOE9V9x5qV9Vs52cu8D7VfyYFJJeqFqhqkTM8H4gWkQ7ezOvPXDVcTq0/bf34eXnDje3LKy5sX15xaRtriIBvYyISTXXxfk1V36tjEt9vZ/7o0G9g538UsBXoyX868AfXmmYq/935v8Lbef2cqzuQARxfq70F0KrG8FJgcgBzdeI/F2mNBXY4n52rn5czXRuq+zBbBOLzqrGORA5/QC7g25eXuQK+fTUgW8C3MW9yubWNOb/7XOAfR5jG59uZT//Bj+GXn0L1UdstwO+ctpnAzBofzpPO+HVA0pHmDWCu54D9wBrnley093L+EdYCqS7kutlZ71qqD34df6R5A5XLeX8N8Gat+fz9eb0B7AYqqN7bmREk21d9uVzZvrzM5tY2dsRcLm5jE6ju9kip8e81xd/bmV1Kb4wxISoY+sCNMcYcBSvgxhgToqyAG2NMiLICbowxIcoKuDHGhCgr4MYYE6KsgBtjTIj6f3x6aW4kXLj2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting average MSE vs lambda\n",
    "l1 = [] #to store the values of the average MSE\n",
    "m1 = [] #to store the values of the parameter lambda \n",
    "for i in range(0,200):\n",
    "    lambda_val = 0.01*i\n",
    "    m1.append(lambda_val)\n",
    "    l1.append(cross_validation_score(X_aug_ridge, y_ridge, folds_indexes, lambda_val)) #calculating MSE and appending it to l\n",
    "plt.plot(m1,l1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for increaing $\\lambda$ the model gets worse and so for to small values of $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know compute the optimal $\\lambda$ minimizing the average MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_lambda(X, y, folds_indexes, lambda_range):   #part of the code for this loop is taken from the functions provided in the weekly coding tasks of this module\n",
    "    lambda_scores = np.zeros((len(lambda_range),))\n",
    "  \n",
    "    for i, lamb in enumerate(lambda_range):\n",
    "        lambda_scores[i] = cross_validation_score(X, y, folds_indexes, lamb)\n",
    "        print(f'lambda={lamb}: average MSE= {lambda_scores[i]:.7f}')\n",
    "\n",
    "    best_lambda_index = np.argmin(lambda_scores)  #We choose the optimal lambda\n",
    "    return lambda_range[best_lambda_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.0001: average MSE= 0.1879288\n",
      "lambda=0.00512462311557789: average MSE= 0.1879287\n",
      "lambda=0.01014924623115578: average MSE= 0.1879287\n",
      "lambda=0.015173869346733669: average MSE= 0.1879287\n",
      "lambda=0.02019849246231156: average MSE= 0.1879286\n",
      "lambda=0.02522311557788945: average MSE= 0.1879286\n",
      "lambda=0.03024773869346734: average MSE= 0.1879286\n",
      "lambda=0.035272361809045236: average MSE= 0.1879286\n",
      "lambda=0.04029698492462312: average MSE= 0.1879285\n",
      "lambda=0.04532160804020101: average MSE= 0.1879285\n",
      "lambda=0.0503462311557789: average MSE= 0.1879285\n",
      "lambda=0.055370854271356795: average MSE= 0.1879284\n",
      "lambda=0.06039547738693468: average MSE= 0.1879284\n",
      "lambda=0.06542010050251257: average MSE= 0.1879284\n",
      "lambda=0.07044472361809047: average MSE= 0.1879283\n",
      "lambda=0.07546934673366835: average MSE= 0.1879283\n",
      "lambda=0.08049396984924624: average MSE= 0.1879283\n",
      "lambda=0.08551859296482413: average MSE= 0.1879283\n",
      "lambda=0.09054321608040201: average MSE= 0.1879282\n",
      "lambda=0.09556783919597991: average MSE= 0.1879282\n",
      "lambda=0.1005924623115578: average MSE= 0.1879282\n",
      "lambda=0.10561708542713569: average MSE= 0.1879282\n",
      "lambda=0.11064170854271359: average MSE= 0.1879281\n",
      "lambda=0.11566633165829147: average MSE= 0.1879281\n",
      "lambda=0.12069095477386936: average MSE= 0.1879281\n",
      "lambda=0.12571557788944723: average MSE= 0.1879280\n",
      "lambda=0.13074020100502512: average MSE= 0.1879280\n",
      "lambda=0.135764824120603: average MSE= 0.1879280\n",
      "lambda=0.14078944723618092: average MSE= 0.1879280\n",
      "lambda=0.1458140703517588: average MSE= 0.1879279\n",
      "lambda=0.1508386934673367: average MSE= 0.1879279\n",
      "lambda=0.15586331658291458: average MSE= 0.1879279\n",
      "lambda=0.16088793969849247: average MSE= 0.1879279\n",
      "lambda=0.16591256281407035: average MSE= 0.1879279\n",
      "lambda=0.17093718592964824: average MSE= 0.1879278\n",
      "lambda=0.17596180904522613: average MSE= 0.1879278\n",
      "lambda=0.180986432160804: average MSE= 0.1879278\n",
      "lambda=0.18601105527638193: average MSE= 0.1879278\n",
      "lambda=0.1910356783919598: average MSE= 0.1879277\n",
      "lambda=0.1960603015075377: average MSE= 0.1879277\n",
      "lambda=0.20108492462311559: average MSE= 0.1879277\n",
      "lambda=0.20610954773869347: average MSE= 0.1879277\n",
      "lambda=0.21113417085427136: average MSE= 0.1879276\n",
      "lambda=0.21615879396984924: average MSE= 0.1879276\n",
      "lambda=0.22118341708542716: average MSE= 0.1879276\n",
      "lambda=0.22620804020100505: average MSE= 0.1879276\n",
      "lambda=0.23123266331658293: average MSE= 0.1879276\n",
      "lambda=0.23625728643216082: average MSE= 0.1879275\n",
      "lambda=0.2412819095477387: average MSE= 0.1879275\n",
      "lambda=0.2463065326633166: average MSE= 0.1879275\n",
      "lambda=0.2513311557788945: average MSE= 0.1879275\n",
      "lambda=0.25635577889447236: average MSE= 0.1879275\n",
      "lambda=0.26138040201005025: average MSE= 0.1879274\n",
      "lambda=0.26640502512562814: average MSE= 0.1879274\n",
      "lambda=0.271429648241206: average MSE= 0.1879274\n",
      "lambda=0.2764542713567839: average MSE= 0.1879274\n",
      "lambda=0.28147889447236185: average MSE= 0.1879274\n",
      "lambda=0.28650351758793974: average MSE= 0.1879273\n",
      "lambda=0.2915281407035176: average MSE= 0.1879273\n",
      "lambda=0.2965527638190955: average MSE= 0.1879273\n",
      "lambda=0.3015773869346734: average MSE= 0.1879273\n",
      "lambda=0.3066020100502513: average MSE= 0.1879273\n",
      "lambda=0.31162663316582917: average MSE= 0.1879273\n",
      "lambda=0.31665125628140706: average MSE= 0.1879272\n",
      "lambda=0.32167587939698494: average MSE= 0.1879272\n",
      "lambda=0.32670050251256283: average MSE= 0.1879272\n",
      "lambda=0.3317251256281407: average MSE= 0.1879272\n",
      "lambda=0.3367497487437186: average MSE= 0.1879272\n",
      "lambda=0.3417743718592965: average MSE= 0.1879272\n",
      "lambda=0.3467989949748744: average MSE= 0.1879271\n",
      "lambda=0.35182361809045226: average MSE= 0.1879271\n",
      "lambda=0.35684824120603015: average MSE= 0.1879271\n",
      "lambda=0.36187286432160803: average MSE= 0.1879271\n",
      "lambda=0.366897487437186: average MSE= 0.1879271\n",
      "lambda=0.37192211055276386: average MSE= 0.1879271\n",
      "lambda=0.37694673366834175: average MSE= 0.1879270\n",
      "lambda=0.38197135678391964: average MSE= 0.1879270\n",
      "lambda=0.3869959798994975: average MSE= 0.1879270\n",
      "lambda=0.3920206030150754: average MSE= 0.1879270\n",
      "lambda=0.3970452261306533: average MSE= 0.1879270\n",
      "lambda=0.4020698492462312: average MSE= 0.1879270\n",
      "lambda=0.40709447236180907: average MSE= 0.1879270\n",
      "lambda=0.41211909547738695: average MSE= 0.1879269\n",
      "lambda=0.41714371859296484: average MSE= 0.1879269\n",
      "lambda=0.4221683417085427: average MSE= 0.1879269\n",
      "lambda=0.4271929648241206: average MSE= 0.1879269\n",
      "lambda=0.4322175879396985: average MSE= 0.1879269\n",
      "lambda=0.4372422110552764: average MSE= 0.1879269\n",
      "lambda=0.44226683417085433: average MSE= 0.1879269\n",
      "lambda=0.4472914572864322: average MSE= 0.1879269\n",
      "lambda=0.4523160804020101: average MSE= 0.1879269\n",
      "lambda=0.457340703517588: average MSE= 0.1879268\n",
      "lambda=0.4623653266331659: average MSE= 0.1879268\n",
      "lambda=0.46738994974874376: average MSE= 0.1879268\n",
      "lambda=0.47241457286432165: average MSE= 0.1879268\n",
      "lambda=0.47743919597989953: average MSE= 0.1879268\n",
      "lambda=0.4824638190954774: average MSE= 0.1879268\n",
      "lambda=0.4874884422110553: average MSE= 0.1879268\n",
      "lambda=0.4925130653266332: average MSE= 0.1879268\n",
      "lambda=0.4975376884422111: average MSE= 0.1879268\n",
      "lambda=0.502562311557789: average MSE= 0.1879268\n",
      "lambda=0.5075869346733669: average MSE= 0.1879267\n",
      "lambda=0.5126115577889447: average MSE= 0.1879267\n",
      "lambda=0.5176361809045227: average MSE= 0.1879267\n",
      "lambda=0.5226608040201005: average MSE= 0.1879267\n",
      "lambda=0.5276854271356785: average MSE= 0.1879267\n",
      "lambda=0.5327100502512563: average MSE= 0.1879267\n",
      "lambda=0.5377346733668342: average MSE= 0.1879267\n",
      "lambda=0.5427592964824121: average MSE= 0.1879267\n",
      "lambda=0.54778391959799: average MSE= 0.1879267\n",
      "lambda=0.5528085427135678: average MSE= 0.1879267\n",
      "lambda=0.5578331658291458: average MSE= 0.1879267\n",
      "lambda=0.5628577889447237: average MSE= 0.1879267\n",
      "lambda=0.5678824120603015: average MSE= 0.1879267\n",
      "lambda=0.5729070351758795: average MSE= 0.1879267\n",
      "lambda=0.5779316582914573: average MSE= 0.1879266\n",
      "lambda=0.5829562814070353: average MSE= 0.1879266\n",
      "lambda=0.5879809045226131: average MSE= 0.1879266\n",
      "lambda=0.593005527638191: average MSE= 0.1879266\n",
      "lambda=0.5980301507537689: average MSE= 0.1879266\n",
      "lambda=0.6030547738693468: average MSE= 0.1879266\n",
      "lambda=0.6080793969849246: average MSE= 0.1879266\n",
      "lambda=0.6131040201005026: average MSE= 0.1879266\n",
      "lambda=0.6181286432160804: average MSE= 0.1879266\n",
      "lambda=0.6231532663316584: average MSE= 0.1879266\n",
      "lambda=0.6281778894472362: average MSE= 0.1879266\n",
      "lambda=0.6332025125628141: average MSE= 0.1879266\n",
      "lambda=0.638227135678392: average MSE= 0.1879266\n",
      "lambda=0.6432517587939699: average MSE= 0.1879266\n",
      "lambda=0.6482763819095478: average MSE= 0.1879266\n",
      "lambda=0.6533010050251257: average MSE= 0.1879266\n",
      "lambda=0.6583256281407036: average MSE= 0.1879266\n",
      "lambda=0.6633502512562814: average MSE= 0.1879266\n",
      "lambda=0.6683748743718594: average MSE= 0.1879266\n",
      "lambda=0.6733994974874372: average MSE= 0.1879266\n",
      "lambda=0.6784241206030152: average MSE= 0.1879266\n",
      "lambda=0.683448743718593: average MSE= 0.1879266\n",
      "lambda=0.6884733668341709: average MSE= 0.1879266\n",
      "lambda=0.6934979899497488: average MSE= 0.1879266\n",
      "lambda=0.6985226130653267: average MSE= 0.1879266\n",
      "lambda=0.7035472361809045: average MSE= 0.1879266\n",
      "lambda=0.7085718592964825: average MSE= 0.1879266\n",
      "lambda=0.7135964824120603: average MSE= 0.1879266\n",
      "lambda=0.7186211055276382: average MSE= 0.1879266\n",
      "lambda=0.7236457286432161: average MSE= 0.1879266\n",
      "lambda=0.728670351758794: average MSE= 0.1879266\n",
      "lambda=0.733694974874372: average MSE= 0.1879266\n",
      "lambda=0.7387195979899498: average MSE= 0.1879266\n",
      "lambda=0.7437442211055277: average MSE= 0.1879266\n",
      "lambda=0.7487688442211056: average MSE= 0.1879266\n",
      "lambda=0.7537934673366835: average MSE= 0.1879266\n",
      "lambda=0.7588180904522613: average MSE= 0.1879266\n",
      "lambda=0.7638427135678393: average MSE= 0.1879266\n",
      "lambda=0.7688673366834171: average MSE= 0.1879266\n",
      "lambda=0.773891959798995: average MSE= 0.1879266\n",
      "lambda=0.7789165829145729: average MSE= 0.1879266\n",
      "lambda=0.7839412060301508: average MSE= 0.1879266\n",
      "lambda=0.7889658291457287: average MSE= 0.1879266\n",
      "lambda=0.7939904522613066: average MSE= 0.1879266\n",
      "lambda=0.7990150753768844: average MSE= 0.1879266\n",
      "lambda=0.8040396984924624: average MSE= 0.1879266\n",
      "lambda=0.8090643216080403: average MSE= 0.1879266\n",
      "lambda=0.8140889447236181: average MSE= 0.1879266\n",
      "lambda=0.8191135678391961: average MSE= 0.1879266\n",
      "lambda=0.8241381909547739: average MSE= 0.1879266\n",
      "lambda=0.8291628140703519: average MSE= 0.1879266\n",
      "lambda=0.8341874371859297: average MSE= 0.1879266\n",
      "lambda=0.8392120603015076: average MSE= 0.1879267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.8442366834170855: average MSE= 0.1879267\n",
      "lambda=0.8492613065326634: average MSE= 0.1879267\n",
      "lambda=0.8542859296482412: average MSE= 0.1879267\n",
      "lambda=0.8593105527638192: average MSE= 0.1879267\n",
      "lambda=0.864335175879397: average MSE= 0.1879267\n",
      "lambda=0.869359798994975: average MSE= 0.1879267\n",
      "lambda=0.8743844221105528: average MSE= 0.1879267\n",
      "lambda=0.8794090452261307: average MSE= 0.1879267\n",
      "lambda=0.8844336683417087: average MSE= 0.1879267\n",
      "lambda=0.8894582914572865: average MSE= 0.1879267\n",
      "lambda=0.8944829145728644: average MSE= 0.1879267\n",
      "lambda=0.8995075376884423: average MSE= 0.1879267\n",
      "lambda=0.9045321608040202: average MSE= 0.1879267\n",
      "lambda=0.909556783919598: average MSE= 0.1879268\n",
      "lambda=0.914581407035176: average MSE= 0.1879268\n",
      "lambda=0.9196060301507538: average MSE= 0.1879268\n",
      "lambda=0.9246306532663318: average MSE= 0.1879268\n",
      "lambda=0.9296552763819096: average MSE= 0.1879268\n",
      "lambda=0.9346798994974875: average MSE= 0.1879268\n",
      "lambda=0.9397045226130654: average MSE= 0.1879268\n",
      "lambda=0.9447291457286433: average MSE= 0.1879268\n",
      "lambda=0.9497537688442211: average MSE= 0.1879268\n",
      "lambda=0.9547783919597991: average MSE= 0.1879268\n",
      "lambda=0.9598030150753769: average MSE= 0.1879269\n",
      "lambda=0.9648276381909549: average MSE= 0.1879269\n",
      "lambda=0.9698522613065328: average MSE= 0.1879269\n",
      "lambda=0.9748768844221106: average MSE= 0.1879269\n",
      "lambda=0.9799015075376886: average MSE= 0.1879269\n",
      "lambda=0.9849261306532664: average MSE= 0.1879269\n",
      "lambda=0.9899507537688443: average MSE= 0.1879269\n",
      "lambda=0.9949753768844222: average MSE= 0.1879269\n",
      "lambda=1.0: average MSE= 0.1879269\n",
      "best_lambda: 0.7035472361809045\n"
     ]
    }
   ],
   "source": [
    "lambda_scores = np.linspace(0.0001,1,200)\n",
    "\n",
    "best_lambda = choose_best_lambda(X_aug_ridge, y_ridge, folds_indexes, lambda_scores)\n",
    "\n",
    "print('best_lambda:', best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18792657379824249"
      ]
     },
     "execution_count": 1310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average MSE for lambda=0.06\n",
    "cross_validation_score(X_aug_ridge, y_ridge, folds_indexes, 0.7035472361809045)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our optimal optimal parameter is $\\lambda = 0.7035472361809045$ and for this value our average MSE is equal to $0.18792657379824249$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will analyse the bias and the variance. We know that $MSE = Variance+Bias^2$. In particular we will have for incrasing $\\lambda$ greater bias and lower variance. So the aim of the research of the hyperparameter is to find the right compromise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2\n",
    "We now assume the parameter to be the optimal one, so $\\lambda = 0.7035472361809045$. Then we compute $\\beta$ for this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal beta = [ 2.61820782  0.04433538  1.25232768 -0.03780222  0.36279709  0.00495263\n",
      "  0.39065421 -0.0746366  -0.03570966 -0.01524782  0.00318129]\n"
     ]
    }
   ],
   "source": [
    "#beta for ridge regression with optimal parameter lambda\n",
    "beta_optimal_ridge = ridge_estimate(X_aug_ridge,y_ridge,lamb = 0.7035472361809045)\n",
    "print('optimal beta =', beta_optimal_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can compute the parameter $R^2$ for the set $\\texttt{chemistry_samples.csv}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8718250505208601\n"
     ]
    }
   ],
   "source": [
    "R_square_ridge = R_square_calculator(X_aug,y,beta_optimal_ridge)\n",
    "R_square_ridge\n",
    "print(R_square_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the parameter $R^2$ for the set of $\\texttt{chemistry_test.csv}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8642792450696695\n"
     ]
    }
   ],
   "source": [
    "R_square_ridge_test = R_square_calculator(X_aug_test,y_test,beta_optimal_ridge)\n",
    "print(R_square_ridge_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the 'relative' difference in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative difference = 0.8655183108907591\n"
     ]
    }
   ],
   "source": [
    "#difference training-test R_squares\n",
    "diff_122_tt = R_square_ridge-R_square_ridge_test\n",
    "diff_122_percent_rel_tt = diff_122_tt/R_square_ridge*100\n",
    "print ('relative difference =', diff_122_percent_rel_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that that like for the linear regression the $R^2$ score is better for the training set (since the model has been built on it), however the model can be generalised, as we can see from the fact that also for the test set we get a good score. In particular for the testing data set the $R^2$ score drops by $0.86\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the difference between this model and the linear regression of the previous excercise. We compute the difference between the $R^2$ score for the testing data with the previous model, and the score we obtain using the ridge-regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative difference -0.0016304560564716544\n"
     ]
    }
   ],
   "source": [
    "diff_122 = R_square_ridge_test-R_square_test\n",
    "diff_122_percent_rel = diff_122/R_square_test*100\n",
    "print ('relative difference', diff_122_percent_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Ridge Regression is slightly worse, but still the difference is not so 'heavy', so both approachs provide a good model for the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9OTrzYly0Oz"
   },
   "source": [
    "<a name=\"q13\"></a>\n",
    "\n",
    "## 1.3: Relaxation of Lasso Regression [^](#outline)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1\n",
    "We are considering a smooth version of Lasso, which uses the Huber functions $L_c$. The cost function can be written as $L(\\beta) = ||y-X\\beta||^2+\\lambda \\sum_i^p L_c(\\beta_i)$, where the $\\lambda$ is the penalty hyper parameter and p the number of the predictors we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2\n",
    "We import the data set and we shuffle, then we also define the folds indexes, that we will use for the 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "metadata": {
    "id": "JV3UJe2A0IH-"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1344,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chemistry_samples = pd.read_csv(\"chemistry_samples.csv\") #reading from th csv file\n",
    "#Obtaining the matrices and the arrays from the data set\n",
    "X_1 = df_chemistry_samples.to_numpy()\n",
    "y = X_1[:,10] # creating the array y with the observation (LC50)\n",
    "N = len(y)\n",
    "X = X_1[:,:10] #creating a matrix storing the input data\n",
    "X_aug = np.hstack([np.ones((len(y),1)), X]) #augmented training matrix\n",
    "\n",
    "\n",
    "\n",
    "#I also shuffle my data \n",
    "p_lasso = np.random.permutation(len(y))\n",
    "X_aug_lasso = 1.0*X_aug\n",
    "y_lasso = 1.0*y\n",
    "\n",
    "# Now we divide the indices in 5 sets\n",
    "folds_indexes = np.split(np.arange(len(y)-1), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function computing the huber function. We know that $L_c(\\beta) = \\begin{cases} \n",
    "      \\frac{1}{2} \\beta^2  & |\\beta|\\leq c \\\\\n",
    "     c(|\\beta|-\\frac{1}{2}c) & otherwise\n",
    "   \\end{cases}\n",
    "$. The function we implement works in particular for arrays (so each component will be $L_c(\\beta_i)$. Note that in the cost function for Lasso we had the sum $\\sum_i^p L_c(\\beta_i)$, where each $\\beta_i$ is a component of $\\beta$. \n",
    "Now we can see that if we do the gradient we get $\\nabla (\\sum_i^p L_c(\\beta_i))= \\nabla (L_c(\\beta_1)+L_c(\\beta_2)+...+L_c(\\beta_p)) = v $, where $v$ is a p-dimensional array where each component $i$ is $\\frac{d L_c(\\beta_i)}{d \\beta_i} = \\begin{cases} \n",
    "       \\beta_i  & |\\beta_i|\\leq c \\\\\n",
    "     c\\frac{\\beta_i}{|\\beta_i|} & otherwise\n",
    "   \\end{cases}$. \n",
    "   \n",
    "We define then a function $\\texttt{grad_huber}$, which computes this gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1345,
   "metadata": {
    "id": "jg2TWPoT0J0Z"
   },
   "outputs": [],
   "source": [
    "#Uber functions\n",
    "def huber(beta, c=1e-6):\n",
    "    \n",
    "    p = len(beta)\n",
    "    huber = np.zeros(p)\n",
    "    for j in range(p): #runs over each component\n",
    "        if np.abs(beta[j])<=c:     #for this condition we return 1/2*beta^2\n",
    "             huber[j] = 0.5*beta[j]**2 \n",
    "        else:\n",
    "              huber[j] = c*(np.abs(beta[j])-0.5*c)  #Otherwise I return this\n",
    "    return huber\n",
    "\n",
    "#gradient of Huber\n",
    "\n",
    "def grad_huber(beta, c=1e-6):\n",
    "    #Given a beta p - dimensional array and c, this functions computes the gradient of \n",
    "    # the gradient of SUM from i to p of Lc(beta_i)\n",
    "    p = len(beta)\n",
    "    grad_huber = np.zeros(p)\n",
    "    for j in range(p): #runs over each component\n",
    "        if np.abs(beta[j])<=c:   #first case\n",
    "            grad_huber[j] = beta[j]\n",
    "        else:     #second possible case\n",
    "            k = beta[j]\n",
    "            l = np.abs(beta[j])\n",
    "            grad_huber[j] = c*(k/l)\n",
    "    return grad_huber #return the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that $\n",
    "\\frac{\\partial ||y-X\\beta||^2}{\\partial \\beta}=2X^TX\\beta-2X^Ty$, so we can compute $\\nabla L_{lasso}(\\beta) = 2X^TX\\beta-2X^Ty +\\lambda v$, with $v$ defined as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement a functions which runs the gradient descent. We give as a max number of iterations 10000 and stepsize $10^{-3}$, and return the beta obtained with the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1346,
   "metadata": {
    "id": "U7hMrAxK0LQN"
   },
   "outputs": [],
   "source": [
    "def minimize_ls_huber(X, y, lambd, n_iters=100000, step_size=1e-3):\n",
    "    n, p = X.shape\n",
    "    XX = X.T @ X /n\n",
    "    Xy = X.T @ y /n\n",
    "    \n",
    "    # next line: initialise betas\n",
    "    beta = np.array([ 2.59176109e+00,  5.24173069e-02,  1.25879616e+00, -4.75743650e-02,\n",
    "         3.63434515e-01,  5.35322170e-03,  3.90598346e-01, -7.30193241e-02,\n",
    "        -3.51297259e-02, -1.49767687e-02,  1.65132173e-03])  #we start from a point we already now being a good one, to get better results\n",
    "    Loss = 0\n",
    "    \n",
    "    # gradient descent \n",
    "    for i in range(n_iters):\n",
    "        grad = 2*XX@beta-2*Xy+lambd*grad_huber(beta,c=1e-6)\n",
    "        # next line: gradient descent update\n",
    "        beta = beta-step_size*grad\n",
    "        Loss = np.linalg.norm(y-X@beta)**2+lambd*np.sum(huber(beta,c = 1e-6))\n",
    "        #print(np.linalg.norm(grad))\n",
    "        \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the 5-cross validation for the Lasso-Huber with a function, which we will call $\\texttt{cross_validation_score_lasso}$. (We proceed similarly as for the ridge-regression part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSEcalculator_Lasso(X_val,y_val,beta):\n",
    "    \"\"\"Calculates the MSE\"\"\"\n",
    "    N = len(y_val)\n",
    "    values = []\n",
    "    for i in range(N):\n",
    "        calc = (y_val[i]-(X_val@beta)[i])**2\n",
    "        values.append(calc)\n",
    "    MSE = sum(values)/N\n",
    "    return MSE\n",
    "        \n",
    "\n",
    "def cross_validation_score_lasso(X, y, folds, lambd):\n",
    "   #Calculates the average MSE computing cross validation\n",
    "    #Inputs: \n",
    "\n",
    "    #X: matrix \n",
    "    #y: vector\n",
    "    #folds: folds with the indices\n",
    "    #lamb: given parameter\n",
    "    \n",
    "    #returns: MSE\n",
    "\n",
    "  scores = []\n",
    "  for i in range(5):  #part of the code for this loop is taken from the functions provided in the weekly coding tasks of this module\n",
    "    \n",
    "    val_indexes = folds[i]\n",
    "    train_indexes = list(set(range(y.shape[0]-1)) - set(val_indexes))\n",
    "    \n",
    "    #defining the training X and y\n",
    "    X_train_i = X[train_indexes, :]\n",
    "    y_train_i = y[train_indexes]\n",
    "\n",
    "    #defining the validation X and y\n",
    "    X_val_i = X[val_indexes, :] \n",
    "    y_val_i = y[val_indexes]\n",
    "    \n",
    "    beta_i = minimize_ls_huber(X_train_i, y_train_i, lambd, n_iters=10000, step_size=1e-3) #calculating beta for each one of the 5 training sets\n",
    "    \n",
    "    scores_i = np.linalg.norm(y_val_i-(X_val_i@beta_i))**2/len(y_val_i) #calculating MSE \n",
    "    scores.append(scores_i)\n",
    "\n",
    "  # Computing the average MSE\n",
    "  return sum(scores) / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose the best parameter $\\lambda$ minimizing the average MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.0: average MSE= 0.1875454918240246\n",
      "lambda=8.695652173913043: average MSE= 0.1875454843660000\n",
      "lambda=17.391304347826086: average MSE= 0.1875454756133821\n",
      "lambda=26.08695652173913: average MSE= 0.1875454657024281\n",
      "lambda=34.78260869565217: average MSE= 0.1875454547071871\n",
      "lambda=43.47826086956522: average MSE= 0.1875454427464378\n",
      "lambda=52.17391304347826: average MSE= 0.1875454298587919\n",
      "lambda=60.8695652173913: average MSE= 0.1875454161538284\n",
      "lambda=69.56521739130434: average MSE= 0.1875454016866974\n",
      "lambda=78.26086956521739: average MSE= 0.1875453865051617\n",
      "lambda=86.95652173913044: average MSE= 0.1875453706933509\n",
      "lambda=95.65217391304347: average MSE= 0.1875453542828553\n",
      "lambda=104.34782608695652: average MSE= 0.1875453372671916\n",
      "lambda=113.04347826086956: average MSE= 0.1875453198089238\n",
      "lambda=121.7391304347826: average MSE= 0.1875453018402916\n",
      "lambda=130.43478260869566: average MSE= 0.1875452834327340\n",
      "lambda=139.1304347826087: average MSE= 0.1875452646405085\n",
      "lambda=147.82608695652172: average MSE= 0.1875452454975147\n",
      "lambda=156.52173913043478: average MSE= 0.1875452260351219\n",
      "lambda=165.2173913043478: average MSE= 0.1875452062802972\n",
      "lambda=173.91304347826087: average MSE= 0.1875451861917836\n",
      "lambda=182.6086956521739: average MSE= 0.1875451659138602\n",
      "lambda=191.30434782608694: average MSE= 0.1875451453555362\n",
      "lambda=200.0: average MSE= 0.1875451246549557\n",
      "lambda=208.69565217391303: average MSE= 0.1875451037512310\n",
      "lambda=217.39130434782606: average MSE= 0.1875450826649569\n",
      "lambda=226.08695652173913: average MSE= 0.1875450614431885\n",
      "lambda=234.78260869565216: average MSE= 0.1875450401024296\n",
      "lambda=243.4782608695652: average MSE= 0.1875450186378964\n",
      "lambda=252.17391304347825: average MSE= 0.1875449970513177\n",
      "lambda=260.8695652173913: average MSE= 0.1875449754201444\n",
      "lambda=269.5652173913043: average MSE= 0.1875449537082031\n",
      "lambda=278.2608695652174: average MSE= 0.1875449319304878\n",
      "lambda=286.95652173913044: average MSE= 0.1875449100937411\n",
      "lambda=295.65217391304344: average MSE= 0.1875448882595120\n",
      "lambda=304.3478260869565: average MSE= 0.1875448663718974\n",
      "lambda=313.04347826086956: average MSE= 0.1875448444549964\n",
      "lambda=321.73913043478257: average MSE= 0.1875448225351163\n",
      "lambda=330.4347826086956: average MSE= 0.1875448006197216\n",
      "lambda=339.1304347826087: average MSE= 0.1875447787156657\n",
      "lambda=347.82608695652175: average MSE= 0.1875447568166963\n",
      "lambda=356.52173913043475: average MSE= 0.1875447349129061\n",
      "lambda=365.2173913043478: average MSE= 0.1875447130648246\n",
      "lambda=373.9130434782609: average MSE= 0.1875446912568521\n",
      "lambda=382.6086956521739: average MSE= 0.1875446694482445\n",
      "lambda=391.30434782608694: average MSE= 0.1875446477244686\n",
      "lambda=400.0: average MSE= 0.1875446260168571\n",
      "lambda=408.695652173913: average MSE= 0.1875446043574840\n",
      "lambda=417.39130434782606: average MSE= 0.1875445827897748\n",
      "lambda=426.0869565217391: average MSE= 0.1875445612619747\n",
      "lambda=434.78260869565213: average MSE= 0.1875445397895751\n",
      "lambda=443.4782608695652: average MSE= 0.1875445183876621\n",
      "lambda=452.17391304347825: average MSE= 0.1875444970589272\n",
      "lambda=460.86956521739125: average MSE= 0.1875444758058213\n",
      "lambda=469.5652173913043: average MSE= 0.1875444671455259\n",
      "lambda=478.2608695652174: average MSE= 0.1875444590365403\n",
      "lambda=486.9565217391304: average MSE= 0.1875444502483272\n",
      "lambda=495.65217391304344: average MSE= 0.1875444411393685\n",
      "lambda=504.3478260869565: average MSE= 0.1875444318929101\n",
      "lambda=513.0434782608695: average MSE= 0.1875444226260863\n",
      "lambda=521.7391304347826: average MSE= 0.1875444134094902\n",
      "lambda=530.4347826086956: average MSE= 0.1875444042855021\n",
      "lambda=539.1304347826086: average MSE= 0.1875443952850128\n",
      "lambda=547.8260869565217: average MSE= 0.1875443864309278\n",
      "lambda=556.5217391304348: average MSE= 0.1875443777390288\n",
      "lambda=565.2173913043478: average MSE= 0.1875443692160803\n",
      "lambda=573.9130434782609: average MSE= 0.1875443608731048\n",
      "lambda=582.6086956521739: average MSE= 0.1875443527129746\n",
      "lambda=591.3043478260869: average MSE= 0.1875443447393123\n",
      "lambda=600.0: average MSE= 0.1875443369543440\n",
      "lambda=608.695652173913: average MSE= 0.1875443293579401\n",
      "lambda=617.391304347826: average MSE= 0.1875443219479346\n",
      "lambda=626.0869565217391: average MSE= 0.1875443147230899\n",
      "lambda=634.7826086956521: average MSE= 0.1875443076822116\n",
      "lambda=643.4782608695651: average MSE= 0.1875443008241272\n",
      "lambda=652.1739130434783: average MSE= 0.1875442941476855\n",
      "lambda=660.8695652173913: average MSE= 0.1875442876517567\n",
      "lambda=669.5652173913043: average MSE= 0.1875442813352314\n",
      "lambda=678.2608695652174: average MSE= 0.1875442751970206\n",
      "lambda=686.9565217391304: average MSE= 0.1875442692364977\n",
      "lambda=695.6521739130435: average MSE= 0.1875442634526915\n",
      "lambda=704.3478260869565: average MSE= 0.1875442578442024\n",
      "lambda=713.0434782608695: average MSE= 0.1875442524100145\n",
      "lambda=721.7391304347826: average MSE= 0.1875442471496049\n",
      "lambda=730.4347826086956: average MSE= 0.1875442420626100\n",
      "lambda=739.1304347826086: average MSE= 0.1875442371471176\n",
      "lambda=747.8260869565217: average MSE= 0.1875442324030439\n",
      "lambda=756.5217391304348: average MSE= 0.1875442278297150\n",
      "lambda=765.2173913043478: average MSE= 0.1875442234254726\n",
      "lambda=773.9130434782609: average MSE= 0.1875442191911788\n",
      "lambda=782.6086956521739: average MSE= 0.1875442151242288\n",
      "lambda=791.3043478260869: average MSE= 0.1875442112259376\n",
      "lambda=800.0: average MSE= 0.1875442074938161\n",
      "lambda=808.695652173913: average MSE= 0.1875442039285891\n",
      "lambda=817.391304347826: average MSE= 0.1875442005289040\n",
      "lambda=826.0869565217391: average MSE= 0.1875441972938930\n",
      "lambda=834.7826086956521: average MSE= 0.1875441942243227\n",
      "lambda=843.4782608695651: average MSE= 0.1875441913182232\n",
      "lambda=852.1739130434783: average MSE= 0.1875441885751535\n",
      "lambda=860.8695652173913: average MSE= 0.1875441859958316\n",
      "lambda=869.5652173913043: average MSE= 0.1875441835786595\n",
      "lambda=878.2608695652174: average MSE= 0.1875441813231186\n",
      "lambda=886.9565217391304: average MSE= 0.1875441792287902\n",
      "lambda=895.6521739130434: average MSE= 0.1875441772956178\n",
      "lambda=904.3478260869565: average MSE= 0.1875441755231958\n",
      "lambda=913.0434782608695: average MSE= 0.1875441739106611\n",
      "lambda=921.7391304347825: average MSE= 0.1875441724576168\n",
      "lambda=930.4347826086956: average MSE= 0.1875441711636713\n",
      "lambda=939.1304347826086: average MSE= 0.1875441700284375\n",
      "lambda=947.8260869565216: average MSE= 0.1875441690515330\n",
      "lambda=956.5217391304348: average MSE= 0.1875441682325803\n",
      "lambda=965.2173913043478: average MSE= 0.1875441675712063\n",
      "lambda=973.9130434782608: average MSE= 0.1875441670670426\n",
      "lambda=982.6086956521739: average MSE= 0.1875441667197253\n",
      "lambda=991.3043478260869: average MSE= 0.1875441665288949\n",
      "lambda=1000.0: average MSE= 0.1875441664941963\n",
      "lambda=1008.695652173913: average MSE= 0.1875441666152789\n",
      "lambda=1017.391304347826: average MSE= 0.1875441668917962\n",
      "lambda=1026.086956521739: average MSE= 0.1875441673234061\n",
      "lambda=1034.782608695652: average MSE= 0.1875441679097709\n",
      "lambda=1043.4782608695652: average MSE= 0.1875441686507944\n",
      "lambda=1052.1739130434783: average MSE= 0.1875441695464787\n",
      "lambda=1060.8695652173913: average MSE= 0.1875441705960063\n",
      "lambda=1069.5652173913043: average MSE= 0.1875441717990543\n",
      "lambda=1078.2608695652173: average MSE= 0.1875441731553041\n",
      "lambda=1086.9565217391303: average MSE= 0.1875441746647785\n",
      "lambda=1095.6521739130435: average MSE= 0.1875441763276768\n",
      "lambda=1104.3478260869565: average MSE= 0.1875441781429155\n",
      "lambda=1113.0434782608695: average MSE= 0.1875441801101903\n",
      "lambda=1121.7391304347825: average MSE= 0.1875441822300057\n",
      "lambda=1130.4347826086955: average MSE= 0.1875441845019242\n",
      "lambda=1139.1304347826087: average MSE= 0.1875441869250553\n",
      "lambda=1147.8260869565217: average MSE= 0.1875441894998183\n",
      "lambda=1156.5217391304348: average MSE= 0.1875441922261797\n",
      "lambda=1165.2173913043478: average MSE= 0.1875441951029566\n",
      "lambda=1173.9130434782608: average MSE= 0.1875441981310823\n",
      "lambda=1182.6086956521738: average MSE= 0.1875442013097199\n",
      "lambda=1191.304347826087: average MSE= 0.1875442046382994\n",
      "lambda=1200.0: average MSE= 0.1875442081179504\n",
      "lambda=1208.695652173913: average MSE= 0.1875442117467659\n",
      "lambda=1217.391304347826: average MSE= 0.1875442155262279\n",
      "lambda=1226.086956521739: average MSE= 0.1875442194547554\n",
      "lambda=1234.782608695652: average MSE= 0.1875442235331576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=1243.4782608695652: average MSE= 0.1875442277605922\n",
      "lambda=1252.1739130434783: average MSE= 0.1875442321373713\n",
      "lambda=1260.8695652173913: average MSE= 0.1875442366629232\n",
      "lambda=1269.5652173913043: average MSE= 0.1875442413375257\n",
      "lambda=1278.2608695652173: average MSE= 0.1875442461604207\n",
      "lambda=1286.9565217391303: average MSE= 0.1875442511323027\n",
      "lambda=1295.6521739130435: average MSE= 0.1875442562516430\n",
      "lambda=1304.3478260869565: average MSE= 0.1875442615204090\n",
      "lambda=1313.0434782608695: average MSE= 0.1875442669362117\n",
      "lambda=1321.7391304347825: average MSE= 0.1875442725005752\n",
      "lambda=1330.4347826086955: average MSE= 0.1875442782123914\n",
      "lambda=1339.1304347826085: average MSE= 0.1875442840713931\n",
      "lambda=1347.8260869565217: average MSE= 0.1875442900788736\n",
      "lambda=1356.5217391304348: average MSE= 0.1875442962328776\n",
      "lambda=1365.2173913043478: average MSE= 0.1875443025344085\n",
      "lambda=1373.9130434782608: average MSE= 0.1875443089832491\n",
      "lambda=1382.6086956521738: average MSE= 0.1875443155784638\n",
      "lambda=1391.304347826087: average MSE= 0.1875443223209750\n",
      "lambda=1400.0: average MSE= 0.1875443292103592\n",
      "lambda=1408.695652173913: average MSE= 0.1875443362458308\n",
      "lambda=1417.391304347826: average MSE= 0.1875443434277821\n",
      "lambda=1426.086956521739: average MSE= 0.1875443507569445\n",
      "lambda=1434.782608695652: average MSE= 0.1875443582317420\n",
      "lambda=1443.4782608695652: average MSE= 0.1875443658518747\n",
      "lambda=1452.1739130434783: average MSE= 0.1875443736197429\n",
      "lambda=1460.8695652173913: average MSE= 0.1875443815330421\n",
      "lambda=1469.5652173913043: average MSE= 0.1875443895918091\n",
      "lambda=1478.2608695652173: average MSE= 0.1875443977955409\n",
      "lambda=1486.9565217391303: average MSE= 0.1875444061464295\n",
      "lambda=1495.6521739130435: average MSE= 0.1875444146427035\n",
      "lambda=1504.3478260869565: average MSE= 0.1875444232839862\n",
      "lambda=1513.0434782608695: average MSE= 0.1875444320704346\n",
      "lambda=1521.7391304347825: average MSE= 0.1875444410013272\n",
      "lambda=1530.4347826086955: average MSE= 0.1875444500792449\n",
      "lambda=1539.1304347826085: average MSE= 0.1875444593019045\n",
      "lambda=1547.8260869565217: average MSE= 0.1875444686691525\n",
      "lambda=1556.5217391304348: average MSE= 0.1875444781811938\n",
      "lambda=1565.2173913043478: average MSE= 0.1875444878378020\n",
      "lambda=1573.9130434782608: average MSE= 0.1875444976385205\n",
      "lambda=1582.6086956521738: average MSE= 0.1875445075855798\n",
      "lambda=1591.3043478260868: average MSE= 0.1875445176771274\n",
      "lambda=1600.0: average MSE= 0.1875445279128951\n",
      "lambda=1608.695652173913: average MSE= 0.1875445382928875\n",
      "lambda=1617.391304347826: average MSE= 0.1875445488171877\n",
      "lambda=1626.086956521739: average MSE= 0.1875445594857331\n",
      "lambda=1634.782608695652: average MSE= 0.1875445702984199\n",
      "lambda=1643.478260869565: average MSE= 0.1875445812548376\n",
      "lambda=1652.1739130434783: average MSE= 0.1875445923553488\n",
      "lambda=1660.8695652173913: average MSE= 0.1875446036003854\n",
      "lambda=1669.5652173913043: average MSE= 0.1875446149897436\n",
      "lambda=1678.2608695652173: average MSE= 0.1875446265229310\n",
      "lambda=1686.9565217391303: average MSE= 0.1875446381998870\n",
      "lambda=1695.6521739130435: average MSE= 0.1875446500205521\n",
      "lambda=1704.3478260869565: average MSE= 0.1875446619848667\n",
      "lambda=1713.0434782608695: average MSE= 0.1875446740927717\n",
      "lambda=1721.7391304347825: average MSE= 0.1875446863441353\n",
      "lambda=1730.4347826086955: average MSE= 0.1875446987389792\n",
      "lambda=1739.1304347826085: average MSE= 0.1875447112772597\n",
      "lambda=1747.8260869565217: average MSE= 0.1875447239589198\n",
      "lambda=1756.5217391304348: average MSE= 0.1875447367839032\n",
      "lambda=1765.2173913043478: average MSE= 0.1875447497521532\n",
      "lambda=1773.9130434782608: average MSE= 0.1875447628636143\n",
      "lambda=1782.6086956521738: average MSE= 0.1875447761182309\n",
      "lambda=1791.3043478260868: average MSE= 0.1875447895159235\n",
      "lambda=1800.0: average MSE= 0.1875448030565659\n",
      "lambda=1808.695652173913: average MSE= 0.1875448167401736\n",
      "lambda=1817.391304347826: average MSE= 0.1875448305666917\n",
      "lambda=1826.086956521739: average MSE= 0.1875448445360661\n",
      "lambda=1834.782608695652: average MSE= 0.1875448586482428\n",
      "lambda=1843.478260869565: average MSE= 0.1875448729031683\n",
      "lambda=1852.1739130434783: average MSE= 0.1875448873008826\n",
      "lambda=1860.8695652173913: average MSE= 0.1875449018421340\n",
      "lambda=1869.5652173913043: average MSE= 0.1875449165260597\n",
      "lambda=1878.2608695652173: average MSE= 0.1875449313526073\n",
      "lambda=1886.9565217391303: average MSE= 0.1875449463216067\n",
      "lambda=1895.6521739130433: average MSE= 0.1875449614328049\n",
      "lambda=1904.3478260869565: average MSE= 0.1875449766864444\n",
      "lambda=1913.0434782608695: average MSE= 0.1875449920824745\n",
      "lambda=1921.7391304347825: average MSE= 0.1875450076206358\n",
      "lambda=1930.4347826086955: average MSE= 0.1875450233007768\n",
      "lambda=1939.1304347826085: average MSE= 0.1875450391231251\n",
      "lambda=1947.8260869565215: average MSE= 0.1875450550886083\n",
      "lambda=1956.5217391304348: average MSE= 0.1875450711970937\n",
      "lambda=1965.2173913043478: average MSE= 0.1875450874476856\n",
      "lambda=1973.9130434782608: average MSE= 0.1875451038397419\n",
      "lambda=1982.6086956521738: average MSE= 0.1875451203739119\n",
      "lambda=1991.3043478260868: average MSE= 0.1875451370269390\n",
      "lambda=2000.0: average MSE= 0.1875451521326071\n",
      "lambda=2008.695652173913: average MSE= 0.1875451724924446\n",
      "lambda=2017.391304347826: average MSE= 0.1875451896101943\n",
      "lambda=2026.086956521739: average MSE= 0.1875452032766918\n",
      "lambda=2034.782608695652: average MSE= 0.1875452206134963\n",
      "lambda=2043.478260869565: average MSE= 0.1875452415211266\n",
      "lambda=2052.173913043478: average MSE= 0.1875452597289019\n",
      "lambda=2060.869565217391: average MSE= 0.1875452734766052\n",
      "lambda=2069.565217391304: average MSE= 0.1875452919038924\n",
      "lambda=2078.2608695652175: average MSE= 0.1875453136643195\n",
      "lambda=2086.9565217391305: average MSE= 0.1875453310109374\n",
      "lambda=2095.6521739130435: average MSE= 0.1875453503881815\n",
      "lambda=2104.3478260869565: average MSE= 0.1875453643830684\n",
      "lambda=2113.0434782608695: average MSE= 0.1875453838529508\n",
      "lambda=2121.7391304347825: average MSE= 0.1875454063898593\n",
      "lambda=2130.4347826086955: average MSE= 0.1875454240678898\n",
      "lambda=2139.1304347826085: average MSE= 0.1875454445405251\n",
      "lambda=2147.8260869565215: average MSE= 0.1875454588460405\n",
      "lambda=2156.5217391304345: average MSE= 0.1875454792873568\n",
      "lambda=2165.2173913043475: average MSE= 0.1875454975739461\n",
      "lambda=2173.9130434782605: average MSE= 0.1875455187281732\n",
      "lambda=2182.608695652174: average MSE= 0.1875455423403619\n",
      "lambda=2191.304347826087: average MSE= 0.1875455601991492\n",
      "lambda=2200.0: average MSE= 0.1875455808067581\n",
      "lambda=2208.695652173913: average MSE= 0.1875455972182098\n",
      "lambda=2217.391304347826: average MSE= 0.1875456189967416\n",
      "lambda=2226.086956521739: average MSE= 0.1875456395764410\n",
      "lambda=2234.782608695652: average MSE= 0.1875456584005148\n",
      "lambda=2243.478260869565: average MSE= 0.1875456808756012\n",
      "lambda=2252.173913043478: average MSE= 0.1875457058607368\n",
      "lambda=2260.869565217391: average MSE= 0.1875457243307136\n",
      "lambda=2269.565217391304: average MSE= 0.1875457453755056\n",
      "lambda=2278.2608695652175: average MSE= 0.1875457688867802\n",
      "lambda=2286.9565217391305: average MSE= 0.1875457884156756\n",
      "lambda=2295.6521739130435: average MSE= 0.1875458255766281\n",
      "lambda=2304.3478260869565: average MSE= 0.1875458603437322\n",
      "lambda=2313.0434782608695: average MSE= 0.1875458852400325\n",
      "lambda=2321.7391304347825: average MSE= 0.1875459167006320\n",
      "lambda=2330.4347826086955: average MSE= 0.1875459458662451\n",
      "lambda=2339.1304347826085: average MSE= 0.1875459787789251\n",
      "lambda=2347.8260869565215: average MSE= 0.1875460065137425\n",
      "lambda=2356.5217391304345: average MSE= 0.1875460333112648\n",
      "lambda=2365.2173913043475: average MSE= 0.1875460641778562\n",
      "lambda=2373.9130434782605: average MSE= 0.1875460874866257\n",
      "lambda=2382.608695652174: average MSE= 0.1875461128327594\n",
      "lambda=2391.304347826087: average MSE= 0.1875461367339106\n",
      "lambda=2400.0: average MSE= 0.1875461624317448\n",
      "lambda=2408.695652173913: average MSE= 0.1875461853726621\n",
      "lambda=2417.391304347826: average MSE= 0.1875462099911793\n",
      "lambda=2426.086956521739: average MSE= 0.1875462346138967\n",
      "lambda=2434.782608695652: average MSE= 0.1875462655236349\n",
      "lambda=2443.478260869565: average MSE= 0.1875462929593603\n",
      "lambda=2452.173913043478: average MSE= 0.1875463128920012\n",
      "lambda=2460.869565217391: average MSE= 0.1875463389919958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=2469.565217391304: average MSE= 0.1875463666368886\n",
      "lambda=2478.260869565217: average MSE= 0.1875463912546146\n",
      "lambda=2486.9565217391305: average MSE= 0.1875464104532258\n",
      "lambda=2495.6521739130435: average MSE= 0.1875464353303481\n",
      "lambda=2504.3478260869565: average MSE= 0.1875464591508483\n",
      "lambda=2513.0434782608695: average MSE= 0.1875464939193229\n",
      "lambda=2521.7391304347825: average MSE= 0.1875465243045593\n",
      "lambda=2530.4347826086955: average MSE= 0.1875465454248466\n",
      "lambda=2539.1304347826085: average MSE= 0.1875465786676427\n",
      "lambda=2547.8260869565215: average MSE= 0.1875466058646555\n",
      "lambda=2556.5217391304345: average MSE= 0.1875466410896200\n",
      "lambda=2565.2173913043475: average MSE= 0.1875466680037297\n",
      "lambda=2573.9130434782605: average MSE= 0.1875467107096458\n",
      "lambda=2582.608695652174: average MSE= 0.1875467443973535\n",
      "lambda=2591.304347826087: average MSE= 0.1875467692601903\n",
      "lambda=2600.0: average MSE= 0.1875467993509395\n",
      "best_lambda: 1000.0\n"
     ]
    }
   ],
   "source": [
    "def choose_best_lambda_lasso(X, y, folds_indexes, lambda_range):   #part of the code for this loop is taken from the functions provided in the weekly coding tasks of this module\n",
    "    lambda_scores = np.zeros((len(lambda_range),))\n",
    "  \n",
    "    for i, lamb in enumerate(lambda_range):\n",
    "        lambda_scores[i] = cross_validation_score_lasso(X, y, folds_indexes, lamb)\n",
    "        print(f'lambda={lamb}: average MSE= {lambda_scores[i]:.16f}')\n",
    "\n",
    "    best_lambda_index = np.argmin(lambda_scores)  #We choose the optimal lambda\n",
    "    return lambda_range[best_lambda_index]\n",
    "\n",
    "lambda_scores_lasso = np.linspace(0,2600,300)\n",
    "\n",
    "best_lambda_lasso = choose_best_lambda_lasso(X_aug_lasso, y_lasso, folds_indexes, lambda_scores_lasso)\n",
    "\n",
    "print('best_lambda:', best_lambda_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best $\\lambda$ is $1000$. We con compute the $\\beta$ for this hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.59416468e+00,  4.62529464e-02,  1.25796495e+00, -3.89868993e-02,\n",
       "        3.62562016e-01,  3.80725816e-03,  3.91464648e-01, -7.35374758e-02,\n",
       "       -3.57221479e-02, -1.52400109e-02,  7.58410204e-07])"
      ]
     },
     "execution_count": 1365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_lasso = minimize_ls_huber(X_aug_lasso,y_lasso,1000,n_iters=10000, step_size=1e-3)\n",
    "beta_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the average MSE depending on the parameter $\\lambda$ to see what is happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe001191450>]"
      ]
     },
     "execution_count": 1351,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEDCAYAAAAlRP8qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqEUlEQVR4nO3dd3xW9d3/8dcnm4SElYRNwp6KQGSI4kZxe6utA6mrlLvau0NrtVrvLnvX9rbeRa3WDdW6Zyu4cYCssMNMgBDCSsLIIGRe398fOfSXxgAhXMlJrryfj8f1yMn3nOucz/e68O3J9yxzziEiIq1fmN8FiIhIcCjQRURChAJdRCREKNBFREKEAl1EJEQo0EVEQoSvgW5mz5lZnpllBGl9fczsIzNbb2brzCy1ge8bYmYLzazczO46ynLnmtlyM1tpZvPNbIDX/lOvbaWZZZhZtZl19uZlm9kab156Peu8y8ycmSXWarvXzLLMbKOZXVCrfYy3riwzm2lm5rVHm9mrXvvi2v02s++YWab3+k6t9r7espnee6O8dvPWnWVmq81sdEM+w6Mxs0ne51ZlZlef6PpE5Aicc769gEnAaCAjSOv7HDjfm24PxNazTHY9bcnAqcCDwF1HWf8mYKg3/X3ghXqWuRT4rPb2gMQjrK838CGw7fAywDBgFRAN9AU2A+HevCXABMCAucCUWrU86U1fC7zqTXcGtng/O3nTnbx5rwHXetNPAv/pTV/krduA8cDiIHwvqcDJwGzgaj//zemlVyi/fN1Dd859Ceyr3WZm/c3sAzNbZmZfmdmQhqzLzIYBEc65j711lzjnShtYR55zbilQeaxFgQRvugOws55lrgNebsh2gUeAu731HnY58Ipzrtw5txXIAsaaWXcgwTm30DnnqAnHK2q9Z5Y3/QZwrrf3fgHwsXNun3NuP/AxcKE37xxvWbz31l7XbFdjEdDR2zZmNtXMlnh/bfzVzMIb0knnXLZzbjUQaODnIiKN0BLH0J8CfuCcGwPcBfylge8bBBwws7fMbIWZ/bGhgXMcbgPmmFkucCPw+9ozzSwWuBB4s1azAz7y/gc1vdaylwE7nHOr6myjJ7C91u+5XltPb7pu+7+9xzlXBRQCXY6yri7AAW/ZI66r9jwzGwp8G5jonDsFqAZuQERajAi/C6jNzNoDpwGve8PDUDP0gJn9B/Dret62wzl3ATV9OQMYBeQArwI3Ac+a2ePARG/5Hma20pt+3Tn34HGU+GPgIufcYjP7KfAnakL+sEuBBc652n91THTO7TSzZOBjM9sApAP3AZPr2YbV0+aO0t6Y9zRmXecCY4Cl3nfTDsgDMLPZ1Ayd1fUX51xD/4csIieoRQU6NX8xHPD2AP+Nc+4t4K2jvDcXWOGc2wJgZu9QMwb8rHPu9sMLmVl2fes/FjNLAkY65xZ7Ta8CH9RZ7FrqDLc453Z6P/PM7G1gLLCfmvHxVV449gKWm9lYrx+9a62iFzVDO7nedN12ar0n18wiqBkO2ue1n1XnPZ8DBdQMpUR4e+n1ravudgyY5Zy7t+5n45ybVrdNRJpfixpycc4VAVvN7Br41xkXIxv49qVAJy94oWaMeF0Qy9sPdDCzQd7v5wPrD880sw7AmcC7tdrizCz+8DQ1e+QZzrk1zrlk51yqcy6VmhAd7ZzbDbwHXOududIXGAgscc7tAorNbLw3Bj6t1rbeAw6fwXI1NQdlHTUHXCebWScz6+Rt/0Nv3jxvWbz31l7XNO+zHw8Uetv+FLja+0sDM+tsZikn8oGKSJD5eUSWmr3ZXdQcjMwFbqVmz/UDas70WAc8cBzrOx9YDawBXgCi6lkmu562bt72i4AD3nSCN28O0MObvtJb9ypq9nT71VrHTdQczKy93n7esquAtcB9R6g7m1pnwlAzHLMZ2Ih3JovXngZkePMeA8xrjwFep+YA6pI6dd3itWcBN9epbYnX/joQ7bUb8Li3jTVAWq33fBtY6X3Gy4DxDfxeTvU+04PAXmCtn//u9NIrVF+HA0FERFq5FjXkIiIijefbQdHExESXmprq1+ZFRFqlZcuWFTjnkuqb51ugp6amkp7+jSvhRUTkKMxs25HmachFRCREKNBFREKEAl1EJEQo0EVEQoQCXUQkRCjQRURChAJdRCREKNBFRJrRnz/JZGn2vmMv2AgKdBGRZrJpTzGPfLKJhZv3Nsn6FegiIs3kqS+30C4ynBvHN82dpxXoIiLNYHdhGe+u3MG30nrRKS6qSbahQBcRaQbPL9hKdcBx2xn9mmwbCnQRkSZWXFbJ3xfncNFJ3endObbJtqNAFxFpYi8vyaG4vIrvTerfpNtRoIuINKHyqmqem5/Naf27cFKvDk26LQW6iEgTenPZDnYXlTHjzKbdOwcFuohIk6msDvD4vCxO6d2RMwYmNvn2FOgiIk3k7eU72HHgED88dyBm1uTbU6CLiDSBquoAj3+exUk9O3DW4HofARp0CnQRkSbw3qqdbNtbyg/OGdAse+egQBcRCbrqgOOxeVkM7Z7A+cO6Ntt2FegiIkH20drdbMk/yB1nN9/eOSjQRUSCyjnHX7/cQkqXWC4c0a1Zt61AFxEJoqXZ+1m5/QC3nd6X8LDm2zsHBbqISFA99eVmOsdFcfWY3s2+bQW6iEiQZOUV88n6PG4cn0K7qPBm374CXUQkSJ7+civREWFMm9A0D7A4FgW6iEgQ5BWX8faKHVyT1osu7aN9qUGBLiISBLO+zqYyEODW05vuARbHokAXETlBpRVVvLgoh8nDutI3Mc63OhToIiIn6PX0XAoPVTJ9kn9756BAFxE5IdUBxzPztzC6T0fGpHT2tRYFuojICfhw7W627zvk+945KNBFRBrNOcdT3mX+5w9r3sv866NAFxFppGXb/LvMvz4KdBGRRnrqyy10jI3kqjG9/C4FUKCLiDTK1oKDfLx+DzeOTyE2KsLvcgAFuohIozw7fwuRYWFMm5Dqdyn/okAXETlOe0vKeT09lytH9SQp3p/L/OvToEA3s45m9oaZbTCz9WY2oc58M7OZZpZlZqvNbHTTlCsi4r8XF+VQXhXgtjP6+l3Kv2nowM+fgQ+cc1ebWRQQW2f+FGCg9xoHPOH9FBEJKWWV1cxemM05Q5IZ2DXe73L+zTH30M0sAZgEPAvgnKtwzh2os9jlwGxXYxHQ0cy6B7tYERG/vbtyB3sPVrS4vXNo2JBLPyAfeN7MVpjZM2ZW9+4zPYHttX7P9dr+jZlNN7N0M0vPz89vdNEiIn5wzjF74TYGd41nQr8ufpfzDQ0J9AhgNPCEc24UcBC4p84y9Z1R777R4NxTzrk051xaUlLScRcrIuKn5TkHWLuziBsnpGDm/4VEdTUk0HOBXOfcYu/3N6gJ+LrL1H6AXi9g54mXJyLScry4aBvx0RFcOeobAxAtwjED3Tm3G9huZoO9pnOBdXUWew+Y5p3tMh4odM7tCm6pIiL+KSgp5/3Vu7hqTC/iolvGhUR1NbSqHwAveWe4bAFuNrMZAM65J4E5wEVAFlAK3NwEtYqI+ObVpdupqA4wdbw/zwttiAYFunNuJZBWp/nJWvMdcHvwyhIRaTmqA46/L87htP5dGJDc3u9yjkhXioqIHMNnG/LYceAQ0ya03L1zUKCLiBzTq0tzSI6P5ryhXf0u5agU6CIiR5FXVMa8jflcNaYXEeEtOzJbdnUiIj57c/kOqgOOb6X1PvbCPlOgi4gcgXOO19O3Mza1M30T614g3/Io0EVEjmBp9n62FBzkW6e2/L1zUKCLiBzRa+nbiYsK56KT/H8AdEMo0EVE6lFcVsn7q3dx6cgeLeYRc8eiQBcRqcc/V+/iUGV1qxluAQW6iMg3OOeY9XU2Q7rFM6p3R7/LaTAFuohIHQu37GXD7mJunpjaIm+TeyQKdBGROp5fkE2n2EguP6Vl3ib3SBToIiK15Owt5ZP1e7h+XB9iIsP9Lue4KNBFRGqZtTCbcDNuHJ/qdynHTYEuIuIpKa/itaXbmXJSd7p1iPG7nOOmQBcR8byRvp3i8ipunpjqdymNokAXEaFm7/yxeVmcmtqJ0X06+V1OoyjQRUSAv36xmYKSCn5+0VC/S2k0BbqItHm7Cg/x9FdbuHRkD0a10r1zUKCLiPC/H24iEIC7LxjsdyknRIEuIm1axo5C3lqRy80TU+ndOdbvck6IAl1E2iznHA++v54O7SL5/tkD/C7nhCnQRaTN+iBjNwu37OXO8wfRoV2k3+WcMAW6iLRJZZXVPDhnPUO6xXPd2D5+lxMUCnQRaZOe/nILufsP8cAlw4gID40oDI1eiIgch12Fh/jL55u5cHg3ThuQ6Hc5QaNAF5E253/mbKDaOe67uPVeRFQfBbqItCnzMwt4b9VOZpzZv9WfpliXAl1E2oyyymp+8W4GKV1i+f5Z/f0uJ+hax6OsRUSC4MkvNrO14CCzbxnb6h5e0RDaQxeRNmFrwUH+Mm8zl47swaRBSX6X0yQU6CIS8pxzPPBuBtGRYfziktA6EFqbAl1EQt7cjN18lVnAnecPIjm+9T2JqKEU6CIS0korqvjtP9cxtHsCU8en+F1Ok1Kgi0hIe+yzLHYWlvGby4eHzBWhRxLavRORNm1LfglPf7WF/xjdk7TUzn6X0+QadNqimWUDxUA1UOWcS6sz/yzgXWCr1/SWc+7XQatSROQ4Oef41T/WERMRzr1TQvdAaG3Hcx762c65gqPM/8o5d8mJFiQiEgyfrs/ji035/OKSYSTFR/tdTrNolUMueUVlfpcgIi1YeVU1v31/Hf2T4pg2IbQPhNbW0EB3wEdmtszMph9hmQlmtsrM5prZ8PoWMLPpZpZuZun5+fmNKnjOml1M+uM8Xlq8Dedco9YhIqHt+QXZZO8t5YFLhxMZ4gdCa2toTyc650YDU4DbzWxSnfnLgRTn3EjgUeCd+lbinHvKOZfmnEtLSmrclVppKZ04NbUz972dwYwXl7H/YEWj1iMioSmvuIxHP83kvKHJnBmiV4QeSYMC3Tm30/uZB7wNjK0zv8g5V+JNzwEizaxJbjKcnBDDrJvHcv/FQ/lsQx5T/vwV63YWNcWmRKQV+sMHG6moDnDfxcP8LqXZHTPQzSzOzOIPTwOTgYw6y3QzM/Omx3rr3Rv8cmuEhRm3ndGPt78/ETO4/plFZOwobKrNiUgrsSCrgDeW5XLr6f3omxjndznNriF76F2B+Wa2ClgCvO+c+8DMZpjZDG+Zq4EMb5mZwLWuGQa4R/TswKvTJxAXFcH1Ty9i1fYDTb1JEWmhissqufuN1fRLjONH5w30uxxfmF8HFtPS0lx6enpQ1rV9XynXP7OIAwcreXn6eEb07BCU9YpI63HvW2t4dWkOr884jTEpnfwup8mY2bK61wIdFhKHf3t3juXV6ROIj4ng1llL2V2o0xpF2pIvN+Xz8pIcvntGv5AO82MJiUAH6NGxHc/edColZVXcNnsppRVVfpckIs2guKySe95cTf+kOH58/iC/y/FVyAQ6wNDuCcy8bhTrdhbxk1dXEQjoPHWRUPf7uRvYXVTGH68ZGZJPIToeIRXoAOcO7cp9Fw/jg7W7eWxelt/liEgT+npzAS8tzuHW0/syuk/bHWo5LOQCHeCWialcOaon//fJJhZtabKzJ0XER6UVVdzz5hpSu8Tyk/MH+11OixCSgW5m/OaKEaR2ieOHr6xgb0m53yWJSJA9/NEmcvaV8tBVJ9Muqm0PtRwWkoEO0D46gkevH8X+0krufF3j6SKhZHXuAZ5bsJWp4/swrl8Xv8tpMUI20AGG9+jALy4Zxucb83luwdZjv0FEWrzqgOP+dzJIbB/N3RcO8bucFiWkAx1g6rg+nD+sK3/4cCNZecV+lyMiJ+jlJTmszi3k/ouHkhAT6Xc5LUrIB7qZ8bsrTyIuKpw7X1tFVXXA75JEpJH2lpTzxw83MqFfFy4b2cPvclqckA90gKT4aH57xUmsyi3kyS82+12OiDTS7+du4GB5Fb++fDje/QClljYR6AAXn9ydS07uzp8/zWTtTt2ZUaS1mZ9ZwOvLcrn19L4M7BrvdzktUpsJdIDfXD6CDu2i+NmbqzX0ItKKFJSU8+PXVjIguT0/Oq9tX95/NG0q0DvFRfGry4aTsaOIF77O9rscEWmAQMBx52urKDxUyaPXjdI550fRpgId4KKTunHukGQe/mgT2/eV+l2OiBzDcwu28sWmfO6/eChDuyf4XU6L1uYC3cz49RUjCDO4/50MPWhapAVbv6uIhz7YwPnDunLj+BS/y2nx2lygA/Ts2I67LhjMF5vyeW/VTr/LEZF6VAcc97y5moSYSP5w1ck6q6UB2mSgA0ybkMrIXh34zT/XU3io0u9yRKSOF77OZlVuIQ9cOoxOcVF+l9MqtNlADw+ruYHX3oPlPPLxJr/LEZFacveX8vBHGzlrcJIuIDoObTbQAU7u1ZEbxvVh9sJsnZsu0kI45/jFOxkA/PaKERpqOQ5tOtABfjp5CJ1io3jg3bW6I6NIC/D2ih3M25jPnZMH06tTrN/ltCptPtA7xEZyz5QhLNu2nzeW5/pdjkibtn1fKf/97lpOTe3ETael+l1Oq9PmAx3gqtG9GJPSiYfmbtABUhGfVAccP3ltJQ7407dOITxMQy3HS4EOhIUZv7psOPtKK5j5aabf5Yi0SU9+sZml2fv59eXD6d1ZQy2NoUD3jOjZgWtP7c2sr7N133SRZrY69wCPfLyJi0/uzpWjevpdTqulQK/lrsmDaRcVzq/+sU5XkIo0k6KySu74+wqS4qN5UGe1nBAFei1d2kfzk/MH8VVmAR+v2+N3OSIhz7maq0F3HDjEY9ePomOsLiA6EQr0OqaOT2FQ1/b89v31lFdV+12OSEh7cdE25qzZzV2TBzMmpbPf5bR6CvQ6IsPDuP/iYeTsK+WFBdl+lyMSstbuLOQ3/1zPWYOT+N6kfn6XExIU6PWYNCiJc4Yk8+hnWRSUlPtdjkjIKSmv4o6/r6BTXCQPXzOSMJ2iGBQK9CP4+UVDKaus5k+6z4tI0D3wTgbb9h7kz9eOokv7aL/LCRkK9CMYkNyeqeNTeGVJDut3FfldjkjIeGNZLm+t2MF/nTuQ8f26+F1OSFGgH8WPzhtIfEwkv31fpzGKBMPm/BJ+8U4G4/p25gfnDPS7nJCjQD+KjrFR/Oi8gSzI2su8jXl+lyPSqh2qqOb2l5bTLiqcP187Spf2NwEF+jHcMC6FvolxPPj+eiqrA36XI9Jq/fK9tWzcU8yfvjWSbh1i/C4nJDUo0M0s28zWmNlKM0uvZ76Z2UwzyzKz1WY2Ovil+iMqIox7pwxhc/5BXlmS43c5Iq3Sm8tyeTV9O7efNYCzBif7XU7IOp499LOdc6c459LqmTcFGOi9pgNPBKO4luL8YV0Z17czj3ySSVGZ7sYocjwy9xRz/zsZjO/XmR+dp3HzphSsIZfLgdmuxiKgo5l1D9K6fWdm3H/xMPYdrOAv8zb7XY5Iq1FSXsWMF5cRFx3OzGtHERGuUd6m1NBP1wEfmdkyM5tez/yewPZav+d6bSHjpF4d+I9RPXluwVZy95f6XY5Ii+ec42dvrGZrwUEevW40yQkaN29qDQ30ic650dQMrdxuZpPqzK/vcPU3zvMzs+lmlm5m6fn5+cdZqv/uumAwBvzvhxv9LkWkxXt2/lbeX7OLuy8cwoT+Ot+8OTQo0J1zO72fecDbwNg6i+QCvWv93gvYWc96nnLOpTnn0pKSkhpXsY96dGzHraf35Z2VO1mTq4dKixzJkq37+J+5G7hgeFfdp6UZHTPQzSzOzOIPTwOTgYw6i70HTPPOdhkPFDrndgW92hZgxln96RwXxYNzdLGRSH12F5bx/ZeW06dzLH+8ZqTub96MGrKH3hWYb2argCXA+865D8xshpnN8JaZA2wBsoCnge83SbUtQEJMJD86byCLtuzjsw262EiktvKqav7zpWWUVlTx1xvHkBAT6XdJbUrEsRZwzm0BRtbT/mStaQfcHtzSWq7rxvbhhQXZ/G7Oes4clKQj9yKeX763jhU5B3jihtEM6hrvdzltjpKoESLDw/iZd7HRq+nbj/0GkTbg74tzeHlJDref3Z8pJ4XMWcutigK9kSYP60paSice+TiTg+VVfpcj4qul2fv47/cyOHNQEj85f7Df5bRZCvRGMjN+fvFQCkrKeerLLX6XI+KbHQcOMeNvy+jdKZaZ1+mmW35SoJ+A0X06cdFJ3Xj6qy3kFZX5XY5IsyutqOK7s9KpqA7w9HfS6NBOB0H9pEA/QXdfMISKqgCPfJLpdykizSoQcNz1+irW7y5i5nWj6J/U3u+S2jwF+glKTYxj6vgUXl2aQ+aeYr/LEWk2j3yyiTlrdnPvlCGcrTsotggK9CD4wTkDiIuK4KEPdEsAaRveXpHLo59l8e203nz3DF0J2lIo0IOgS/toZpzVn0/W72Hxlr1+lyPSpNKz9/GzN9Ywvl9nfnPFCF0J2oIo0IPklol96ZYQw+/mbtAtASRkbdt7kOl/W0bPTu14cuoYoiIUIS2Jvo0gaRcVzk8mD2LV9gPMWbPb73JEgm7/wQpufn4pAed49jtpdIyN8rskqUOBHkRXje7FkG7x/OHDDVRU6fmjEjrKKquZ/rd0cvcf4ulpafTTGS0tkgI9iMLDjHumDGHb3lJeWrzN73JEgiIQcNz9xmqWZu/n4W+N5NTUzn6XJEegQA+yMwclMXFAF2Z+quePSmj4/QcbeG/VTu6+cDCXjuzhdzlyFAr0IDMz7p0ylP2llTz5uZ4/Kq3bs/O38tSXW5g2IYX/PLO/3+XIMSjQm8CInh244pQePDt/K7sKD/ldjkij/GPVTn7zz3VMGdGN/750uE5PbAUU6E3kzsmDcQ4e/miT36WIHLf5mQXc+doqxqZ25pFvn6IbbrUSCvQm0rtzLN85LYU3l+eyfleR3+WINNjK7QeY/rd0+iXF8fS0NGIiw/0uSRpIgd6Ebj97AAkxkfx+7ga/SxFpkMw9xdz0/BIS20cz+5axdIjV3RNbEwV6E+oYG8UdZw/gi035zM8s8LsckaPavq+UG59dQmR4GC/eOo7khBi/S5LjpEBvYjdOSKFnx3b8bs56AgHdEkBapj1FZdzwzGIOVVYz+5ax9OkS63dJ0ggK9CYWExnO3RcOZt2uIt5ZucPvckS+YW9JOTc8s5i9JeXMumUsQ7sn+F2SNJICvRlcenIPRvRM4H8/3EhZZbXf5Yj8S2FpJdOeW0Lu/lKeu+lUTund0e+S5AQo0JtBWJjx84uGsrOwjOcXZPtdjggAhYcqufG5xWTuKeHJqWMY16+L3yXJCVKgN5PT+idyzpBk/jIvi30HK/wuR9q4orKaPfP1u4p4YupoztITh0KCAr0Z3TtlCAcrqpj5qZ4/Kv4pLqvkpueWsHZHIY9fP5pzh3b1uyQJEgV6MxrYNZ5vn9qHFxdtY2vBQb/LkTbo8J756txCHrt+FJOHd/O7JAkiBXoz+/H5A4mKCOMPH+hiI2lehaWVTH1mMRk7Cnns+tFcOKK73yVJkCnQm1lyfAzTJ/VjbsZulm3b53c50kbsP1jB9c8sYsOuYp64YQwXjtCeeShSoPtg+qR+JMdH8+D76/X8UWlyeUVlfPuphWTmlfDXaWM4b5jGzEOVAt0HsVER3Dl5EMtzDjA3Q88flaaTu7+Ua/66kNz9h3jh5lM5W2ezhDQFuk+uHtObwV3j+f1cPX9Umsbm/BKueXIh+w9W8OJt4zitf6LfJUkTU6D7JDzM+PnFQ8nZV8rfFun5oxJcK7cf4OonvqayOsAr0ycwuk8nv0uSZqBA99GZg5KYNCiJmZ9mUliq549KcHy5KZ/rn15E+5gI3phxGsN66N4sbYUC3Wc/v2gIRWWVzPxMFxvJiXt35Q5unbWUlC5xvDnjNFIT4/wuSZqRAt1nQ7ol8K0xvZm9MJtsXWwkjeSc4/F5WfzwlZWM7tOJV6aP1/3M2yAFegtw5+RBRIaH8ZAuNpJGqKoOcP87Gfzxw41cNrIHs28dS4d2etJQW9TgQDezcDNbYWb/rGfeWWZWaGYrvdcDwS0ztCUnxDDjzP7MzdjNkq262Egarriskttmp/PS4hz+86z+/N+3TyE6Qs8AbauOZw/9h8D6o8z/yjl3ivf69QnW1eZ894x+dEuI4cH31+nJRtIg2/eVctUTXzM/s4DfXXkSP7twCGFh5ndZ4qMGBbqZ9QIuBp5p2nLarnZR4fz0gsGsyi3ktfTtfpcjLVx69j4uf3wBuwvLmHXLWK4f18fvkqQFaOge+v8BdwNHuwJmgpmtMrO5Zja8vgXMbLqZpZtZen5+/nGWGvquHNWT8f0688C7a1maraEXqd/LS3K47ulFJMRE8PbtE5k4QBcMSY1jBrqZXQLkOeeWHWWx5UCKc24k8CjwTn0LOeeecs6lOefSkpKSGlNvSAsLM56cOoZendoxfXa6znqRf1NRFeD+d9Zw71trGN+vC+/efjr9k9r7XZa0IA3ZQ58IXGZm2cArwDlm9mLtBZxzRc65Em96DhBpZtptaISOsVE8d9OpANz8wlL26+lGAuwpKuP6pxfx4qIcvjepHy/cPJYOsTqTRf7dMQPdOXevc66Xcy4VuBb4zDk3tfYyZtbNzMybHuutd28T1NsmpCbG8dS0NHbsP8RVT3zN+l1FfpckPlq4eS8Xz/yKtTuL+PO1p3DvRUMJ18FPqUejz0M3sxlmNsP79Wogw8xWATOBa53uC3tCTk3tzKxbxlJcXsXljy/g74tzdKvdNiYQcDzx+WZueGYRHdpF8t4dE7n8lJ5+lyUtmPkVEmlpaS49Pd2XbbcmBSXl/PjVlXyVWcBlI3vwP/9xEnHREX6XJU2soKScn7y2ii835XPxyd156KqTaa/vXQAzW+acS6tvnv6FtHCJ7aOZdfNYnvhiMw9/tJF1u4p4cupoBiTH+12aNJEFWQX86NWVFB2q5MErR3D92D54I5oiR6VL/1uBsDDj9rMH8OKt49h/sILLHlvAuyt3+F2WBFl5VTW/m7Oeqc8uJiEmgnfvmMgN41IU5tJgCvRW5LQBibz/X2cwrHsCP3xlJfe+tYayymq/y5Ig2Li7mMsfW8BTX27h+rF9+McPTmdIN932Vo6PhlxamW4dYnh5+nge/mgTT36xmRU5+3n8htE6H7mVqqoO8PRXW3nk403Ex0Tw7HfSOHeonvkpjaM99FYoMjyMe6YM4fmbTyWvuJxLZs7n5SU6C6a1ycor5qonF/LQBxs4Z0gyH/54ksJcTogCvRU7e3Ayc394BmNSOnHvW2uY/rdl7NOFSC1eRVWAmZ9mctGf55Oz9yCPXjeKJ6aOJrF9tN+lSSun0xZDQCDgeG7BVv7wwUYS2kXy2ytGcOGIbn6XJfVIz97HvW+tITOvhEtH9uCBS4aRFK8gl4bTaYshLizMuO2MfkwckMhdr69ixovLuHRkD3512XA6x0X5XZ5Qc175Q3M38PqyXHp2bMfzN53K2UOS/S5LQowCPYQM7Z7AO7dP5InPN/PoZ5l8nVXAfRcP5cpRPXXqm0+qqgO8tDiHhz/aSGlFNd87sx//dc5AXRwmTUJDLiFqw+4i7n1rDStyDjChXxd+e+UInQnTzOZtzOPB99eTlVfC6QMS+eVlwxmQrO9ATszRhlwU6CEsEHC8vDSHh+Zu4FBlNTdP7Msd5wwgIUZ36WtKa3cW8tAHG/lyUz6pXWK5Z8pQLhjeVX8lSVAo0Nu4/OJy/vhhzfht59go7rpgMNeM6UVEuE5yCqatBQf508eb+MeqnXRoF8l/nTuQG8enEBWhz1mCR4EuAKzJLeTX/1zL0uz99E+K46cXDOaC4d2053iCsgsO8vi8LN5asYOo8DBuOT2V6ZP606Gd/hKS4FOgy7845/ho3R7++OFGsvJKGNmrAz88byBnD05WsB+nzD3FPPH5Zt5ZuYPI8DCuG9uH75/dn+T4GL9LkxCmQJdvqA443lqey/99ksmOA4cY0TOBO84eyORhXfXk+KNwzrF46z6e+nILn23IIyYyjKnjUph+Zj8FuTQLBbocUWV1gLdX7ODxeVls21tKv8Q4bjm9L1eN7kW7qHC/y2sxyiqreW/lTmYtzGbtziK6xEUxbUIqN05I0bn+0qwU6HJMVdUB5mTs5pmvtrA6t5COsZF8O603143tQ2pinN/l+Wbj7mJeS9/Om8tzOVBayeCu8Uw7LYWrRvciJlL/w5Pmp0CXBnPOsTR7P88v2MpH6/ZQHXCcPiCRa9J6MXlYtzax1763pJw5a3bxxvIdrNp+gMhwY/Kwbtw4IYVxfTvrWIP4SoEujbKnqIzXlm7nlaXb2XHgEO2jI7hwRDcuHdmD0/p3ITKETnvcf7CCT9bv4f01u/gqs4DqgGNw13iuSevFlaN60kU3zpIWQoEuJyQQcCzaupe3l+9gbsZuSsqrSIiJ4LxhXTl/aFcmDkxsdRcrOefIyivhi035fLJ+D0u27iPgoGfHdlx2Sg8uG9mDId3itTcuLY4CXYKmrLKarzILmJuxi0/W7aGorIqIMGN0SidOH5DIuL6dGdm7Y4sbX3bOkbv/EIu37mPJ1r3MzyxgZ2EZAAOT23PB8G5cMLwbI3omKMSlRVOgS5OorA6wIucAn2/M4/ON+azfXYRzEBURxkk9O3Byrw6M7NWR4T0SSE2Ma7YhGuccu4vK2Li7mIwdhazKLWR17gH2FJUD0DE2kvF9uzBpUBKTBiXSq1Nss9QlEgwKdGkWB0orWJq9n8Vb9rJi+wHW7iykrDIAQGS40Tcxjv5J7endOZZendrRs2M7EttHkxQfTee4KKIjwhq0d1xeVU3RoSryi8vJLylnT1EZuftKydlXyrZ9pWTllVBcVvWv5fslxnFyrw6MTunEuL5dGJjcXufaS6ulQBdfVFUHyMwrYcPuIjbtKSFzTzFbCg6Su/8QFVWBbywfEWa0j4kgNjKciPAwIsIMM6gKOCqrApRXBSgur6r3vWEG3Tu0o0/nWAYkt2dQ1/YM7BrPsB4JrW58X+Ro9IAL8UVEeBhDuycwtPu/P70+EHDkl5Sz88AhCkoqKCgpZ9/BCg6WV1FSXsXB8moCzlEVcAQCjshwIzI8jMiIMOKjI4iPiSChXSSJ7aNJjo8mOT6Gbh1idBMsafMU6NLswsKMrgkxdE3QpfIiwaRdGhGREKFAFxEJEQp0EZEQoUAXEQkRCnQRkRChQBcRCREKdBGREKFAFxEJEb5d+m9m+cC2Rr49ESgIYjmtRVvsd1vsM7TNfrfFPsPx9zvFOZdU3wzfAv1EmFn6ke5lEMraYr/bYp+hbfa7LfYZgttvDbmIiIQIBbqISIhorYH+lN8F+KQt9rst9hnaZr/bYp8hiP1ulWPoIiLyTa11D11EROpQoIuIhIhWF+hmdqGZbTSzLDO7x+96gsnMss1sjZmtNLN0r62zmX1sZpnez061lr/X+xw2mtkF/lV+fMzsOTPLM7OMWm3H3U8zG+N9XllmNtMa8kBSnxyhz780sx3e973SzC6qNS8U+tzbzOaZ2XozW2tmP/TaQ/27PlK/m/77ds61mhcQDmwG+gFRwCpgmN91BbF/2UBinbY/APd40/cAD3nTw7z+RwN9vc8l3O8+NLCfk4DRQMaJ9BNYAkwADJgLTPG7b8fZ518Cd9WzbKj0uTsw2puOBzZ5fQv17/pI/W7y77u17aGPBbKcc1uccxXAK8DlPtfU1C4HZnnTs4ArarW/4pwrd85tBbKo+XxaPOfcl8C+Os3H1U8z6w4kOOcWupp/+bNrvafFOUKfjyRU+rzLObfcmy4G1gM9Cf3v+kj9PpKg9bu1BXpPYHut33M5+gfV2jjgIzNbZmbTvbauzrldUPMPBUj22kPtszjefvb0puu2tzZ3mNlqb0jm8NBDyPXZzFKBUcBi2tB3Xaff0MTfd2sL9PrGj0LpvMuJzrnRwBTgdjObdJRlQ/2zOOxI/QyF/j8B9AdOAXYBD3vtIdVnM2sPvAn8yDlXdLRF62kLpX43+ffd2gI9F+hd6/dewE6fagk659xO72ce8DY1Qyh7vD+98H7meYuH2mdxvP3M9abrtrcazrk9zrlq51wAeJr/P2QWMn02s0hqQu0l59xbXnPIf9f19bs5vu/WFuhLgYFm1tfMooBrgfd8rikozCzOzOIPTwOTgQxq+vcdb7HvAO960+8B15pZtJn1BQZScwCltTqufnp/qheb2XjvyP+0Wu9pFQ6HmudKar5vCJE+ezU+C6x3zv2p1qyQ/q6P1O9m+b79PiLciCPIF1Fz1HgzcJ/f9QSxX/2oOdK9Clh7uG9AF+BTINP72bnWe+7zPoeNtOCj/vX09WVq/uSspGYv5NbG9BNI8/6j2Aw8hnflc0t8HaHPfwPWAKu9/6i7h1ifT6dmiGA1sNJ7XdQGvusj9bvJv29d+i8iEiJa25CLiIgcgQJdRCREKNBFREKEAl1EJEQo0EVEQoQCXUQkRCjQRURCxP8DIcyrDO1Nc4UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting average MSE vs lambda\n",
    "l_lasso1 = [] #to store the values of the average MSE\n",
    "m_lasso1 = [] #to store the values of the parameter lambda \n",
    "for i in range(0,100):\n",
    "    lambda_val = 25*i\n",
    "    m_lasso1.append(lambda_val)\n",
    "    l_lasso1.append(cross_validation_score_lasso(X_aug_lasso, y_lasso, folds_indexes, lambda_val)) #calculating MSE and appending it to l\n",
    "plt.plot(m_lasso1,l_lasso1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know can calculate the $R^2$ score of the Lasso-Huber model for the training data-set (we mean  in-sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8718162628869924\n"
     ]
    }
   ],
   "source": [
    "R_square_lasso = R_square_calculator(X_aug_lasso,y_lasso,beta_lasso)\n",
    "print(R_square_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know can calculate the $R^2$ score of the Lasso-Huber model for the testing data-set (we mean  out-sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8643183548530967\n"
     ]
    }
   ],
   "source": [
    "R_square_lasso_test = R_square_calculator(X_aug_test,y_test,beta_lasso)\n",
    "print(R_square_lasso_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can still see that the fist score is better but only slightly, so the method can be generalised, and used for previsions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso-Huber forces many parameters to have small values inducing a stronger sparcity then ridge. This can be seen for example in the last component of $\\beta$. In fact for the ridge regression we have this component equal to $0.00318129$, while for the the lasso-huber is $7.58410204e-07$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta for ridge = [ 2.61820782  0.04433538  1.25232768 -0.03780222  0.36279709  0.00495263\n",
      "  0.39065421 -0.0746366  -0.03570966 -0.01524782  0.00318129]\n",
      "beta for lasso = [ 2.59416468e+00  4.62529464e-02  1.25796495e+00 -3.89868993e-02\n",
      "  3.62562016e-01  3.80725816e-03  3.91464648e-01 -7.35374758e-02\n",
      " -3.57221479e-02 -1.52400109e-02  7.58410204e-07]\n"
     ]
    }
   ],
   "source": [
    "print('beta for ridge =',beta_optimal_ridge)\n",
    "print('beta for lasso =',beta_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So an 'increased level' of sparsity passing from Lasso-huber to Ridge can be observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the difference between the norms of $\\beta$ for the ridge and the lasso regression, to see that the norm for $\\beta$ obtained with ridge is greater than the norm of  $\\beta$ obtained with Lasso-Huber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference = 0.01879303840265134\n",
      "relative difference in % = 0.6364827759835024\n"
     ]
    }
   ],
   "source": [
    "print('difference =',np.linalg.norm(beta_optimal_ridge)-np.linalg.norm(beta_lasso))\n",
    "print('relative difference in % =',(np.linalg.norm(beta_optimal_ridge)-np.linalg.norm(beta_lasso))*100/np.linalg.norm(beta_optimal_ridge))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi0I8ERdy0mD"
   },
   "source": [
    "<a name=\"task-2\"></a>\n",
    "\n",
    "# Task 2: Classification [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgBfTmgpy08Z"
   },
   "source": [
    "<a name=\"q21\"></a>\n",
    "\n",
    "## 2.1: kNN classifier [^](#outline)\n",
    "### 2.1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we import the data from $\\texttt{tumour_samples.csv}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n1_radius</th>\n",
       "      <th>n1_texture</th>\n",
       "      <th>n1_perimeter</th>\n",
       "      <th>n1_area</th>\n",
       "      <th>n1_smoothness</th>\n",
       "      <th>n1_compactness</th>\n",
       "      <th>n1_concavity</th>\n",
       "      <th>n1_concave_points</th>\n",
       "      <th>n1_symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>n3_texture</th>\n",
       "      <th>n3_perimeter</th>\n",
       "      <th>n3_area</th>\n",
       "      <th>n3_smoothness</th>\n",
       "      <th>n3_compactness</th>\n",
       "      <th>n3_concavity</th>\n",
       "      <th>n3_concave_points</th>\n",
       "      <th>n3_symmetry</th>\n",
       "      <th>n3_fractal_dimension</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.855170</td>\n",
       "      <td>15.248290</td>\n",
       "      <td>69.167041</td>\n",
       "      <td>359.534878</td>\n",
       "      <td>0.105488</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.066410</td>\n",
       "      <td>0.034194</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>...</td>\n",
       "      <td>18.984557</td>\n",
       "      <td>81.443134</td>\n",
       "      <td>466.879302</td>\n",
       "      <td>0.149080</td>\n",
       "      <td>0.200185</td>\n",
       "      <td>0.205695</td>\n",
       "      <td>0.111592</td>\n",
       "      <td>0.335999</td>\n",
       "      <td>0.093477</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.068958</td>\n",
       "      <td>15.532758</td>\n",
       "      <td>66.130635</td>\n",
       "      <td>330.040665</td>\n",
       "      <td>0.099813</td>\n",
       "      <td>0.109540</td>\n",
       "      <td>0.057583</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.195650</td>\n",
       "      <td>...</td>\n",
       "      <td>22.840293</td>\n",
       "      <td>82.133171</td>\n",
       "      <td>473.367822</td>\n",
       "      <td>0.125478</td>\n",
       "      <td>0.330466</td>\n",
       "      <td>0.283304</td>\n",
       "      <td>0.088021</td>\n",
       "      <td>0.312882</td>\n",
       "      <td>0.096158</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.271409</td>\n",
       "      <td>18.100314</td>\n",
       "      <td>78.195610</td>\n",
       "      <td>421.537832</td>\n",
       "      <td>0.105147</td>\n",
       "      <td>0.095315</td>\n",
       "      <td>0.043317</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>0.188801</td>\n",
       "      <td>...</td>\n",
       "      <td>26.365608</td>\n",
       "      <td>84.598334</td>\n",
       "      <td>620.586067</td>\n",
       "      <td>0.146766</td>\n",
       "      <td>0.118707</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>0.291805</td>\n",
       "      <td>0.069556</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.531733</td>\n",
       "      <td>18.452486</td>\n",
       "      <td>67.227069</td>\n",
       "      <td>340.063033</td>\n",
       "      <td>0.086041</td>\n",
       "      <td>0.049961</td>\n",
       "      <td>0.049709</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>0.203093</td>\n",
       "      <td>...</td>\n",
       "      <td>24.385385</td>\n",
       "      <td>73.296855</td>\n",
       "      <td>429.675600</td>\n",
       "      <td>0.100060</td>\n",
       "      <td>0.143683</td>\n",
       "      <td>0.177225</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.287749</td>\n",
       "      <td>0.073174</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12.367686</td>\n",
       "      <td>14.399191</td>\n",
       "      <td>80.643814</td>\n",
       "      <td>460.849710</td>\n",
       "      <td>0.106410</td>\n",
       "      <td>0.101420</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>0.195326</td>\n",
       "      <td>...</td>\n",
       "      <td>19.614305</td>\n",
       "      <td>89.910502</td>\n",
       "      <td>472.323112</td>\n",
       "      <td>0.138135</td>\n",
       "      <td>0.276127</td>\n",
       "      <td>0.151098</td>\n",
       "      <td>0.074396</td>\n",
       "      <td>0.345258</td>\n",
       "      <td>0.095830</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>2561</td>\n",
       "      <td>14.383173</td>\n",
       "      <td>19.454910</td>\n",
       "      <td>100.495126</td>\n",
       "      <td>637.413375</td>\n",
       "      <td>0.076174</td>\n",
       "      <td>0.224136</td>\n",
       "      <td>0.305786</td>\n",
       "      <td>0.073760</td>\n",
       "      <td>0.168884</td>\n",
       "      <td>...</td>\n",
       "      <td>23.016513</td>\n",
       "      <td>108.867289</td>\n",
       "      <td>731.638144</td>\n",
       "      <td>0.079317</td>\n",
       "      <td>0.410666</td>\n",
       "      <td>0.674672</td>\n",
       "      <td>0.146962</td>\n",
       "      <td>0.241495</td>\n",
       "      <td>0.106978</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>2562</td>\n",
       "      <td>10.018359</td>\n",
       "      <td>18.661516</td>\n",
       "      <td>61.848450</td>\n",
       "      <td>291.512307</td>\n",
       "      <td>0.083671</td>\n",
       "      <td>0.048121</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>0.007389</td>\n",
       "      <td>0.164375</td>\n",
       "      <td>...</td>\n",
       "      <td>24.919767</td>\n",
       "      <td>67.920361</td>\n",
       "      <td>374.629250</td>\n",
       "      <td>0.129882</td>\n",
       "      <td>0.081497</td>\n",
       "      <td>0.109356</td>\n",
       "      <td>0.028243</td>\n",
       "      <td>0.252432</td>\n",
       "      <td>0.081462</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>2563</td>\n",
       "      <td>11.168036</td>\n",
       "      <td>12.651203</td>\n",
       "      <td>67.102303</td>\n",
       "      <td>376.640056</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.050181</td>\n",
       "      <td>0.017739</td>\n",
       "      <td>0.022895</td>\n",
       "      <td>0.183905</td>\n",
       "      <td>...</td>\n",
       "      <td>17.412221</td>\n",
       "      <td>73.221040</td>\n",
       "      <td>421.681446</td>\n",
       "      <td>0.135757</td>\n",
       "      <td>0.089036</td>\n",
       "      <td>0.070456</td>\n",
       "      <td>0.039851</td>\n",
       "      <td>0.295527</td>\n",
       "      <td>0.070584</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>2564</td>\n",
       "      <td>11.719646</td>\n",
       "      <td>18.933783</td>\n",
       "      <td>78.556817</td>\n",
       "      <td>426.631397</td>\n",
       "      <td>0.113492</td>\n",
       "      <td>0.091756</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>0.039025</td>\n",
       "      <td>0.203736</td>\n",
       "      <td>...</td>\n",
       "      <td>26.677691</td>\n",
       "      <td>86.572055</td>\n",
       "      <td>539.508865</td>\n",
       "      <td>0.132254</td>\n",
       "      <td>0.194688</td>\n",
       "      <td>0.179610</td>\n",
       "      <td>0.071053</td>\n",
       "      <td>0.330223</td>\n",
       "      <td>0.079510</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>2565</td>\n",
       "      <td>11.844247</td>\n",
       "      <td>20.689350</td>\n",
       "      <td>74.057823</td>\n",
       "      <td>391.547178</td>\n",
       "      <td>0.110264</td>\n",
       "      <td>0.090631</td>\n",
       "      <td>0.049565</td>\n",
       "      <td>0.019809</td>\n",
       "      <td>0.193889</td>\n",
       "      <td>...</td>\n",
       "      <td>31.513549</td>\n",
       "      <td>82.313581</td>\n",
       "      <td>468.522601</td>\n",
       "      <td>0.163906</td>\n",
       "      <td>0.168648</td>\n",
       "      <td>0.143961</td>\n",
       "      <td>0.073360</td>\n",
       "      <td>0.292210</td>\n",
       "      <td>0.091949</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  n1_radius  n1_texture  n1_perimeter     n1_area  \\\n",
       "0              0  10.855170   15.248290     69.167041  359.534878   \n",
       "1              1  10.068958   15.532758     66.130635  330.040665   \n",
       "2              2  12.271409   18.100314     78.195610  421.537832   \n",
       "3              3  10.531733   18.452486     67.227069  340.063033   \n",
       "4              4  12.367686   14.399191     80.643814  460.849710   \n",
       "...          ...        ...         ...           ...         ...   \n",
       "2561        2561  14.383173   19.454910    100.495126  637.413375   \n",
       "2562        2562  10.018359   18.661516     61.848450  291.512307   \n",
       "2563        2563  11.168036   12.651203     67.102303  376.640056   \n",
       "2564        2564  11.719646   18.933783     78.556817  426.631397   \n",
       "2565        2565  11.844247   20.689350     74.057823  391.547178   \n",
       "\n",
       "      n1_smoothness  n1_compactness  n1_concavity  n1_concave_points  \\\n",
       "0          0.105488        0.080200      0.066410           0.034194   \n",
       "1          0.099813        0.109540      0.057583           0.023322   \n",
       "2          0.105147        0.095315      0.043317           0.031539   \n",
       "3          0.086041        0.049961      0.049709           0.011046   \n",
       "4          0.106410        0.101420      0.020806           0.021990   \n",
       "...             ...             ...           ...                ...   \n",
       "2561       0.076174        0.224136      0.305786           0.073760   \n",
       "2562       0.083671        0.048121      0.028997           0.007389   \n",
       "2563       0.096154        0.050181      0.017739           0.022895   \n",
       "2564       0.113492        0.091756      0.070046           0.039025   \n",
       "2565       0.110264        0.090631      0.049565           0.019809   \n",
       "\n",
       "      n1_symmetry  ...  n3_texture  n3_perimeter     n3_area  n3_smoothness  \\\n",
       "0        0.182796  ...   18.984557     81.443134  466.879302       0.149080   \n",
       "1        0.195650  ...   22.840293     82.133171  473.367822       0.125478   \n",
       "2        0.188801  ...   26.365608     84.598334  620.586067       0.146766   \n",
       "3        0.203093  ...   24.385385     73.296855  429.675600       0.100060   \n",
       "4        0.195326  ...   19.614305     89.910502  472.323112       0.138135   \n",
       "...           ...  ...         ...           ...         ...            ...   \n",
       "2561     0.168884  ...   23.016513    108.867289  731.638144       0.079317   \n",
       "2562     0.164375  ...   24.919767     67.920361  374.629250       0.129882   \n",
       "2563     0.183905  ...   17.412221     73.221040  421.681446       0.135757   \n",
       "2564     0.203736  ...   26.677691     86.572055  539.508865       0.132254   \n",
       "2565     0.193889  ...   31.513549     82.313581  468.522601       0.163906   \n",
       "\n",
       "      n3_compactness  n3_concavity  n3_concave_points  n3_symmetry  \\\n",
       "0           0.200185      0.205695           0.111592     0.335999   \n",
       "1           0.330466      0.283304           0.088021     0.312882   \n",
       "2           0.118707      0.147900           0.050402     0.291805   \n",
       "3           0.143683      0.177225           0.028111     0.287749   \n",
       "4           0.276127      0.151098           0.074396     0.345258   \n",
       "...              ...           ...                ...          ...   \n",
       "2561        0.410666      0.674672           0.146962     0.241495   \n",
       "2562        0.081497      0.109356           0.028243     0.252432   \n",
       "2563        0.089036      0.070456           0.039851     0.295527   \n",
       "2564        0.194688      0.179610           0.071053     0.330223   \n",
       "2565        0.168648      0.143961           0.073360     0.292210   \n",
       "\n",
       "      n3_fractal_dimension  DIAGNOSIS  \n",
       "0                 0.093477          B  \n",
       "1                 0.096158          B  \n",
       "2                 0.069556          B  \n",
       "3                 0.073174          B  \n",
       "4                 0.095830          B  \n",
       "...                    ...        ...  \n",
       "2561              0.106978          B  \n",
       "2562              0.081462          B  \n",
       "2563              0.070584          B  \n",
       "2564              0.079510          B  \n",
       "2565              0.091949          B  \n",
       "\n",
       "[2566 rows x 32 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tumour_samples = pd.read_csv(\"tumour_samples.csv\") #reading from thr csv file\n",
    "df_tumour_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain matrices and arrays to work with from the dataset and standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the matrices and the arrays from the data set\n",
    "Tumour_samples = df_tumour_samples.to_numpy()\n",
    "y_tumour_samples = Tumour_samples[:,31] # creating the array with the classifications (B or M)\n",
    "N_tumour_samples = len(y_tumour_samples)\n",
    "X_tumour_samples = Tumour_samples[:,1:31] #creating a matrix storing the input data\n",
    "X_tumour_samples.astype(float)\n",
    "\n",
    "#We stamdardise\n",
    "X_tumour_samples = (X_tumour_samples-np.mean(X_tumour_samples))/np.std(X_tumour_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the euclidian dinstance. (Which could be defined also by hand, but I prefered to use numpy's built in function np.linalg.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p, q):\n",
    "    return np.linalg.norm(p-q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the KNN function. This function takes a training set (both data and measured values), a parameter $k$ and return the predictions for we want to esaminate. More precisely the algorithm works as follows:\n",
    "1. We take a 'point' x (in this case the point is a vector containing the 30 features we want to use in the prediction). We want to predict the diagnosis for this point.\n",
    "2. We analyse the k-nearest 'point' and check their diagnosis\n",
    "3. If analysing the diagnosis of this k elements we see that we have more 'B' than 'M', we 'predict' for x the diagnosis 'B' and vice-versa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is structured in quite an original way compared to the one provided in the weekly notebooks. I tried to use dictionaries and I coded in a different way to make the implementation faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_neighbours(X_train, X_test, y_train, k, return_distance=False):\n",
    "    \"\"\"k-NN\n",
    "    returns: the predictor array y_test\"\"\"\n",
    "    number_neighbours = k\n",
    "    dist = []\n",
    "    neigh_ind = []\n",
    "    strs = ['' for x in range((X_test).shape[0])]\n",
    "    y_pred = np.array(strs)\n",
    "    for j in range((X_test).shape[0]):\n",
    "        x_test = X_test[j,:]\n",
    "        D = {euclidian_distance(x_test,X_train[i,:]):y_train[i] for i in range(X_train.shape[0])} #{dictionary distance:diagnosis}\n",
    "        listfuc = [D[key] for key in sorted(D)] #sorting and creating a list with the diagnosis of the sorted distance (I this way i have a list of diagnosis, in which each diagnosis is associated to a measurement more distant from the tester than the previous one)\n",
    "        listfuc_k = listfuc[:k] #only the diagnosis of the nearest k elements\n",
    "        my_dict = {i:listfuc_k.count(i) for i in listfuc_k} #counting the number of 'B' and 'M'\n",
    "        \n",
    "        #predicting analysing the number of 'B' and 'M' on the k nearest diagnosis\n",
    "        if 'B' in my_dict:\n",
    "            if my_dict['B'] > k//2:  #if more than half are 'B' assigns 'B', otherwise 'M'. \n",
    "                y_pred[j] = 'B'\n",
    "            else:\n",
    "                y_pred[j] = 'M'\n",
    "        else:\n",
    "            y_pred[j] = 'M'\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that in tie situations we assign 'M'. This decision is made on a 'real life adaptation' idea. In fact in my opinion in this case is better to have false positive patients which can be proceed with further diagnosis, instead of false negatives patients who are not aware of their medical condition.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate we will use the mean accuracy to see the precision of the predictions. We define a function to do that, which counts the number of the 'test points' that have been classified in a correct way and divides this number by the number of the elements we want to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X_train, y_train, X_test, y_test, k):   #This function is strongly inspired by the weekly notebooks of this module\n",
    "    y_pred = k_neighbours(X_train,X_test, y_train, k=k) \n",
    "    return float(sum(y_pred==y_test))/ float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to perform the 5-fold cross-validation. Before we will divide the indexes in 5 sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the rows in X and y\n",
    "p = np.random.permutation(len(y_tumour_samples))\n",
    "X_tumour_samples = X_tumour_samples[p]\n",
    "y_tumour_samples = y_tumour_samples[p]\n",
    "\n",
    "\n",
    "# Now we divide the indices in 5 sets\n",
    "folds_indexes_tumour_samples = np.split(np.arange(len(y_tumour_samples)-1), 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is similar to the cross-validation functions we've done previusly in this coursework. We use the 'accuracy' to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score_KNN(X_train, y_train, folds, k):\n",
    "    #performs 5 cross validation and returns the average accuracy\n",
    "    \n",
    "  scores = []\n",
    "  for i in range(len(folds)):\n",
    "    val_indexes = folds[i]\n",
    "    train_indexes = list(set(range(y_train.shape[0]-1)) - set(val_indexes))\n",
    "    \n",
    "    X_train_i = X_train[train_indexes, :]\n",
    "    y_train_i = y_train[train_indexes]\n",
    "\n",
    "\n",
    "    X_val_i = X_train[val_indexes, :] \n",
    "    y_val_i = y_train[val_indexes] \n",
    "\n",
    "    score_i = score(X_train_i, y_train_i, X_val_i, y_val_i, k=k) \n",
    "    scores.append(score_i)\n",
    "\n",
    "  # Return the average score\n",
    "  return sum(scores) / len(scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_k(X_train, y_train, folds, k_range):   #This function is strongly inspired by the weekly notebooks of this module\n",
    "  k_scores = np.zeros((len(k_range),))\n",
    "  \n",
    "  for i, k in enumerate(k_range):\n",
    "    k_scores[i] = cross_validation_score_KNN(X_train, y_train, folds, k)\n",
    "    print(f'CV_ACC@k={k}: {k_scores[i]:f}')\n",
    "\n",
    "  best_k_index = np.argmax(k_scores)\n",
    "  return k_range[best_k_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_ACC@k=1: 0.959454\n",
      "CV_ACC@k=2: 0.957505\n",
      "CV_ACC@k=3: 0.962573\n",
      "CV_ACC@k=4: 0.963743\n",
      "CV_ACC@k=5: 0.959844\n",
      "CV_ACC@k=6: 0.961793\n",
      "CV_ACC@k=7: 0.960234\n",
      "CV_ACC@k=8: 0.961404\n",
      "CV_ACC@k=9: 0.960234\n",
      "CV_ACC@k=10: 0.960234\n",
      "CV_ACC@k=11: 0.959454\n",
      "CV_ACC@k=12: 0.960234\n",
      "CV_ACC@k=13: 0.958674\n",
      "CV_ACC@k=14: 0.958674\n",
      "best_k: 4\n"
     ]
    }
   ],
   "source": [
    "best_k = choose_best_k(X_tumour_samples, y_tumour_samples, folds_indexes_tumour_samples, np.arange(1, 15))\n",
    "\n",
    "print('best_k:', best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The best $k$ we get is $k=4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the accuracy vs $k$ to see how it evolves for $k$ between 3 and 10. We expect the plot to reach a peak for $k=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f975051a650>]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting accuracy vs k\n",
    "l_knn1 = [] #to store the values of the accuracy\n",
    "m_knn1 = [] #to store the values of the parameter k\n",
    "for i in range(3,10):\n",
    "    k_val = i\n",
    "    m_knn1.append(k_val)\n",
    "    l_knn1.append(cross_validation_score_KNN(X_tumour_samples, y_tumour_samples, folds_indexes_tumour_samples, k_val)) #calculating accuracy and appending it to l_knn1\n",
    "plt.plot(m_knn1,l_knn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I import the testing data set and standardising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n1_radius</th>\n",
       "      <th>n1_texture</th>\n",
       "      <th>n1_perimeter</th>\n",
       "      <th>n1_area</th>\n",
       "      <th>n1_smoothness</th>\n",
       "      <th>n1_compactness</th>\n",
       "      <th>n1_concavity</th>\n",
       "      <th>n1_concave_points</th>\n",
       "      <th>n1_symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>n3_texture</th>\n",
       "      <th>n3_perimeter</th>\n",
       "      <th>n3_area</th>\n",
       "      <th>n3_smoothness</th>\n",
       "      <th>n3_compactness</th>\n",
       "      <th>n3_concavity</th>\n",
       "      <th>n3_concave_points</th>\n",
       "      <th>n3_symmetry</th>\n",
       "      <th>n3_fractal_dimension</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.40</td>\n",
       "      <td>20.52</td>\n",
       "      <td>88.64</td>\n",
       "      <td>556.7</td>\n",
       "      <td>0.11060</td>\n",
       "      <td>0.14690</td>\n",
       "      <td>0.14450</td>\n",
       "      <td>0.08172</td>\n",
       "      <td>0.2116</td>\n",
       "      <td>...</td>\n",
       "      <td>29.66</td>\n",
       "      <td>113.30</td>\n",
       "      <td>844.4</td>\n",
       "      <td>0.15740</td>\n",
       "      <td>0.38560</td>\n",
       "      <td>0.51060</td>\n",
       "      <td>0.20510</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.11090</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.21</td>\n",
       "      <td>25.25</td>\n",
       "      <td>84.10</td>\n",
       "      <td>537.9</td>\n",
       "      <td>0.08791</td>\n",
       "      <td>0.05205</td>\n",
       "      <td>0.02772</td>\n",
       "      <td>0.02068</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>...</td>\n",
       "      <td>34.23</td>\n",
       "      <td>91.29</td>\n",
       "      <td>632.9</td>\n",
       "      <td>0.12890</td>\n",
       "      <td>0.10630</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.06005</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.06788</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14.02</td>\n",
       "      <td>15.66</td>\n",
       "      <td>89.59</td>\n",
       "      <td>606.5</td>\n",
       "      <td>0.07966</td>\n",
       "      <td>0.05581</td>\n",
       "      <td>0.02087</td>\n",
       "      <td>0.02652</td>\n",
       "      <td>0.1589</td>\n",
       "      <td>...</td>\n",
       "      <td>19.31</td>\n",
       "      <td>96.53</td>\n",
       "      <td>688.9</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.06260</td>\n",
       "      <td>0.08216</td>\n",
       "      <td>0.2136</td>\n",
       "      <td>0.06710</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>14.26</td>\n",
       "      <td>18.17</td>\n",
       "      <td>91.22</td>\n",
       "      <td>633.1</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>0.05220</td>\n",
       "      <td>0.02475</td>\n",
       "      <td>0.01374</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>...</td>\n",
       "      <td>25.26</td>\n",
       "      <td>105.80</td>\n",
       "      <td>819.7</td>\n",
       "      <td>0.09445</td>\n",
       "      <td>0.21670</td>\n",
       "      <td>0.15650</td>\n",
       "      <td>0.07530</td>\n",
       "      <td>0.2636</td>\n",
       "      <td>0.07676</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>13.03</td>\n",
       "      <td>18.42</td>\n",
       "      <td>82.61</td>\n",
       "      <td>523.8</td>\n",
       "      <td>0.08983</td>\n",
       "      <td>0.03766</td>\n",
       "      <td>0.02562</td>\n",
       "      <td>0.02923</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>...</td>\n",
       "      <td>22.81</td>\n",
       "      <td>84.46</td>\n",
       "      <td>545.9</td>\n",
       "      <td>0.09701</td>\n",
       "      <td>0.04619</td>\n",
       "      <td>0.04833</td>\n",
       "      <td>0.05013</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.06169</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>19.73</td>\n",
       "      <td>19.82</td>\n",
       "      <td>130.70</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>0.10620</td>\n",
       "      <td>0.18490</td>\n",
       "      <td>0.24170</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.1733</td>\n",
       "      <td>...</td>\n",
       "      <td>25.59</td>\n",
       "      <td>159.80</td>\n",
       "      <td>1933.0</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.59550</td>\n",
       "      <td>0.84890</td>\n",
       "      <td>0.25070</td>\n",
       "      <td>0.2749</td>\n",
       "      <td>0.12970</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>23.29</td>\n",
       "      <td>26.67</td>\n",
       "      <td>158.90</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.20840</td>\n",
       "      <td>0.35230</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>...</td>\n",
       "      <td>32.68</td>\n",
       "      <td>177.00</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>0.15360</td>\n",
       "      <td>0.41670</td>\n",
       "      <td>0.78920</td>\n",
       "      <td>0.27330</td>\n",
       "      <td>0.3198</td>\n",
       "      <td>0.08762</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>13.20</td>\n",
       "      <td>17.43</td>\n",
       "      <td>84.13</td>\n",
       "      <td>541.6</td>\n",
       "      <td>0.07215</td>\n",
       "      <td>0.04524</td>\n",
       "      <td>0.04336</td>\n",
       "      <td>0.01105</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>...</td>\n",
       "      <td>27.82</td>\n",
       "      <td>88.28</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.11010</td>\n",
       "      <td>0.15080</td>\n",
       "      <td>0.22980</td>\n",
       "      <td>0.04970</td>\n",
       "      <td>0.2767</td>\n",
       "      <td>0.07198</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>13.64</td>\n",
       "      <td>15.60</td>\n",
       "      <td>87.38</td>\n",
       "      <td>575.3</td>\n",
       "      <td>0.09423</td>\n",
       "      <td>0.06630</td>\n",
       "      <td>0.04705</td>\n",
       "      <td>0.03731</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>...</td>\n",
       "      <td>19.05</td>\n",
       "      <td>94.11</td>\n",
       "      <td>683.4</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.12910</td>\n",
       "      <td>0.15330</td>\n",
       "      <td>0.09222</td>\n",
       "      <td>0.2530</td>\n",
       "      <td>0.06510</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>14.99</td>\n",
       "      <td>25.20</td>\n",
       "      <td>95.54</td>\n",
       "      <td>698.8</td>\n",
       "      <td>0.09387</td>\n",
       "      <td>0.05131</td>\n",
       "      <td>0.02398</td>\n",
       "      <td>0.02899</td>\n",
       "      <td>0.1565</td>\n",
       "      <td>...</td>\n",
       "      <td>25.20</td>\n",
       "      <td>95.54</td>\n",
       "      <td>698.8</td>\n",
       "      <td>0.09387</td>\n",
       "      <td>0.05131</td>\n",
       "      <td>0.02398</td>\n",
       "      <td>0.02899</td>\n",
       "      <td>0.1565</td>\n",
       "      <td>0.05504</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  n1_radius  n1_texture  n1_perimeter  n1_area  n1_smoothness  \\\n",
       "0             0      13.40       20.52         88.64    556.7        0.11060   \n",
       "1             1      13.21       25.25         84.10    537.9        0.08791   \n",
       "2             2      14.02       15.66         89.59    606.5        0.07966   \n",
       "3             3      14.26       18.17         91.22    633.1        0.06576   \n",
       "4             4      13.03       18.42         82.61    523.8        0.08983   \n",
       "..          ...        ...         ...           ...      ...            ...   \n",
       "195         195      19.73       19.82        130.70   1206.0        0.10620   \n",
       "196         196      23.29       26.67        158.90   1685.0        0.11410   \n",
       "197         197      13.20       17.43         84.13    541.6        0.07215   \n",
       "198         198      13.64       15.60         87.38    575.3        0.09423   \n",
       "199         199      14.99       25.20         95.54    698.8        0.09387   \n",
       "\n",
       "     n1_compactness  n1_concavity  n1_concave_points  n1_symmetry  ...  \\\n",
       "0           0.14690       0.14450            0.08172       0.2116  ...   \n",
       "1           0.05205       0.02772            0.02068       0.1619  ...   \n",
       "2           0.05581       0.02087            0.02652       0.1589  ...   \n",
       "3           0.05220       0.02475            0.01374       0.1635  ...   \n",
       "4           0.03766       0.02562            0.02923       0.1467  ...   \n",
       "..              ...           ...                ...          ...  ...   \n",
       "195         0.18490       0.24170            0.09740       0.1733  ...   \n",
       "196         0.20840       0.35230            0.16200       0.2200  ...   \n",
       "197         0.04524       0.04336            0.01105       0.1487  ...   \n",
       "198         0.06630       0.04705            0.03731       0.1717  ...   \n",
       "199         0.05131       0.02398            0.02899       0.1565  ...   \n",
       "\n",
       "     n3_texture  n3_perimeter  n3_area  n3_smoothness  n3_compactness  \\\n",
       "0         29.66        113.30    844.4        0.15740         0.38560   \n",
       "1         34.23         91.29    632.9        0.12890         0.10630   \n",
       "2         19.31         96.53    688.9        0.10340         0.10170   \n",
       "3         25.26        105.80    819.7        0.09445         0.21670   \n",
       "4         22.81         84.46    545.9        0.09701         0.04619   \n",
       "..          ...           ...      ...            ...             ...   \n",
       "195       25.59        159.80   1933.0        0.17100         0.59550   \n",
       "196       32.68        177.00   1986.0        0.15360         0.41670   \n",
       "197       27.82         88.28    602.0        0.11010         0.15080   \n",
       "198       19.05         94.11    683.4        0.12780         0.12910   \n",
       "199       25.20         95.54    698.8        0.09387         0.05131   \n",
       "\n",
       "     n3_concavity  n3_concave_points  n3_symmetry  n3_fractal_dimension  \\\n",
       "0         0.51060            0.20510       0.3585               0.11090   \n",
       "1         0.13900            0.06005       0.2444               0.06788   \n",
       "2         0.06260            0.08216       0.2136               0.06710   \n",
       "3         0.15650            0.07530       0.2636               0.07676   \n",
       "4         0.04833            0.05013       0.1987               0.06169   \n",
       "..            ...                ...          ...                   ...   \n",
       "195       0.84890            0.25070       0.2749               0.12970   \n",
       "196       0.78920            0.27330       0.3198               0.08762   \n",
       "197       0.22980            0.04970       0.2767               0.07198   \n",
       "198       0.15330            0.09222       0.2530               0.06510   \n",
       "199       0.02398            0.02899       0.1565               0.05504   \n",
       "\n",
       "     DIAGNOSIS  \n",
       "0            M  \n",
       "1            B  \n",
       "2            B  \n",
       "3            B  \n",
       "4            B  \n",
       "..         ...  \n",
       "195          M  \n",
       "196          M  \n",
       "197          B  \n",
       "198          B  \n",
       "199          M  \n",
       "\n",
       "[200 rows x 32 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_tumour_test = pd.read_csv(\"tumour_test.csv\") #reading from thr csv file\n",
    "df_tumour_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Obtaining the matrices and the arrays from the data set\n",
    "Tumour_test = df_tumour_test.to_numpy()\n",
    "y_tumour_test = Tumour_test[:,31] # creating the array with the classifications (B or M)\n",
    "N_tumour_test = len(y_tumour_test)\n",
    "X_tumour_test = Tumour_test[:,1:31] #creating a matrix storing the input data\n",
    "X_tumour_test.astype(float)\n",
    "\n",
    "#I standardise\n",
    "\n",
    "X_tumour_test = (X_tumour_test-np.mean(X_tumour_test))/np.std(X_tumour_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtain the list of the predictions to see what result we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_testing_set = k_neighbours(X_tumour_samples, X_tumour_test, y_tumour_samples, k=4, return_distance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'M'\n",
      " 'M' 'M' 'M' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'B' 'M' 'B' 'M'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'B' 'B' 'M' 'B' 'M' 'M' 'M' 'B' 'B' 'M'\n",
      " 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'B' 'M' 'M'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'M' 'B'\n",
      " 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B'\n",
      " 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'M' 'B'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B'\n",
      " 'B' 'M' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B'\n",
      " 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'M' 'M' 'B'\n",
      " 'B' 'B']\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the accuracy for the data set $\\texttt{tumour_samples.csv}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746687451286048"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(X_tumour_samples, y_tumour_samples, X_tumour_samples, y_tumour_samples, k = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the accuracy for the data set $\\texttt{tumour_test.csv}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(X_tumour_samples, y_tumour_samples, X_tumour_test, y_tumour_test, k = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that our predictions are better for the data set $\\texttt{tumour_samples.csv}$ (we have $0.9746687451286048$, which is normal, because is the data set I used to build the model.\n",
    "However the accuracy for data set $\\texttt{tumour_test.csv}$ is $0.88$, which is good. We can say that our model can be quite generalised and used for prediction.\n",
    "\n",
    "It is important to note however that depending on the circumstances this decrease of 'precision' on the testing data  could not be tolerated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCPlajQOy1ER"
   },
   "source": [
    "<a name=\"q22\"></a>\n",
    "\n",
    "## 2.2: Random forest [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1\n",
    "In this task we use a 5-cross fold validation to find the combination of optimal parameters (Max Depth of trees, Number of Trees) for the random forest method. To implement this method we use decision trees, that we use in order to classify the data. In particular we:\n",
    "1) Perform bootstrapping on the given training data-set, so we create B sub-samples, from which we obtain a decision tree for each of these. Each tree is used to build a probability vector\n",
    "\n",
    "2) Then averaging the probability vectors obtained from the different subsets we obtain a final probability vector.\n",
    "\n",
    "3) Now we can proceed with our classification, classifing each sample in the class corresponding to the higher probability.\n",
    "\n",
    "It is also important to say that we will proceed employing cross entropy minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are taken and adapted from the weekly jupyter notebooks provided as study material during this module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to use weighted samples so we take a vector of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1588,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights_basic = np.ones(len(y_tumour_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1691,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE_index(y, sample_weights=None):    #slight modification of the weekly notebooks functions\n",
    "  \"\"\"  \n",
    "  Calculate the cross entropy-index for labels.\n",
    "  Arguments:\n",
    "      y: array of the training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  Returns:\n",
    "      (float): the cross entropy-index for y.\n",
    "  \"\"\"\n",
    "\n",
    "  # We count different labels in yÔºåand then we store them in label_weights\n",
    "  # initialize with zero for each distinct label.\n",
    "  label_weights = {yi: 0 for yi in set(y)}  #we define a dictionary\n",
    "  for yi, wi in zip(y, sample_weights):\n",
    "      label_weights[yi] += wi\n",
    "\n",
    "  total_weight = sum(label_weights.values())\n",
    "  CE = 0 \n",
    "  for label, weight in label_weights.items():\n",
    "      CE -= (weight / total_weight)*np.log((weight / total_weight))\n",
    "\n",
    "  return CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1692,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_samples(X, y, sample_weights, column, value, categorical):  #function taken from the weekly notebooks\n",
    "  \"\"\"\n",
    "  Return the split of data whose column-th feature:\n",
    "    1. equals value, in case `column` is categorical, or\n",
    "    2. less than value, in case `column` is not categorical (i.e. numerical)\n",
    "\n",
    "  Arguments:\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      column: the column of the feature for splitting.\n",
    "      value: splitting threshold  the samples \n",
    "      categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
    "  Returns:\n",
    "      tuple(np.array, np.array): tuple of subsets of X splitted based on column-th value.\n",
    "      tuple(np.array, np.array): tuple of subsets of y splitted based on column-th value.\n",
    "      tuple(np.array, np.array): tuple of subsets of sample weights based on column-th value.\n",
    "  \"\"\" \n",
    "\n",
    "  if categorical:\n",
    "    left_mask =(X[:, column] == value)\n",
    "  else:\n",
    "    left_mask = (X[:, column] < value)\n",
    "  X_left, X_right = X[left_mask, :], X[~left_mask, :]\n",
    "  y_left, y_right = y[left_mask], y[~left_mask]\n",
    "  w_left, w_right  = sample_weights[left_mask], sample_weights[~left_mask]\n",
    "  return (X_left, X_right), (y_left, y_right), (w_left, w_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE_split_value(X, y, sample_weights, column, categorical):\n",
    "  \"\"\"\n",
    "  Calculate the cross entropy -index based on `column` with the split that minimizes the cross entropy -index.\n",
    "  Arguments:\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      column: the column of the feature for calculating. 0 <= column < D\n",
    "      categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
    "  Returns:\n",
    "      (float, float): the resulted cross entropy -index and the corresponding value used in splitting.\n",
    "  \"\"\"\n",
    "  \n",
    "  unique_vals = np.unique(X[:, column])\n",
    "\n",
    "  assert len(unique_vals) > 1, f\"There must be more than one distinct feature value. Given: {unique_vals}.\"\n",
    "\n",
    "  CE_index_val, threshold = np.inf, None\n",
    "  \n",
    "  # split the values of i-th feature and calculate the cost \n",
    "  for value in unique_vals:\n",
    "    (X_l, X_r), (y_l, y_r), (w_l, w_r) = split_samples(X, y, sample_weights, column, value, categorical) \n",
    "\n",
    "    # if one of the two sides is empty, skip this split.\n",
    "    if len(y_l) == 0 or len(y_r) == 0:\n",
    "      continue\n",
    "    \n",
    "    p_left = sum(w_l)/(sum(w_l) + sum(w_r))\n",
    "    p_right = 1 - p_left\n",
    "    new_cost = p_left * CE_index(y_l, w_l) + p_right * CE_index(y_r, w_r) \n",
    "    if new_cost < CE_index_val:\n",
    "      CE_index_val, threshold = new_cost, value\n",
    "    \n",
    "  return CE_index_val, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41489691189061656, -0.279838916623836)"
      ]
     },
     "execution_count": 2228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE_split_value(X_tumour_samples,y_tumour_samples,sample_weights_basic,column = 4, categorical = False) #just verifying that everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE_split_rf(n_features, X, y, sample_weights, columns_dict):\n",
    "  \"\"\"\n",
    "  Choose the best feature to split according to criterion.\n",
    "  Args:\n",
    "      n_features: number of sampled features.\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "  Returns:\n",
    "      (float, int, float): the minimized cross entropy-index, the best feature index and value used in splitting.\n",
    "  \"\"\"\n",
    "  columns = np.random.choice(list(columns_dict.keys()), n_features, replace=False)\n",
    "  columns_dict = {c: columns_dict[c] for c in columns}\n",
    "\n",
    "  min_CE_index, split_column, split_val = np.inf, 0, 0\n",
    "  for column, categorical in columns_dict.items():\n",
    "    # skip column if samples are not seperable by that column.\n",
    "    if len(np.unique(X[:, column])) < 2:\n",
    "      continue\n",
    "\n",
    "    # search for the best splitting value for the given column.\n",
    "    CE_index, val = CE_split_value(X, y, sample_weights, column, categorical)    \n",
    "    if CE_index < min_CE_index:\n",
    "        min_CE_index, split_column, split_val = CE_index, column, val\n",
    "\n",
    "  return min_CE_index, split_column, split_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we are considering are all numerical, so we will take a dictionary containing all 'false'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2230,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_categorical = {index: False for index in range(X_tumour_samples.shape[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18150069741187327, 23, 4.8793197669441595)"
      ]
     },
     "execution_count": 2231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE_split_rf(10, X_tumour_samples,y_tumour_samples,sample_weights_basic,dictionary_categorical)  #just verifying that everything works\n",
    "#n_features, X, y, sample_weights, columns_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1698,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(y, sample_weights):\n",
    "  \"\"\"\n",
    "  Return the label which appears the most in y.\n",
    "  Args:\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  Returns:\n",
    "      (int): the majority label\n",
    "  \"\"\"\n",
    "  majority_label = {yi: 0 for yi in set(y)}\n",
    "\n",
    "  for yi, wi in zip(y, sample_weights):\n",
    "    majority_label[yi] += wi\n",
    "  return max(majority_label, key=majority_label.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building tree function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1699,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_rf(n_features, X, y, sample_weights, columns_dict, depth,  max_depth=10, min_samples_leaf=2):\n",
    "  \"\"\"Build the decision tree according to the data.\n",
    "  Args:\n",
    "      X: (np.array) training features, of shape (N, D).\n",
    "      y: (np.array) vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "      columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "      feature_names (list): record the name of features in X in the original dataset.\n",
    "      depth (int): current depth for this node.\n",
    "  Returns:\n",
    "      (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
    "        1. 'feature_name': The column name of the split.\n",
    "        2. 'feature_index': The column index of the split.\n",
    "        3. 'value': The value used for the split.\n",
    "        4. 'categorical': indicator for categorical/numerical variables.\n",
    "        5. 'majority_label': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
    "        6. 'left': The left sub-tree with the same structure.\n",
    "        7. 'right' The right sub-tree with the same structure.\n",
    "      Example:\n",
    "          mytree = {\n",
    "              'feature_name': 'petal length (cm)',\n",
    "              'feature_index': 2,\n",
    "              'value': 3.0,\n",
    "              'categorical': False,\n",
    "              'majority_label': None,\n",
    "              'left': {\n",
    "                  'feature_name': str,\n",
    "                  'feature_index': int,\n",
    "                  'value': float,\n",
    "                  'categorical': bool,\n",
    "                  'majority_label': None,\n",
    "                  'left': {..etc.},\n",
    "                  'right': {..etc.}\n",
    "              }\n",
    "              'right': {\n",
    "                  'feature_name': str,\n",
    "                  'feature_index': int,\n",
    "                  'value': float,\n",
    "                  'categorical': bool,\n",
    "                  'majority_label': None,\n",
    "                  'left': {..etc.},\n",
    "                  'right': {..etc.}\n",
    "              }\n",
    "          }\n",
    "  \"\"\"\n",
    "  # include a clause for the cases where (i) all lables are the same, (ii) depth exceed (iii) X is too small\n",
    "  if len(np.unique(y)) == 1 or depth>=max_depth or len(X)<=min_samples_leaf: \n",
    "      return {'majority_label': majority_vote(y, sample_weights)}\n",
    "  \n",
    "  else:\n",
    "    CE, split_index, split_val = CE_split_rf(n_features, X, y, sample_weights, columns_dict)\n",
    "    \n",
    "    # If CE is infinity, it means that samples are not seperable by the sampled features.\n",
    "    if CE == np.inf:\n",
    "      return {'majority_label': majority_vote(y, sample_weights)}\n",
    "    categorical = columns_dict[split_index]\n",
    "    (X_l, X_r), (y_l, y_r), (w_l, w_r) = split_samples(X, y, sample_weights, split_index, split_val, categorical) \n",
    "    return {\n",
    "        'feature_index': split_index,\n",
    "        'value': split_val,\n",
    "        'categorical': categorical,\n",
    "        'majority_label': None,\n",
    "        'left': build_tree_rf(n_features, X_l, y_l, w_l, columns_dict, depth + 1, max_depth, min_samples_leaf),\n",
    "        'right': build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, depth + 1, max_depth, min_samples_leaf)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1700,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(B, max_depth, n_features, X, y,  columns_dict, sample_weights=None):\n",
    "  \"\"\"\n",
    "  Build the decision tree according to the training data.\n",
    "  Args:\n",
    "      B: number of decision trees.\n",
    "      max_depth: max depth allowed for the trees\n",
    "      X: (pd.Dataframe) training features, of shape (N, D). Each X[i] is a training sample.\n",
    "      y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
    "      an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
    "      columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  \"\"\"\n",
    "  if sample_weights is None:\n",
    "      # if the sample weights is not provided, we assume the samples have uniform weights\n",
    "      sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
    "  else:\n",
    "      sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n",
    "\n",
    "  N = X.shape[0]\n",
    "  training_indices = np.arange(N)\n",
    "  trees = []\n",
    "\n",
    "  for _ in range(B):\n",
    "    sample = np.random.choice(training_indices, N, replace=True)\n",
    "    X_sample = X[sample, :]\n",
    "    y_sample = y[sample]\n",
    "    w_sample = sample_weights[sample]\n",
    "    tree = build_tree_rf(n_features, X_sample, y_sample, w_sample, columns_dict, depth=1,  max_depth=max_depth)\n",
    "    trees.append(tree)\n",
    "\n",
    "  return trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1701,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x):\n",
    "  \"\"\"\n",
    "  Classify a single sample with the fitted decision tree.\n",
    "  Args:\n",
    "      x: ((pd.Dataframe) a single sample features, of shape (D,).\n",
    "  Returns:\n",
    "      (int): predicted testing sample label.\n",
    "  \"\"\"\n",
    "  if tree['majority_label'] is not None: \n",
    "    return tree['majority_label']\n",
    "\n",
    "  elif tree['categorical']:\n",
    "    if x[tree['feature_index']] == tree['value']:\n",
    "      return classify(tree['left'], x)\n",
    "    else:\n",
    "      return classify(tree['right'], x)\n",
    "\n",
    "  else:\n",
    "    if x[tree['feature_index']] < tree['value']:\n",
    "      return classify(tree['left'], x)\n",
    "    else:\n",
    "      return classify(tree['right'], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rf(rf, X):\n",
    "  \"\"\"\n",
    "  Predict classification results for X.\n",
    "  Args:\n",
    "      rf: A trained random forest through train_rf function.\n",
    "      X: (pd.Dataframe) testing sample features, of shape (N, D).\n",
    "  Returns:\n",
    "      (np.array): predicted testing sample labels, of shape (N,).\n",
    "  \"\"\"\n",
    "\n",
    "  def aggregate(decisions):\n",
    "    count = defaultdict(int)\n",
    "    for decision in decisions:\n",
    "      count[decision] += 1\n",
    "    return max(count, key=count.get)\n",
    "\n",
    "  if len(X.shape) == 1:\n",
    "      return aggregate([classify(tree, X) for tree in rf])\n",
    "  else:\n",
    "      return np.array([aggregate([classify(tree, x) for tree in rf]) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.340e+01, 2.052e+01, 8.864e+01, ..., 2.051e-01, 3.585e-01,\n",
       "        1.109e-01],\n",
       "       [1.321e+01, 2.525e+01, 8.410e+01, ..., 6.005e-02, 2.444e-01,\n",
       "        6.788e-02],\n",
       "       [1.402e+01, 1.566e+01, 8.959e+01, ..., 8.216e-02, 2.136e-01,\n",
       "        6.710e-02],\n",
       "       ...,\n",
       "       [1.320e+01, 1.743e+01, 8.413e+01, ..., 4.970e-02, 2.767e-01,\n",
       "        7.198e-02],\n",
       "       [1.364e+01, 1.560e+01, 8.738e+01, ..., 9.222e-02, 2.530e-01,\n",
       "        6.510e-02],\n",
       "       [1.499e+01, 2.520e+01, 9.554e+01, ..., 2.899e-02, 1.565e-01,\n",
       "        5.504e-02]])"
      ]
     },
     "execution_count": 1703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "Tumour_samples = df_tumour_samples.to_numpy()\n",
    "y_tumour_samples = Tumour_samples[:,31] # creating the array with the classifications (B or M)\n",
    "N_tumour_samples = len(y_tumour_samples)\n",
    "X_tumour_samples = Tumour_samples[:,1:31] #creating a matrix storing the input data\n",
    "\n",
    "\n",
    "#Training\n",
    "Tumour_test = df_tumour_test.to_numpy()\n",
    "y_tumour_test = Tumour_test[:,31] # creating the array with the classifications (B or M)\n",
    "N_tumour_test = len(y_tumour_test)\n",
    "X_tumour_test = Tumour_test[:,1:31] #creating a matrix storing the input data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1704,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_tumour_samples.shape[1] // 3\n",
    "B = 3\n",
    "# fit the random forest with training data\n",
    "rf = train_rf(B,25, n_features, X_tumour_samples, y_tumour_samples, dictionary_categorical)   #training just verifying that everything works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1706,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_score(rf, X_test, y_test):\n",
    "  y_pred = predict_rf(rf, X_test) \n",
    "  return np.mean(y_pred==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1759,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score_RF(X_train, y_train, folds, Number_of_trees, max_depth):\n",
    "    #performs 5 cross validation and returns the average accuracy\n",
    "  scores = []\n",
    "  for i in range(len(folds)):\n",
    "    val_indexes = folds[i]\n",
    "    train_indexes = list(set(range(y_train.shape[0]-1)) - set(val_indexes))\n",
    "    \n",
    "    X_train_i = X_train[train_indexes, :]\n",
    "    y_train_i = y_train[train_indexes]\n",
    "\n",
    "\n",
    "    X_val_i = X_train[val_indexes, :] \n",
    "    y_val_i = y_train[val_indexes] \n",
    "    \n",
    "    \n",
    "    n_features = 10\n",
    "\n",
    "    rf = train_rf(Number_of_trees, max_depth, n_features, X_train_i, y_train_i, dictionary_categorical , sample_weights=None) \n",
    "    scorerf = rf_score(rf,X_val_i,y_val_i)\n",
    "    scores.append(scorerf)\n",
    "\n",
    "  # Return the average score\n",
    "  return np.mean(scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the hyperparameter combinations we will use, which will be: [(5, 1), (5, 3), (5, 5), (10, 1), (10, 3), (10, 5), (15, 1), (15, 3), (15, 5)], where the first value represent the max depth of the trees and the second the number of the trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1782,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5. 10. 15.]\n",
      "[(5.0, 1.0), (5.0, 3.0), (5.0, 5.0), (10.0, 1.0), (10.0, 3.0), (10.0, 5.0), (15.0, 1.0), (15.0, 3.0), (15.0, 5.0)]\n"
     ]
    }
   ],
   "source": [
    "list_of_max_depths = np.linspace(5,15,3)\n",
    "print(list_of_max_depths)\n",
    "\n",
    "list_of_number_of_trees = np.linspace(1,5,3)\n",
    "\n",
    "list_of_hyperparameters = []\n",
    "for i in itertools.product(list_of_max_depths,list_of_number_of_trees):\n",
    "    list_of_hyperparameters.append(i)\n",
    "    \n",
    "print(list_of_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1784,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_hp(X_train, y_train, folds, list_of_hyperparameters):   #This function is strongly inspired by the weekly notebooks of this module\n",
    "  hyperparameter_scores = np.zeros((len(list_of_hyperparameters),))\n",
    "  \n",
    "  for i, hypcomb in enumerate(list_of_hyperparameters):\n",
    "\n",
    "    hyperparameter_scores[i] = cross_validation_score_RF(X_train, y_train, folds, int(hypcomb[1]), int(hypcomb[0]))\n",
    "    \n",
    "    print(f'hyperparameters={hypcomb}: {hyperparameter_scores[i]:f}')\n",
    "\n",
    "  best_hyperparameter_index = np.argmax(hyperparameter_scores)\n",
    "  return list_of_hyperparameters[best_hyperparameter_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1785,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters=(5.0, 1.0): 0.967251\n",
      "hyperparameters=(5.0, 3.0): 0.977778\n",
      "hyperparameters=(5.0, 5.0): 0.977388\n",
      "hyperparameters=(10.0, 1.0): 0.968811\n",
      "hyperparameters=(10.0, 3.0): 0.979727\n",
      "hyperparameters=(10.0, 5.0): 0.982846\n",
      "hyperparameters=(15.0, 1.0): 0.969201\n",
      "hyperparameters=(15.0, 3.0): 0.982456\n",
      "hyperparameters=(15.0, 5.0): 0.985575\n",
      "best couple of parameters: (15.0, 5.0)\n"
     ]
    }
   ],
   "source": [
    "best_parameters = choose_best_hp(X_tumour_samples, y_tumour_samples, folds_indexes_tumour_samples,list_of_hyperparameters)\n",
    "\n",
    "print('best couple of parameters:', best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters=(5.0, 1.0): 0.967251\n",
    "hyperparameters=(5.0, 3.0): 0.977778\n",
    "hyperparameters=(5.0, 5.0): 0.977388\n",
    "hyperparameters=(10.0, 1.0): 0.968811\n",
    "hyperparameters=(10.0, 3.0): 0.979727\n",
    "hyperparameters=(10.0, 5.0): 0.982846\n",
    "hyperparameters=(15.0, 1.0): 0.969201\n",
    "hyperparameters=(15.0, 3.0): 0.982456\n",
    "hyperparameters=(15.0, 5.0): 0.985575\n",
    "best couple of parameters: (15.0, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained that the couple with the greatest number of trees and the highest max depth is the best, this however is odd. This is why we will implement cross validation some other times to see what happens for greater numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1786,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5. 10. 15.]\n",
      "[(25.0, 3.0)]\n"
     ]
    }
   ],
   "source": [
    "list_of_max_depths2 = np.linspace(25,25,1)\n",
    "print(list_of_max_depths)\n",
    "\n",
    "list_of_number_of_trees2 = np.linspace(3,3,1)\n",
    "\n",
    "list_of_hyperparameters2 = []\n",
    "for i in itertools.product(list_of_max_depths2,list_of_number_of_trees2):\n",
    "    list_of_hyperparameters2.append(i)\n",
    "    \n",
    "print(list_of_hyperparameters2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1787,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters=(25.0, 3.0): 0.981287\n",
      "best couple of parameters: (25.0, 3.0)\n"
     ]
    }
   ],
   "source": [
    "best_parameters2 = choose_best_hp(X_tumour_samples, y_tumour_samples, folds_indexes_tumour_samples,list_of_hyperparameters2)\n",
    "\n",
    "print('best couple of parameters:', best_parameters2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that $(25,3)$ is not better than $(15,3)$, but we can investigate further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5. 10. 15.]\n",
      "[(25.0, 3.0), (30.0, 3.0)]\n"
     ]
    }
   ],
   "source": [
    "list_of_max_depths3 = np.linspace(25,30,2)\n",
    "print(list_of_max_depths)\n",
    "\n",
    "list_of_number_of_trees3 = np.linspace(6,10,2)\n",
    "\n",
    "list_of_hyperparameters3 = []\n",
    "for i in itertools.product(list_of_max_depths3,list_of_number_of_trees2):\n",
    "    list_of_hyperparameters3.append(i)\n",
    "    \n",
    "print(list_of_hyperparameters3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1790,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters=(25.0, 3.0): 0.980507\n",
      "hyperparameters=(30.0, 3.0): 0.979337\n",
      "best couple of parameters: (25.0, 3.0)\n"
     ]
    }
   ],
   "source": [
    "best_parameters3 = choose_best_hp(X_tumour_samples, y_tumour_samples, folds_indexes_tumour_samples,list_of_hyperparameters3)\n",
    "\n",
    "print('best couple of parameters:', best_parameters2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it clear that (25,3) and (30,3), despite having a greater possible depth are not better couples than some of the already explored ones.\n",
    "We could investigate further but the computational cost of this algorithm makes it slow and so it requires to time. This is the reason we will stick with the best combination of hyperparameters we have obtained until now, which is (15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best couple we got is (Max Depth,Number of Trees) = (15,5). We train our model with these 2 parameters. We will give as number of features the value 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1807,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_optimal = train_rf(5,15,10, X_tumour_samples, y_tumour_samples, dictionary_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988308651597818"
      ]
     },
     "execution_count": 1808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_score(rf_optimal,X_tumour_samples,y_tumour_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 1809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_score(rf_optimal,X_tumour_test,y_tumour_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy is $0.9988308651597818$ on the training set and $0.975$ on the testing set, so the model provides a really good result.\n",
    "\n",
    "We can do a better analysis employing confusion matrices. To do so we will need to slightly modify the accuracy function. (We will take 'B' as negative and 'M' as positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1837,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_score_CM(rf, X_test, y_test):\n",
    "  y_pred = predict_rf(rf, X_test)\n",
    "  N = len(y_test)\n",
    "  TP = 0 #True positives\n",
    "  TN = 0 #True negatives\n",
    "  FP = 0 #False positives\n",
    "  FN = 0 #False negatives\n",
    "  for j in range(N):\n",
    "    if y_pred[j] == y_test[j] and y_pred[j]== 'B': \n",
    "            TN+=1\n",
    "    elif y_pred[j] == y_test[j] and y_pred[j] == 'M':\n",
    "            TP+=1\n",
    "    elif y_pred[j] != y_test[j] and y_pred[j] == 'M':\n",
    "            FP+=1\n",
    "    elif y_pred[j] != y_test[j] and y_pred[j] == 'B':\n",
    "            FN+=1\n",
    "            \n",
    "  return np.mean(y_pred==y_test),TP,TN,FP,FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate true positives, true negatives, false positives, false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1838,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9980514419329696 420 2141 1 4\n"
     ]
    }
   ],
   "source": [
    "rf_accuracy_samples, TP_1,TN_1,FP_1,FN_1 = rf_score_CM(rf, X_tumour_samples, y_tumour_samples)\n",
    "print(rf_accuracy_samples, TP_1,TN_1,FP_1,FN_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1839,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98 69 127 0 4\n"
     ]
    }
   ],
   "source": [
    "rf_accuracy_test, TP_2,TN_2,FP_2,FN_2 = rf_score_CM(rf, X_tumour_test, y_tumour_test)\n",
    "print(rf_accuracy_test, TP_2,TN_2,FP_2,FN_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on the training samples we have 420 true positive, 2141 true negatives, 1 false positives and 4 false negatives\n",
    "\n",
    "On the testing data set we have 69 true positive, 127 true negatives, 0 false positives and 4 false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build the F-Score, so we need the $precision=\\frac{TP}{TP+FP}$, the $recall = \\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1851,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for the training set = 0.997624703087886\n",
      "precision for the testing set = 1.0\n",
      "recall for the training set = 0.9905660377358491\n",
      "recall for the testing set = 0.9452054794520548\n"
     ]
    }
   ],
   "source": [
    "#precision for the the training set \n",
    "precision1 = TP_1/(TP_1+FP_1)\n",
    "#precision for the the test set \n",
    "precision2 = TP_2/(TP_2+FP_2)\n",
    "#recall for the the training set \n",
    "recall1 = TP_1/(TP_1+FN_1)\n",
    "#recall for the the testing set \n",
    "recall2 = TP_2/(TP_2+FN_2)\n",
    "print('precision for the training set =',precision1)\n",
    "print('precision for the testing set =',precision2)\n",
    "print('recall for the training set =',recall1)\n",
    "print('recall for the testing set =',recall2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the precision of the testing set is 1, which is due to the fact that applying the prediction to the testing dataset we get 0 false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1850,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F score for the training dataset =  0.9940828402366864\n",
      "F score for the testing dataset =  0.971830985915493\n"
     ]
    }
   ],
   "source": [
    "F1 = 2*(precision1*recall1)/(precision1+recall1)\n",
    "F2 = 2*(precision2*recall2)/(precision2+recall2)\n",
    "print('F score for the training dataset = ',F1)\n",
    "print('F score for the testing dataset = ',F2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-score is a way to measure accuracy. It consists on the harmonic mean of recall and precision and is greater for the training dataset, but still it gives a good result for the testing dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfN9uz1qy1L4"
   },
   "source": [
    "<a name=\"q23\"></a>\n",
    "\n",
    "## 2.3: Support vector machine (SVM) [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1\n",
    "In this exercise we use Support Vector Machine (SVM). The idea behind SVM is to determinate an hyperplane to separate and classify the given samples. In particular this can be done minimizing $\\frac{1}{2}||w||^2$ subject to $y^{(i)}(x^{(i)}\\cdot w+b)\\ge 1$, $i =1,2,...,N$, where $w$ is the vector perpendicular to the direction of the hyperplane, $y^{(i)}$ is the class of the i-th element and $x^{(i)}$ represents all the 'info' we have about an element.\n",
    "\n",
    "We will use the soft margin version of SVM, which is used to overcome imperfections encountered during the process of separation. In particular to do this we will need to solve the problem:\n",
    "\n",
    "$\\min_{w}\\frac{1}{2}||w||^2+\\frac{\\lambda}{N} \\sum_i^N C^{(i)}$, subject to $1-y^{(i)}(w\\cdot x^{(i)}+b)\\le C^{(i)}$ for $i=1,...,N$\n",
    "\n",
    "where $C^{(i)} = max \\{0,1-(x^{(i)}\\cdot w+b)y^{(i)}\\}$.\n",
    "\n",
    "$\\lambda$ is the hardness hyper-parameter.\n",
    "\n",
    "We will use 5 fold cross validation to find the optimal hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the functions provided in the weekly notebooks of the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1890,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "Tumour_samples = df_tumour_samples.to_numpy()\n",
    "y_tumour_samples = Tumour_samples[:,31] # creating the array with the classifications (B or M)\n",
    "N_tumour_samples = len(y_tumour_samples)\n",
    "X_tumour_samples = Tumour_samples[:,1:31] #creating a matrix storing the input data\n",
    "\n",
    "\n",
    "#We standardise\n",
    "X_tumour_samples = (X_tumour_samples-np.mean(X_tumour_samples))/np.std(X_tumour_samples)\n",
    "\n",
    "#We augment\n",
    "X_tumour_samples_SVM = np.hstack([X_tumour_samples,1.0 * np.ones((len(y_tumour_samples),1))])\n",
    "\n",
    "#Testing\n",
    "Tumour_test = df_tumour_test.to_numpy()\n",
    "y_tumour_test = Tumour_test[:,31] # creating the array with the classifications (B or M)\n",
    "N_tumour_test = len(y_tumour_test)\n",
    "X_tumour_test = Tumour_test[:,1:31] #creating a matrix storing the input data\n",
    "\n",
    "\n",
    "#We standardise\n",
    "X_tumour_test = (X_tumour_test-np.mean(X_tumour_test))/np.std(X_tumour_test)\n",
    "\n",
    "\n",
    "#We augment\n",
    "X_tumour_test_SVM = np.hstack([X_tumour_test,1.0 * np.ones((len(y_tumour_test),1))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1891,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(w, X, y, regul_strength=1e5):\n",
    "    n = X.shape[0]\n",
    "    distances = 1 - y * (X @ w)  \n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "    hinge = regul_strength * distances.mean()\n",
    "\n",
    "    # calculate cost\n",
    "    return 0.5 * np.dot(w, w) + hinge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate the gradient in order to implement stochastic gradient descendent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1900,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient(w, X_batch, y_batch, regul_strength=1e5):\n",
    "    # if only one example is passed\n",
    "    if type(y_batch) == np.float64:\n",
    "        y_batch = np.asarray([y_batch])\n",
    "        X_batch = np.asarray([X_batch])  # gives multidimensional array\n",
    "\n",
    "    distance = 1 - (y_batch * (X_batch @ w))\n",
    "    dw = np.zeros(len(w), dtype = float)\n",
    "\n",
    "    for ind, d in enumerate(distance):\n",
    "        if max(0, d)==0:\n",
    "            di = w\n",
    "            di = di.astype(float)\n",
    "        else:\n",
    "            di = w - (regul_strength * y_batch[ind] * X_batch[ind])\n",
    "            di = di.astype(float)\n",
    "        dw += di\n",
    "\n",
    "    return dw/len(y_batch)  # average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stochastic gradient descendent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1901,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, y, batch_size=16, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=1e5, print_outcome=False):\n",
    "    # initialise zero weights\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    nth = 0\n",
    "    # initialise starting cost as infinity\n",
    "    prev_cost = np.inf\n",
    "    \n",
    "    # stochastic gradient descent\n",
    "    indices = np.arange(len(y))\n",
    "    for iteration in range(1, max_iterations):\n",
    "        # shuffle to prevent repeating update cycles\n",
    "        np.random.shuffle(indices)\n",
    "        batch_idx = indices[:batch_size]\n",
    "        X_b, y_b = X[batch_idx], y[batch_idx]\n",
    "        for xi, yi in zip(X_b, y_b):\n",
    "            ascent = calculate_cost_gradient(weights, xi, yi, regul_strength) \n",
    "            weights = weights - (learning_rate * ascent)\n",
    "\n",
    "        # convergence check on 2^n'th iteration\n",
    "        if iteration==2**nth or iteration==max_iterations-1:\n",
    "            # compute cost\n",
    "            cost = compute_cost(weights, X, y, regul_strength)\n",
    "            if print_outcome:\n",
    "                print(\"Iteration is: {}, Cost is: {}\".format(iteration, cost))\n",
    "            # stop criterion\n",
    "            if abs(prev_cost - cost) < stop_criterion * prev_cost:\n",
    "                return weights\n",
    "              \n",
    "            prev_cost = cost\n",
    "            nth += 1\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to transform our 'diagnosis vectors' in vectors of +1 and -1. We will give +1 for 'M' and -1 for 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1902,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_diagnosis(y):\n",
    "    #taked a vector of diagnosis and return a vector long the same containing +1 instead of 'M' and -1 instead of 'B'\n",
    "    N = len(y)\n",
    "    y_tr = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        if y[i] == 'M':\n",
    "            y_tr[i]+=1.0\n",
    "        elif y[i] == 'B':\n",
    "            y_tr[i]-=1.0\n",
    "    return y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1903,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tumour_samples_tr = transform_diagnosis(y_tumour_samples)\n",
    "y_tumour_test_tr = transform_diagnosis(y_tumour_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1, Cost is: 1061.701698081285\n",
      "Iteration is: 2, Cost is: 434.3565844210795\n",
      "Iteration is: 4, Cost is: 384.2932850652742\n",
      "Iteration is: 8, Cost is: 323.57924080211563\n",
      "Iteration is: 16, Cost is: 205.35860215972997\n",
      "Iteration is: 32, Cost is: 143.11211625050632\n",
      "Iteration is: 64, Cost is: 130.9301494831552\n",
      "Iteration is: 128, Cost is: 155.9039585351026\n",
      "Iteration is: 256, Cost is: 122.98026353047862\n",
      "Iteration is: 512, Cost is: 135.01091592749418\n",
      "Iteration is: 1024, Cost is: 121.84193931690503\n",
      "Iteration is: 1999, Cost is: 123.34135771042195\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "w = sgd(X_tumour_samples_SVM, y_tumour_samples_tr, batch_size=32, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=1e3, print_outcome=True)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1905,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_SVM(w, X, y):\n",
    "    y_preds = np.sign(X @ w)\n",
    "    return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1927,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score_SVM(X_train, y_train, folds,regul_strength):\n",
    "    #performs 5 cross validation and returns the average accuracy\n",
    "  scores = []\n",
    "  for i in range(len(folds)):\n",
    "    val_indexes = folds[i]\n",
    "    train_indexes = list(set(range(y_train.shape[0]-1)) - set(val_indexes))\n",
    "    \n",
    "    X_train_i = X_train[train_indexes, :]\n",
    "    y_train_i = y_train[train_indexes]\n",
    "\n",
    "\n",
    "    X_val_i = X_train[val_indexes, :] \n",
    "    y_val_i = y_train[val_indexes] \n",
    "    \n",
    "    w = sgd(X_train_i, y_train_i, batch_size=32, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength = regul_strength, print_outcome=False)\n",
    "\n",
    "\n",
    "    scoreSVM = score_SVM(w,X_val_i,y_val_i)\n",
    "    scores.append(scoreSVM)\n",
    "\n",
    "  # Return the average score\n",
    "  return np.mean(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1928,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_lambda_SVM(X_train, y_train, folds, lambda_range):   #This function is strongly inspired by the weekly notebooks of this module\n",
    "  lambda_scores = np.zeros((len(lambda_range),))\n",
    "  \n",
    "  for i, lamb in enumerate(lambda_range):\n",
    "    lambda_scores[i] = cross_validation_score_SVM(X_train, y_train, folds, lamb)\n",
    "    print(f'Hyperparameter lambda={lamb}: {lambda_scores[i]:f}')\n",
    "\n",
    "  best_lambda_index = np.argmax(lambda_scores)\n",
    "  return lambda_range[best_lambda_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2010,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_SVM_param = np.linspace(0.00000001,100,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2011,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter lambda=1e-08: 0.834698\n",
      "Hyperparameter lambda=1.01010102: 0.834698\n",
      "Hyperparameter lambda=2.02020203: 0.834698\n",
      "Hyperparameter lambda=3.03030304: 0.834698\n",
      "Hyperparameter lambda=4.04040405: 0.834698\n",
      "Hyperparameter lambda=5.05050506: 0.834698\n",
      "Hyperparameter lambda=6.06060607: 0.834698\n",
      "Hyperparameter lambda=7.070707080000001: 0.861209\n",
      "Hyperparameter lambda=8.080808090000001: 0.881481\n",
      "Hyperparameter lambda=9.090909100000001: 0.885770\n",
      "Hyperparameter lambda=10.10101011: 0.869006\n",
      "Hyperparameter lambda=11.111111120000002: 0.885380\n",
      "Hyperparameter lambda=12.121212130000002: 0.902534\n",
      "Hyperparameter lambda=13.131313140000001: 0.909162\n",
      "Hyperparameter lambda=14.141414150000003: 0.891228\n",
      "Hyperparameter lambda=15.151515160000002: 0.913840\n",
      "Hyperparameter lambda=16.161616170000002: 0.934893\n",
      "Hyperparameter lambda=17.17171718: 0.934113\n",
      "Hyperparameter lambda=18.18181819: 0.936062\n",
      "Hyperparameter lambda=19.1919192: 0.934503\n",
      "Hyperparameter lambda=20.20202021: 0.911891\n",
      "Hyperparameter lambda=21.212121220000004: 0.935673\n",
      "Hyperparameter lambda=22.222222230000003: 0.918908\n",
      "Hyperparameter lambda=23.232323240000003: 0.934503\n",
      "Hyperparameter lambda=24.242424250000003: 0.913840\n",
      "Hyperparameter lambda=25.252525260000002: 0.914230\n",
      "Hyperparameter lambda=26.262626270000002: 0.916959\n",
      "Hyperparameter lambda=27.27272728: 0.916959\n",
      "Hyperparameter lambda=28.282828290000005: 0.938012\n",
      "Hyperparameter lambda=29.292929300000004: 0.938791\n",
      "Hyperparameter lambda=30.303030310000004: 0.939571\n",
      "Hyperparameter lambda=31.313131320000004: 0.939181\n",
      "Hyperparameter lambda=32.32323233: 0.940741\n",
      "Hyperparameter lambda=33.33333334: 0.923977\n",
      "Hyperparameter lambda=34.34343435: 0.939571\n",
      "Hyperparameter lambda=35.35353536: 0.942300\n",
      "Hyperparameter lambda=36.36363637: 0.942300\n",
      "Hyperparameter lambda=37.37373738: 0.940351\n",
      "Hyperparameter lambda=38.38383839: 0.925926\n",
      "Hyperparameter lambda=39.3939394: 0.943860\n",
      "Hyperparameter lambda=40.40404041: 0.945419\n",
      "Hyperparameter lambda=41.41414142000001: 0.944639\n",
      "Hyperparameter lambda=42.42424243000001: 0.943470\n",
      "Hyperparameter lambda=43.434343440000006: 0.941910\n",
      "Hyperparameter lambda=44.444444450000006: 0.944250\n",
      "Hyperparameter lambda=45.454545460000006: 0.943080\n",
      "Hyperparameter lambda=46.464646470000005: 0.918129\n",
      "Hyperparameter lambda=47.474747480000005: 0.943860\n",
      "Hyperparameter lambda=48.484848490000005: 0.948538\n",
      "Hyperparameter lambda=49.494949500000004: 0.855361\n",
      "Hyperparameter lambda=50.505050510000004: 0.945419\n",
      "Hyperparameter lambda=51.51515152: 0.897076\n",
      "Hyperparameter lambda=52.52525253: 0.946199\n",
      "Hyperparameter lambda=53.53535354: 0.946199\n",
      "Hyperparameter lambda=54.54545455: 0.950097\n",
      "Hyperparameter lambda=55.55555556: 0.942690\n",
      "Hyperparameter lambda=56.56565657000001: 0.946979\n",
      "Hyperparameter lambda=57.57575758000001: 0.946979\n",
      "Hyperparameter lambda=58.58585859000001: 0.944250\n",
      "Hyperparameter lambda=59.59595960000001: 0.946199\n",
      "Hyperparameter lambda=60.60606061000001: 0.945029\n",
      "Hyperparameter lambda=61.61616162000001: 0.949318\n",
      "Hyperparameter lambda=62.62626263000001: 0.949708\n",
      "Hyperparameter lambda=63.636363640000006: 0.946589\n",
      "Hyperparameter lambda=64.64646465: 0.947368\n",
      "Hyperparameter lambda=65.65656566: 0.949708\n",
      "Hyperparameter lambda=66.66666667: 0.930604\n",
      "Hyperparameter lambda=67.67676768: 0.948538\n",
      "Hyperparameter lambda=68.68686869: 0.948148\n",
      "Hyperparameter lambda=69.6969697: 0.947368\n",
      "Hyperparameter lambda=70.70707071: 0.947368\n",
      "Hyperparameter lambda=71.71717172: 0.946589\n",
      "Hyperparameter lambda=72.72727273: 0.882261\n",
      "Hyperparameter lambda=73.73737374: 0.945029\n",
      "Hyperparameter lambda=74.74747475: 0.952827\n",
      "Hyperparameter lambda=75.75757576: 0.948148\n",
      "Hyperparameter lambda=76.76767677: 0.946979\n",
      "Hyperparameter lambda=77.77777778: 0.952047\n",
      "Hyperparameter lambda=78.78787879: 0.949708\n",
      "Hyperparameter lambda=79.7979798: 0.952047\n",
      "Hyperparameter lambda=80.80808080999999: 0.948928\n",
      "Hyperparameter lambda=81.81818182: 0.950877\n",
      "Hyperparameter lambda=82.82828283: 0.951657\n",
      "Hyperparameter lambda=83.83838384: 0.951267\n",
      "Hyperparameter lambda=84.84848485: 0.808967\n",
      "Hyperparameter lambda=85.85858586: 0.945029\n",
      "Hyperparameter lambda=86.86868687: 0.949318\n",
      "Hyperparameter lambda=87.87878788: 0.951267\n",
      "Hyperparameter lambda=88.88888889: 0.953996\n",
      "Hyperparameter lambda=89.8989899: 0.948538\n",
      "Hyperparameter lambda=90.90909091: 0.953216\n",
      "Hyperparameter lambda=91.91919192: 0.950097\n",
      "Hyperparameter lambda=92.92929293: 0.946589\n",
      "Hyperparameter lambda=93.93939394: 0.950097\n",
      "Hyperparameter lambda=94.94949495: 0.946589\n",
      "Hyperparameter lambda=95.95959596: 0.949318\n",
      "Hyperparameter lambda=96.96969697: 0.948538\n",
      "Hyperparameter lambda=97.97979798: 0.953996\n",
      "Hyperparameter lambda=98.98989899: 0.946979\n",
      "Hyperparameter lambda=100.0: 0.949318\n",
      "best_lambda: 88.88888889\n"
     ]
    }
   ],
   "source": [
    "best_lambda_SVM = choose_best_lambda_SVM(X_tumour_samples_SVM, y_tumour_samples_tr, folds_indexes_tumour_samples, list_SVM_param)\n",
    "\n",
    "print('best_lambda:', best_lambda_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal parameter from the one we checked is $\\lambda = 88.88888889$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter $\\lambda$ is such that for large values we will go for 'better fitting plane', but we will have smaller margines. On the opposite smaller $\\lambda$ gives worse 'fitting plan', but with larger margins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot for lambda in [0,1000000] the average accuracy (obtained with cross validation). We get the following plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1940,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdfb8106ad0>]"
      ]
     },
     "execution_count": 1940,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting accuracy vs lambda\n",
    "l_SVM1 = [] #to store the values of the accuracy\n",
    "m_SVM1 = [] #to store the values of the parameter lambda\n",
    "for i in range(0,1000000,10000):\n",
    "    lambda_val = i\n",
    "    m_SVM1.append(lambda_val)\n",
    "    l_SVM1.append(cross_validation_score_SVM(X_tumour_samples_SVM, y_tumour_samples_tr, folds_indexes_tumour_samples, lambda_val)) #calculating accuracy and appending it to l_knn1\n",
    "plt.plot(m_SVM1,l_SVM1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot for lambda in [0,20], and we get the following plot of average accuracies. We can notice how the accuracies are lower for small values of lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1948,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdffdc557d0>]"
      ]
     },
     "execution_count": 1948,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting accuracy vs lambda\n",
    "l_SVM2 = [] #to store the values of the accuracy\n",
    "m_SVM2 = [] #to store the values of the parameter lambda\n",
    "for i in range(1,400,5):\n",
    "    lambda_val = 0.05*i\n",
    "    m_SVM2.append(lambda_val)\n",
    "    l_SVM2.append(cross_validation_score_SVM(X_tumour_samples_SVM, y_tumour_samples_tr, folds_indexes_tumour_samples, lambda_val)) #calculating accuracy and appending it to l_knn1\n",
    "plt.plot(m_SVM2,l_SVM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that counts the true/false positive and true false/negatives and gives as a result $FPR = \\frac{FP}{TN+FP}$, and $TPR = \\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2016,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_SVM_CM(w, X, y):\n",
    "    y_pred = np.sign(X @ w)\n",
    "    N = len(y)\n",
    "    TP = 0 #True positives\n",
    "    TN = 0 #True negatives\n",
    "    FP = 0 #False positives\n",
    "    FN = 0 #False negatives\n",
    "    for j in range(N):\n",
    "        if y_pred[j] == y[j] and y_pred[j]== -1.0: \n",
    "            TN+=1\n",
    "        elif y_pred[j] == y[j] and y_pred[j] == 1.0:\n",
    "            TP+=1\n",
    "        elif y_pred[j] != y[j] and y_pred[j] == 1.0:\n",
    "            FP+=1\n",
    "        elif y_pred[j] != y[j] and y_pred[j] == -1.0:\n",
    "            FN+=1\n",
    "    if (TP + FN) == 0 or (TN + FP) == 0:    #avoiding division by zero\n",
    "        TPR = 'no'\n",
    "        FPR = 'no'\n",
    "    else:\n",
    "        TPR = TP / (TP + FN) \n",
    "        FPR = FP / (TN + FP)\n",
    "    return TPR, FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2006,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_TPR = []\n",
    "list_FPR = []\n",
    "lmhs_list = np.linspace(0.001,49999,320)\n",
    "for i in range (319):\n",
    "    w = sgd(X_tumour_samples_SVM, y_tumour_samples_tr, batch_size=32, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength = lmhs_list[i], print_outcome=False)\n",
    "    TPR,FPR = score_SVM_CM(w, X_tumour_test_SVM, y_tumour_test_tr)\n",
    "    if TPR != 'no' or FPR != 'no':\n",
    "        list_TPR.append(TPR)\n",
    "        list_FPR.append(FPR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get TPR and FPR we get for our optimal $\\lambda$: (notice that we got our optimal parameters checking less possible lambdas than the ones we are using for the ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2017,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_TPR_opt = []\n",
    "list_FPR_opt = []\n",
    "w_optimal = sgd(X_tumour_samples_SVM, y_tumour_samples_tr, batch_size=32, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength = 88.88888889, print_outcome=False)\n",
    "TPR_opt,FPR_opt = score_SVM_CM(w_optimal, X_tumour_test_SVM, y_tumour_test_tr)\n",
    "list_TPR_opt.append(TPR_opt)\n",
    "list_FPR_opt.append(FPR_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2051,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(f\"Receiver Operating Curve\", size=20)\n",
    "plt.scatter(list_FPR, list_TPR)\n",
    "plt.scatter(list_FPR_opt, list_TPR_opt)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('TPR', size=20)\n",
    "plt.xlabel('FPR', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 'scatter points' representing our ROC (I decided to scatter to have a better view) in blue.They are above the diagonal line, so the model is doing better than a random model. Furthermore our 'curve' goes near to the point (0,1), which is what we would like to see ideally.\n",
    "The point we get for our optimal $\\lambda$ is the one in orange, and how we can see it still is above the red line. We notice that maybe we could have optained something better but we computer our optimal lambda searching it in [0,100], while for the plot of ROC we used a larger set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data $\\texttt{tumour_samples_bal.csv}$ with the balanced data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2022,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n1_radius</th>\n",
       "      <th>n1_texture</th>\n",
       "      <th>n1_perimeter</th>\n",
       "      <th>n1_area</th>\n",
       "      <th>n1_smoothness</th>\n",
       "      <th>n1_compactness</th>\n",
       "      <th>n1_concavity</th>\n",
       "      <th>n1_concave_points</th>\n",
       "      <th>n1_symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>n3_texture</th>\n",
       "      <th>n3_perimeter</th>\n",
       "      <th>n3_area</th>\n",
       "      <th>n3_smoothness</th>\n",
       "      <th>n3_compactness</th>\n",
       "      <th>n3_concavity</th>\n",
       "      <th>n3_concave_points</th>\n",
       "      <th>n3_symmetry</th>\n",
       "      <th>n3_fractal_dimension</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14.530395</td>\n",
       "      <td>20.421653</td>\n",
       "      <td>85.363155</td>\n",
       "      <td>675.768723</td>\n",
       "      <td>0.108968</td>\n",
       "      <td>0.160971</td>\n",
       "      <td>0.177338</td>\n",
       "      <td>0.076638</td>\n",
       "      <td>0.211611</td>\n",
       "      <td>...</td>\n",
       "      <td>31.530430</td>\n",
       "      <td>117.452224</td>\n",
       "      <td>995.295094</td>\n",
       "      <td>0.149985</td>\n",
       "      <td>0.361014</td>\n",
       "      <td>0.507859</td>\n",
       "      <td>0.194167</td>\n",
       "      <td>0.364020</td>\n",
       "      <td>0.108554</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15.716418</td>\n",
       "      <td>11.961019</td>\n",
       "      <td>102.643405</td>\n",
       "      <td>676.233715</td>\n",
       "      <td>0.126132</td>\n",
       "      <td>0.156888</td>\n",
       "      <td>0.199903</td>\n",
       "      <td>0.109296</td>\n",
       "      <td>0.202276</td>\n",
       "      <td>...</td>\n",
       "      <td>17.016180</td>\n",
       "      <td>118.556593</td>\n",
       "      <td>1073.826428</td>\n",
       "      <td>0.156691</td>\n",
       "      <td>0.391081</td>\n",
       "      <td>0.612940</td>\n",
       "      <td>0.180875</td>\n",
       "      <td>0.302284</td>\n",
       "      <td>0.106598</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>21.137170</td>\n",
       "      <td>28.522843</td>\n",
       "      <td>123.110775</td>\n",
       "      <td>1350.096674</td>\n",
       "      <td>0.091688</td>\n",
       "      <td>0.105862</td>\n",
       "      <td>0.168914</td>\n",
       "      <td>0.076865</td>\n",
       "      <td>0.136646</td>\n",
       "      <td>...</td>\n",
       "      <td>36.202911</td>\n",
       "      <td>161.032929</td>\n",
       "      <td>1670.209793</td>\n",
       "      <td>0.114542</td>\n",
       "      <td>0.243286</td>\n",
       "      <td>0.404950</td>\n",
       "      <td>0.149961</td>\n",
       "      <td>0.274891</td>\n",
       "      <td>0.082890</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20.985951</td>\n",
       "      <td>21.139095</td>\n",
       "      <td>123.856163</td>\n",
       "      <td>1214.287779</td>\n",
       "      <td>0.102580</td>\n",
       "      <td>0.117669</td>\n",
       "      <td>0.198974</td>\n",
       "      <td>0.098910</td>\n",
       "      <td>0.204487</td>\n",
       "      <td>...</td>\n",
       "      <td>28.726188</td>\n",
       "      <td>146.057844</td>\n",
       "      <td>1410.132241</td>\n",
       "      <td>0.120466</td>\n",
       "      <td>0.226293</td>\n",
       "      <td>0.363629</td>\n",
       "      <td>0.183632</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.072986</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>22.136720</td>\n",
       "      <td>23.129660</td>\n",
       "      <td>137.379083</td>\n",
       "      <td>1304.114821</td>\n",
       "      <td>0.096771</td>\n",
       "      <td>0.147616</td>\n",
       "      <td>0.262871</td>\n",
       "      <td>0.129781</td>\n",
       "      <td>0.216392</td>\n",
       "      <td>...</td>\n",
       "      <td>26.846823</td>\n",
       "      <td>172.665832</td>\n",
       "      <td>1815.393844</td>\n",
       "      <td>0.124286</td>\n",
       "      <td>0.361435</td>\n",
       "      <td>0.677388</td>\n",
       "      <td>0.205779</td>\n",
       "      <td>0.311112</td>\n",
       "      <td>0.070464</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>795</td>\n",
       "      <td>12.586262</td>\n",
       "      <td>17.042080</td>\n",
       "      <td>84.933506</td>\n",
       "      <td>558.765229</td>\n",
       "      <td>0.091761</td>\n",
       "      <td>0.056785</td>\n",
       "      <td>0.043764</td>\n",
       "      <td>0.017047</td>\n",
       "      <td>0.201145</td>\n",
       "      <td>...</td>\n",
       "      <td>23.483917</td>\n",
       "      <td>86.850747</td>\n",
       "      <td>582.852909</td>\n",
       "      <td>0.122505</td>\n",
       "      <td>0.150139</td>\n",
       "      <td>0.162602</td>\n",
       "      <td>0.068105</td>\n",
       "      <td>0.366096</td>\n",
       "      <td>0.073594</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>796</td>\n",
       "      <td>11.803064</td>\n",
       "      <td>29.143953</td>\n",
       "      <td>79.909893</td>\n",
       "      <td>432.828557</td>\n",
       "      <td>0.082877</td>\n",
       "      <td>0.050179</td>\n",
       "      <td>0.013556</td>\n",
       "      <td>0.019917</td>\n",
       "      <td>0.185930</td>\n",
       "      <td>...</td>\n",
       "      <td>34.489916</td>\n",
       "      <td>88.370384</td>\n",
       "      <td>575.936260</td>\n",
       "      <td>0.097904</td>\n",
       "      <td>0.109149</td>\n",
       "      <td>0.037035</td>\n",
       "      <td>0.040736</td>\n",
       "      <td>0.228439</td>\n",
       "      <td>0.068118</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>797</td>\n",
       "      <td>9.133855</td>\n",
       "      <td>20.587504</td>\n",
       "      <td>61.884174</td>\n",
       "      <td>230.260769</td>\n",
       "      <td>0.105125</td>\n",
       "      <td>0.072745</td>\n",
       "      <td>0.018446</td>\n",
       "      <td>0.012392</td>\n",
       "      <td>0.172465</td>\n",
       "      <td>...</td>\n",
       "      <td>30.955895</td>\n",
       "      <td>65.333249</td>\n",
       "      <td>370.693374</td>\n",
       "      <td>0.158548</td>\n",
       "      <td>0.166818</td>\n",
       "      <td>0.062229</td>\n",
       "      <td>0.066901</td>\n",
       "      <td>0.292290</td>\n",
       "      <td>0.094359</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>798</td>\n",
       "      <td>14.604268</td>\n",
       "      <td>15.911254</td>\n",
       "      <td>95.571949</td>\n",
       "      <td>670.932144</td>\n",
       "      <td>0.093725</td>\n",
       "      <td>0.104808</td>\n",
       "      <td>0.084452</td>\n",
       "      <td>0.038038</td>\n",
       "      <td>0.193340</td>\n",
       "      <td>...</td>\n",
       "      <td>22.622737</td>\n",
       "      <td>111.460483</td>\n",
       "      <td>831.977088</td>\n",
       "      <td>0.125943</td>\n",
       "      <td>0.317772</td>\n",
       "      <td>0.433252</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>0.299085</td>\n",
       "      <td>0.088326</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>799</td>\n",
       "      <td>12.481708</td>\n",
       "      <td>15.310366</td>\n",
       "      <td>71.156977</td>\n",
       "      <td>396.082845</td>\n",
       "      <td>0.103470</td>\n",
       "      <td>0.065718</td>\n",
       "      <td>0.054012</td>\n",
       "      <td>0.033229</td>\n",
       "      <td>0.188036</td>\n",
       "      <td>...</td>\n",
       "      <td>20.368779</td>\n",
       "      <td>78.216630</td>\n",
       "      <td>499.251547</td>\n",
       "      <td>0.136583</td>\n",
       "      <td>0.119195</td>\n",
       "      <td>0.199128</td>\n",
       "      <td>0.103132</td>\n",
       "      <td>0.283602</td>\n",
       "      <td>0.073796</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  n1_radius  n1_texture  n1_perimeter      n1_area  \\\n",
       "0             0  14.530395   20.421653     85.363155   675.768723   \n",
       "1             1  15.716418   11.961019    102.643405   676.233715   \n",
       "2             2  21.137170   28.522843    123.110775  1350.096674   \n",
       "3             3  20.985951   21.139095    123.856163  1214.287779   \n",
       "4             4  22.136720   23.129660    137.379083  1304.114821   \n",
       "..          ...        ...         ...           ...          ...   \n",
       "795         795  12.586262   17.042080     84.933506   558.765229   \n",
       "796         796  11.803064   29.143953     79.909893   432.828557   \n",
       "797         797   9.133855   20.587504     61.884174   230.260769   \n",
       "798         798  14.604268   15.911254     95.571949   670.932144   \n",
       "799         799  12.481708   15.310366     71.156977   396.082845   \n",
       "\n",
       "     n1_smoothness  n1_compactness  n1_concavity  n1_concave_points  \\\n",
       "0         0.108968        0.160971      0.177338           0.076638   \n",
       "1         0.126132        0.156888      0.199903           0.109296   \n",
       "2         0.091688        0.105862      0.168914           0.076865   \n",
       "3         0.102580        0.117669      0.198974           0.098910   \n",
       "4         0.096771        0.147616      0.262871           0.129781   \n",
       "..             ...             ...           ...                ...   \n",
       "795       0.091761        0.056785      0.043764           0.017047   \n",
       "796       0.082877        0.050179      0.013556           0.019917   \n",
       "797       0.105125        0.072745      0.018446           0.012392   \n",
       "798       0.093725        0.104808      0.084452           0.038038   \n",
       "799       0.103470        0.065718      0.054012           0.033229   \n",
       "\n",
       "     n1_symmetry  ...  n3_texture  n3_perimeter      n3_area  n3_smoothness  \\\n",
       "0       0.211611  ...   31.530430    117.452224   995.295094       0.149985   \n",
       "1       0.202276  ...   17.016180    118.556593  1073.826428       0.156691   \n",
       "2       0.136646  ...   36.202911    161.032929  1670.209793       0.114542   \n",
       "3       0.204487  ...   28.726188    146.057844  1410.132241       0.120466   \n",
       "4       0.216392  ...   26.846823    172.665832  1815.393844       0.124286   \n",
       "..           ...  ...         ...           ...          ...            ...   \n",
       "795     0.201145  ...   23.483917     86.850747   582.852909       0.122505   \n",
       "796     0.185930  ...   34.489916     88.370384   575.936260       0.097904   \n",
       "797     0.172465  ...   30.955895     65.333249   370.693374       0.158548   \n",
       "798     0.193340  ...   22.622737    111.460483   831.977088       0.125943   \n",
       "799     0.188036  ...   20.368779     78.216630   499.251547       0.136583   \n",
       "\n",
       "     n3_compactness  n3_concavity  n3_concave_points  n3_symmetry  \\\n",
       "0          0.361014      0.507859           0.194167     0.364020   \n",
       "1          0.391081      0.612940           0.180875     0.302284   \n",
       "2          0.243286      0.404950           0.149961     0.274891   \n",
       "3          0.226293      0.363629           0.183632     0.252874   \n",
       "4          0.361435      0.677388           0.205779     0.311112   \n",
       "..              ...           ...                ...          ...   \n",
       "795        0.150139      0.162602           0.068105     0.366096   \n",
       "796        0.109149      0.037035           0.040736     0.228439   \n",
       "797        0.166818      0.062229           0.066901     0.292290   \n",
       "798        0.317772      0.433252           0.140478     0.299085   \n",
       "799        0.119195      0.199128           0.103132     0.283602   \n",
       "\n",
       "     n3_fractal_dimension  DIAGNOSIS  \n",
       "0                0.108554          M  \n",
       "1                0.106598          M  \n",
       "2                0.082890          M  \n",
       "3                0.072986          M  \n",
       "4                0.070464          M  \n",
       "..                    ...        ...  \n",
       "795              0.073594          B  \n",
       "796              0.068118          B  \n",
       "797              0.094359          B  \n",
       "798              0.088326          B  \n",
       "799              0.073796          B  \n",
       "\n",
       "[800 rows x 32 columns]"
      ]
     },
     "execution_count": 2022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tumour_samples_bal = pd.read_csv(\"tumour_samples_bal.csv\") #reading from thr csv file\n",
    "df_tumour_samples_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2040,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tumour_samples_bal = df_tumour_samples_bal.to_numpy()\n",
    "y_tumour_samples_bal = Tumour_samples_bal[:,31] # creating the array with the classifications (B or M)\n",
    "N_tumour_samples_bal = len(y_tumour_samples_bal)\n",
    "X_tumour_samples_bal = Tumour_samples_bal[:,1:31] #creating a matrix storing the input data\n",
    "X_tumour_samples_bal.astype(float)\n",
    "\n",
    "\n",
    "#We standardise\n",
    "X_tumour_samples_bal = (X_tumour_samples_bal-np.mean(X_tumour_samples_bal))/np.std(X_tumour_samples_bal)\n",
    "\n",
    "#We augment\n",
    "X_tumour_samples_bal_SVM = np.hstack([X_tumour_samples_bal,1.0 * np.ones((len(y_tumour_samples_bal),1))])\n",
    "\n",
    "#We transform y in +1,-1 from 'M' and 'B'\n",
    "y_tumour_samples_bal_tr = transform_diagnosis(y_tumour_samples_bal)\n",
    "\n",
    "#We create the 5 folds\n",
    "folds_indexes_bal = np.split(np.arange(len(y_tumour_samples_bal)), 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the best parameter $\\lambda$  (using the cross validation function we defined before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2041,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score_SVM_bal(X_train, y_train, folds,regul_strength):\n",
    "    #performs 5 cross validation and returns the average accuracy\n",
    "  scores = []\n",
    "  for i in range(len(folds)):\n",
    "    val_indexes = folds[i]\n",
    "    train_indexes = list(set(range(y_train.shape[0])) - set(val_indexes))\n",
    "    \n",
    "    X_train_i = X_train[train_indexes, :]\n",
    "    y_train_i = y_train[train_indexes]\n",
    "\n",
    "\n",
    "    X_val_i = X_train[val_indexes, :] \n",
    "    y_val_i = y_train[val_indexes] \n",
    "    \n",
    "    w = sgd(X_train_i, y_train_i, batch_size=32, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength = regul_strength, print_outcome=False)\n",
    "\n",
    "\n",
    "    scoreSVM = score_SVM(w,X_val_i,y_val_i)\n",
    "    scores.append(scoreSVM)\n",
    "\n",
    "  # Return the average score\n",
    "  return np.mean(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2042,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_lambda_SVM_bal(X_train, y_train, folds, lambda_range):   #This function is strongly inspired by the weekly notebooks of this module\n",
    "  lambda_scores = np.zeros((len(lambda_range),))\n",
    "  \n",
    "  for i, lamb in enumerate(lambda_range):\n",
    "    lambda_scores[i] = cross_validation_score_SVM_bal(X_train, y_train, folds, lamb)\n",
    "    print(f'Hyperparameter lambda={lamb}: {lambda_scores[i]:f}')\n",
    "\n",
    "  best_lambda_index = np.argmax(lambda_scores)\n",
    "  return lambda_range[best_lambda_index]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2044,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter lambda=1e-08: 0.342500\n",
      "Hyperparameter lambda=1.01010102: 0.500000\n",
      "Hyperparameter lambda=2.02020203: 0.498750\n",
      "Hyperparameter lambda=3.03030304: 0.538750\n",
      "Hyperparameter lambda=4.04040405: 0.251250\n",
      "Hyperparameter lambda=5.05050506: 0.523750\n",
      "Hyperparameter lambda=6.06060607: 0.842500\n",
      "Hyperparameter lambda=7.070707080000001: 0.592500\n",
      "Hyperparameter lambda=8.080808090000001: 0.681250\n",
      "Hyperparameter lambda=9.090909100000001: 0.918750\n",
      "Hyperparameter lambda=10.10101011: 0.927500\n",
      "Hyperparameter lambda=11.111111120000002: 0.928750\n",
      "Hyperparameter lambda=12.121212130000002: 0.931250\n",
      "Hyperparameter lambda=13.131313140000001: 0.850000\n",
      "Hyperparameter lambda=14.141414150000003: 0.911250\n",
      "Hyperparameter lambda=15.151515160000002: 0.736250\n",
      "Hyperparameter lambda=16.161616170000002: 0.940000\n",
      "Hyperparameter lambda=17.17171718: 0.762500\n",
      "Hyperparameter lambda=18.18181819: 0.952500\n",
      "Hyperparameter lambda=19.1919192: 0.938750\n",
      "Hyperparameter lambda=20.20202021: 0.870000\n",
      "Hyperparameter lambda=21.212121220000004: 0.938750\n",
      "Hyperparameter lambda=22.222222230000003: 0.942500\n",
      "Hyperparameter lambda=23.232323240000003: 0.736250\n",
      "Hyperparameter lambda=24.242424250000003: 0.936250\n",
      "Hyperparameter lambda=25.252525260000002: 0.512500\n",
      "Hyperparameter lambda=26.262626270000002: 0.942500\n",
      "Hyperparameter lambda=27.27272728: 0.752500\n",
      "Hyperparameter lambda=28.282828290000005: 0.852500\n",
      "Hyperparameter lambda=29.292929300000004: 0.911250\n",
      "Hyperparameter lambda=30.303030310000004: 0.691250\n",
      "Hyperparameter lambda=31.313131320000004: 0.671250\n",
      "Hyperparameter lambda=32.32323233: 0.677500\n",
      "Hyperparameter lambda=33.33333334: 0.897500\n",
      "Hyperparameter lambda=34.34343435: 0.682500\n",
      "Hyperparameter lambda=35.35353536: 0.865000\n",
      "Hyperparameter lambda=36.36363637: 0.601250\n",
      "Hyperparameter lambda=37.37373738: 0.725000\n",
      "Hyperparameter lambda=38.38383839: 0.673750\n",
      "Hyperparameter lambda=39.3939394: 0.895000\n",
      "Hyperparameter lambda=40.40404041: 0.903750\n",
      "Hyperparameter lambda=41.41414142000001: 0.835000\n",
      "Hyperparameter lambda=42.42424243000001: 0.756250\n",
      "Hyperparameter lambda=43.434343440000006: 0.503750\n",
      "Hyperparameter lambda=44.444444450000006: 0.841250\n",
      "Hyperparameter lambda=45.454545460000006: 0.901250\n",
      "Hyperparameter lambda=46.464646470000005: 0.830000\n",
      "Hyperparameter lambda=47.474747480000005: 0.710000\n",
      "Hyperparameter lambda=48.484848490000005: 0.742500\n",
      "Hyperparameter lambda=49.494949500000004: 0.737500\n",
      "Hyperparameter lambda=50.505050510000004: 0.627500\n",
      "Hyperparameter lambda=51.51515152: 0.710000\n",
      "Hyperparameter lambda=52.52525253: 0.518750\n",
      "Hyperparameter lambda=53.53535354: 0.875000\n",
      "Hyperparameter lambda=54.54545455: 0.861250\n",
      "Hyperparameter lambda=55.55555556: 0.726250\n",
      "Hyperparameter lambda=56.56565657000001: 0.498750\n",
      "Hyperparameter lambda=57.57575758000001: 0.565000\n",
      "Hyperparameter lambda=58.58585859000001: 0.661250\n",
      "Hyperparameter lambda=59.59595960000001: 0.603750\n",
      "Hyperparameter lambda=60.60606061000001: 0.377500\n",
      "Hyperparameter lambda=61.61616162000001: 0.802500\n",
      "Hyperparameter lambda=62.62626263000001: 0.873750\n",
      "Hyperparameter lambda=63.636363640000006: 0.897500\n",
      "Hyperparameter lambda=64.64646465: 0.837500\n",
      "Hyperparameter lambda=65.65656566: 0.720000\n",
      "Hyperparameter lambda=66.66666667: 0.685000\n",
      "Hyperparameter lambda=67.67676768: 0.526250\n",
      "Hyperparameter lambda=68.68686869: 0.533750\n",
      "Hyperparameter lambda=69.6969697: 0.705000\n",
      "Hyperparameter lambda=70.70707071: 0.517500\n",
      "Hyperparameter lambda=71.71717172: 0.787500\n",
      "Hyperparameter lambda=72.72727273: 0.688750\n",
      "Hyperparameter lambda=73.73737374: 0.680000\n",
      "Hyperparameter lambda=74.74747475: 0.900000\n",
      "Hyperparameter lambda=75.75757576: 0.860000\n",
      "Hyperparameter lambda=76.76767677: 0.860000\n",
      "Hyperparameter lambda=77.77777778: 0.727500\n",
      "Hyperparameter lambda=78.78787879: 0.686250\n",
      "Hyperparameter lambda=79.7979798: 0.498750\n",
      "Hyperparameter lambda=80.80808080999999: 0.861250\n",
      "Hyperparameter lambda=81.81818182: 0.597500\n",
      "Hyperparameter lambda=82.82828283: 0.752500\n",
      "Hyperparameter lambda=83.83838384: 0.495000\n",
      "Hyperparameter lambda=84.84848485: 0.495000\n",
      "Hyperparameter lambda=85.85858586: 0.497500\n",
      "Hyperparameter lambda=86.86868687: 0.893750\n",
      "Hyperparameter lambda=87.87878788: 0.757500\n",
      "Hyperparameter lambda=88.88888889: 0.676250\n",
      "Hyperparameter lambda=89.8989899: 0.673750\n",
      "Hyperparameter lambda=90.90909091: 0.682500\n",
      "Hyperparameter lambda=91.91919192: 0.858750\n",
      "Hyperparameter lambda=92.92929293: 0.690000\n",
      "Hyperparameter lambda=93.93939394: 0.677500\n",
      "Hyperparameter lambda=94.94949495: 0.675000\n",
      "Hyperparameter lambda=95.95959596: 0.780000\n",
      "Hyperparameter lambda=96.96969697: 0.866250\n",
      "Hyperparameter lambda=97.97979798: 0.526250\n",
      "Hyperparameter lambda=98.98989899: 0.596250\n",
      "Hyperparameter lambda=100.0: 0.858750\n",
      "best_lambda: 18.18181819\n"
     ]
    }
   ],
   "source": [
    "best_lambda_bal_SVM = choose_best_lambda_SVM_bal(X_tumour_samples_bal_SVM, y_tumour_samples_bal_tr, folds_indexes_bal, list_SVM_param)\n",
    "print('best_lambda:', best_lambda_bal_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how we get a different optimal parameter which is $\\lambda = 18.18181819$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2045,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdffdb86210>]"
      ]
     },
     "execution_count": 2045,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting accuracy vs lambda\n",
    "l_SVM1_bal = [] #to store the values of the accuracy\n",
    "m_SVM1_bal = [] #to store the values of the parameter lambda\n",
    "for i in range(0,1000000,10000):\n",
    "    lambda_val = i\n",
    "    m_SVM1_bal.append(lambda_val)\n",
    "    l_SVM1_bal.append(cross_validation_score_SVM_bal(X_tumour_samples_bal_SVM, y_tumour_samples_bal_tr, folds_indexes_bal, lambda_val)) #calculating accuracy and appending it to l_knn1\n",
    "plt.plot(m_SVM1_bal,l_SVM1_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2046,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe03a76a350>]"
      ]
     },
     "execution_count": 2046,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting accuracy vs lambda\n",
    "l_SVM2_bal = [] #to store the values of the accuracy\n",
    "m_SVM2_bal = [] #to store the values of the parameter lambda\n",
    "for i in range(1,400,5):\n",
    "    lambda_val = 0.05*i\n",
    "    m_SVM2_bal.append(lambda_val)\n",
    "    l_SVM2_bal.append(cross_validation_score_SVM_bal(X_tumour_samples_bal_SVM, y_tumour_samples_bal_tr, folds_indexes_bal, lambda_val)) #calculating accuracy and appending it to l_knn1\n",
    "plt.plot(m_SVM2_bal,l_SVM2_bal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting (in [0,1000000], and zoomed in [0.05,20]) we can see a similar behaviour to the one we got in the previous exercise with unbalanced data, but there are oscillations noticeable on the 'larger set'. This is probably caused by the presence of less data( only 800 rows instead of 2566 as in the unbalanced data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2047,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_TPR_bal = []\n",
    "list_FPR_bal = []\n",
    "lmhs_list = np.linspace(0.001,49999,320)\n",
    "for i in range (319):\n",
    "    w = sgd(X_tumour_samples_bal_SVM, y_tumour_samples_bal_tr, batch_size=32, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength = lmhs_list[i], print_outcome=False)\n",
    "    TPR,FPR = score_SVM_CM(w, X_tumour_test_SVM, y_tumour_test_tr)\n",
    "    if TPR != 'no' or FPR != 'no':\n",
    "        list_TPR_bal.append(TPR)\n",
    "        list_FPR_bal.append(FPR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2049,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_TPR_opt_bal = []\n",
    "list_FPR_opt_bal = []\n",
    "w_optimal = sgd(X_tumour_samples_bal_SVM, y_tumour_samples_bal_tr, batch_size=32, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength = 18.18181819, print_outcome=False)\n",
    "TPR_opt,FPR_opt = score_SVM_CM(w_optimal, X_tumour_test_SVM, y_tumour_test_tr)\n",
    "list_TPR_opt_bal.append(TPR_opt)\n",
    "list_FPR_opt_bal.append(FPR_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2052,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(f\"Receiver Operating Curve\", size=20)\n",
    "plt.scatter(list_FPR_bal, list_TPR_bal)\n",
    "plt.scatter(list_FPR_opt_bal, list_TPR_opt_bal)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('TPR', size=20)\n",
    "plt.xlabel('FPR', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 'scatter points' representing our ROC (I decided to scatter to have a better view) in blue.We can again notice how the ROC curve is above the red line, so the model is doing better than a random model. \n",
    "\n",
    "Furthermore our 'curve' (so the scatter point) goes near to the line $TPR=1$, which means than differently from what we got for the unbalanced training data we are less likely to get False negatives (We remember $FPR = \\frac{FP}{TN+FP}$, and $TPR = \\frac{TP}{TP+FN}$). If we think about this problem in a real life context we can say that is better to diagnose false positives than false negatives, since in case of a false positive a patience could proceed with further diagnosis, while on the opposite a false negative patient could underestimate the problem.\n",
    "\n",
    "\n",
    "The point we get for our optimal $\\lambda$ is the one in orange, and how we can see it still is above the red line. Morevoer it seems to be a better result than the one we got in the previous exercise.\n",
    "\n",
    "We notice that maybe we could have optained something better for the optimal lambda but we compute our optimal lambda searching it in [0,100], while for the plot of ROC we used a larger set.\n",
    "\n",
    "Finally is important to notice that the balanced data is composed by roughly less than a third of the data (rows) (800 rows against 2566), but still provides a really good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlwygRTDy1fb"
   },
   "source": [
    "<a name=\"task-3\"></a>\n",
    "\n",
    "# Task 3: Mastery Component [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdYowG2Ny1qx"
   },
   "source": [
    "<a name=\"q31\"></a>\n",
    "\n",
    "## 3.1: Logistic regression and bagging [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we import the data and initialise everything (standardising when necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "Tumour_samples = df_tumour_samples.to_numpy()\n",
    "np.random.shuffle(Tumour_samples)  #Shuffling\n",
    "y_tumour_samples = Tumour_samples[:,31] # creating the array with the classifications (B or M)\n",
    "N_tumour_samples = len(y_tumour_samples)\n",
    "X_tumour_samples = Tumour_samples[:,1:31] #creating a matrix storing the input data\n",
    "\n",
    "\n",
    "#We standardise\n",
    "X_tumour_samples = (X_tumour_samples-np.mean(X_tumour_samples))/np.std(X_tumour_samples)\n",
    "X_tumour_samples_lg=(X_tumour_samples.T).astype(float)\n",
    "\n",
    "#Testing\n",
    "Tumour_test = df_tumour_test.to_numpy()\n",
    "np.random.shuffle(Tumour_test)  #Shuffling\n",
    "y_tumour_test = Tumour_test[:,31] # creating the array with the classifications (B or M)\n",
    "N_tumour_test = len(y_tumour_test)\n",
    "X_tumour_test = Tumour_test[:,1:31] #creating a matrix storing the input data\n",
    "\n",
    "\n",
    "#We standardise\n",
    "X_tumour_test = (X_tumour_test-np.mean(X_tumour_test))/np.std(X_tumour_test)\n",
    "X_tumour_test_lg=(X_tumour_test.T).astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n1_radius</th>\n",
       "      <th>n1_texture</th>\n",
       "      <th>n1_perimeter</th>\n",
       "      <th>n1_area</th>\n",
       "      <th>n1_smoothness</th>\n",
       "      <th>n1_compactness</th>\n",
       "      <th>n1_concavity</th>\n",
       "      <th>n1_concave_points</th>\n",
       "      <th>n1_symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>n3_texture</th>\n",
       "      <th>n3_perimeter</th>\n",
       "      <th>n3_area</th>\n",
       "      <th>n3_smoothness</th>\n",
       "      <th>n3_compactness</th>\n",
       "      <th>n3_concavity</th>\n",
       "      <th>n3_concave_points</th>\n",
       "      <th>n3_symmetry</th>\n",
       "      <th>n3_fractal_dimension</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.855170</td>\n",
       "      <td>15.248290</td>\n",
       "      <td>69.167041</td>\n",
       "      <td>359.534878</td>\n",
       "      <td>0.105488</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.066410</td>\n",
       "      <td>0.034194</td>\n",
       "      <td>0.182796</td>\n",
       "      <td>...</td>\n",
       "      <td>18.984557</td>\n",
       "      <td>81.443134</td>\n",
       "      <td>466.879302</td>\n",
       "      <td>0.149080</td>\n",
       "      <td>0.200185</td>\n",
       "      <td>0.205695</td>\n",
       "      <td>0.111592</td>\n",
       "      <td>0.335999</td>\n",
       "      <td>0.093477</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.068958</td>\n",
       "      <td>15.532758</td>\n",
       "      <td>66.130635</td>\n",
       "      <td>330.040665</td>\n",
       "      <td>0.099813</td>\n",
       "      <td>0.109540</td>\n",
       "      <td>0.057583</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.195650</td>\n",
       "      <td>...</td>\n",
       "      <td>22.840293</td>\n",
       "      <td>82.133171</td>\n",
       "      <td>473.367822</td>\n",
       "      <td>0.125478</td>\n",
       "      <td>0.330466</td>\n",
       "      <td>0.283304</td>\n",
       "      <td>0.088021</td>\n",
       "      <td>0.312882</td>\n",
       "      <td>0.096158</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.271409</td>\n",
       "      <td>18.100314</td>\n",
       "      <td>78.195610</td>\n",
       "      <td>421.537832</td>\n",
       "      <td>0.105147</td>\n",
       "      <td>0.095315</td>\n",
       "      <td>0.043317</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>0.188801</td>\n",
       "      <td>...</td>\n",
       "      <td>26.365608</td>\n",
       "      <td>84.598334</td>\n",
       "      <td>620.586067</td>\n",
       "      <td>0.146766</td>\n",
       "      <td>0.118707</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>0.291805</td>\n",
       "      <td>0.069556</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.531733</td>\n",
       "      <td>18.452486</td>\n",
       "      <td>67.227069</td>\n",
       "      <td>340.063033</td>\n",
       "      <td>0.086041</td>\n",
       "      <td>0.049961</td>\n",
       "      <td>0.049709</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>0.203093</td>\n",
       "      <td>...</td>\n",
       "      <td>24.385385</td>\n",
       "      <td>73.296855</td>\n",
       "      <td>429.675600</td>\n",
       "      <td>0.100060</td>\n",
       "      <td>0.143683</td>\n",
       "      <td>0.177225</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.287749</td>\n",
       "      <td>0.073174</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12.367686</td>\n",
       "      <td>14.399191</td>\n",
       "      <td>80.643814</td>\n",
       "      <td>460.849710</td>\n",
       "      <td>0.106410</td>\n",
       "      <td>0.101420</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>0.195326</td>\n",
       "      <td>...</td>\n",
       "      <td>19.614305</td>\n",
       "      <td>89.910502</td>\n",
       "      <td>472.323112</td>\n",
       "      <td>0.138135</td>\n",
       "      <td>0.276127</td>\n",
       "      <td>0.151098</td>\n",
       "      <td>0.074396</td>\n",
       "      <td>0.345258</td>\n",
       "      <td>0.095830</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>2561</td>\n",
       "      <td>14.383173</td>\n",
       "      <td>19.454910</td>\n",
       "      <td>100.495126</td>\n",
       "      <td>637.413375</td>\n",
       "      <td>0.076174</td>\n",
       "      <td>0.224136</td>\n",
       "      <td>0.305786</td>\n",
       "      <td>0.073760</td>\n",
       "      <td>0.168884</td>\n",
       "      <td>...</td>\n",
       "      <td>23.016513</td>\n",
       "      <td>108.867289</td>\n",
       "      <td>731.638144</td>\n",
       "      <td>0.079317</td>\n",
       "      <td>0.410666</td>\n",
       "      <td>0.674672</td>\n",
       "      <td>0.146962</td>\n",
       "      <td>0.241495</td>\n",
       "      <td>0.106978</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>2562</td>\n",
       "      <td>10.018359</td>\n",
       "      <td>18.661516</td>\n",
       "      <td>61.848450</td>\n",
       "      <td>291.512307</td>\n",
       "      <td>0.083671</td>\n",
       "      <td>0.048121</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>0.007389</td>\n",
       "      <td>0.164375</td>\n",
       "      <td>...</td>\n",
       "      <td>24.919767</td>\n",
       "      <td>67.920361</td>\n",
       "      <td>374.629250</td>\n",
       "      <td>0.129882</td>\n",
       "      <td>0.081497</td>\n",
       "      <td>0.109356</td>\n",
       "      <td>0.028243</td>\n",
       "      <td>0.252432</td>\n",
       "      <td>0.081462</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>2563</td>\n",
       "      <td>11.168036</td>\n",
       "      <td>12.651203</td>\n",
       "      <td>67.102303</td>\n",
       "      <td>376.640056</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.050181</td>\n",
       "      <td>0.017739</td>\n",
       "      <td>0.022895</td>\n",
       "      <td>0.183905</td>\n",
       "      <td>...</td>\n",
       "      <td>17.412221</td>\n",
       "      <td>73.221040</td>\n",
       "      <td>421.681446</td>\n",
       "      <td>0.135757</td>\n",
       "      <td>0.089036</td>\n",
       "      <td>0.070456</td>\n",
       "      <td>0.039851</td>\n",
       "      <td>0.295527</td>\n",
       "      <td>0.070584</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>2564</td>\n",
       "      <td>11.719646</td>\n",
       "      <td>18.933783</td>\n",
       "      <td>78.556817</td>\n",
       "      <td>426.631397</td>\n",
       "      <td>0.113492</td>\n",
       "      <td>0.091756</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>0.039025</td>\n",
       "      <td>0.203736</td>\n",
       "      <td>...</td>\n",
       "      <td>26.677691</td>\n",
       "      <td>86.572055</td>\n",
       "      <td>539.508865</td>\n",
       "      <td>0.132254</td>\n",
       "      <td>0.194688</td>\n",
       "      <td>0.179610</td>\n",
       "      <td>0.071053</td>\n",
       "      <td>0.330223</td>\n",
       "      <td>0.079510</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>2565</td>\n",
       "      <td>11.844247</td>\n",
       "      <td>20.689350</td>\n",
       "      <td>74.057823</td>\n",
       "      <td>391.547178</td>\n",
       "      <td>0.110264</td>\n",
       "      <td>0.090631</td>\n",
       "      <td>0.049565</td>\n",
       "      <td>0.019809</td>\n",
       "      <td>0.193889</td>\n",
       "      <td>...</td>\n",
       "      <td>31.513549</td>\n",
       "      <td>82.313581</td>\n",
       "      <td>468.522601</td>\n",
       "      <td>0.163906</td>\n",
       "      <td>0.168648</td>\n",
       "      <td>0.143961</td>\n",
       "      <td>0.073360</td>\n",
       "      <td>0.292210</td>\n",
       "      <td>0.091949</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  n1_radius  n1_texture  n1_perimeter     n1_area  \\\n",
       "0              0  10.855170   15.248290     69.167041  359.534878   \n",
       "1              1  10.068958   15.532758     66.130635  330.040665   \n",
       "2              2  12.271409   18.100314     78.195610  421.537832   \n",
       "3              3  10.531733   18.452486     67.227069  340.063033   \n",
       "4              4  12.367686   14.399191     80.643814  460.849710   \n",
       "...          ...        ...         ...           ...         ...   \n",
       "2561        2561  14.383173   19.454910    100.495126  637.413375   \n",
       "2562        2562  10.018359   18.661516     61.848450  291.512307   \n",
       "2563        2563  11.168036   12.651203     67.102303  376.640056   \n",
       "2564        2564  11.719646   18.933783     78.556817  426.631397   \n",
       "2565        2565  11.844247   20.689350     74.057823  391.547178   \n",
       "\n",
       "      n1_smoothness  n1_compactness  n1_concavity  n1_concave_points  \\\n",
       "0          0.105488        0.080200      0.066410           0.034194   \n",
       "1          0.099813        0.109540      0.057583           0.023322   \n",
       "2          0.105147        0.095315      0.043317           0.031539   \n",
       "3          0.086041        0.049961      0.049709           0.011046   \n",
       "4          0.106410        0.101420      0.020806           0.021990   \n",
       "...             ...             ...           ...                ...   \n",
       "2561       0.076174        0.224136      0.305786           0.073760   \n",
       "2562       0.083671        0.048121      0.028997           0.007389   \n",
       "2563       0.096154        0.050181      0.017739           0.022895   \n",
       "2564       0.113492        0.091756      0.070046           0.039025   \n",
       "2565       0.110264        0.090631      0.049565           0.019809   \n",
       "\n",
       "      n1_symmetry  ...  n3_texture  n3_perimeter     n3_area  n3_smoothness  \\\n",
       "0        0.182796  ...   18.984557     81.443134  466.879302       0.149080   \n",
       "1        0.195650  ...   22.840293     82.133171  473.367822       0.125478   \n",
       "2        0.188801  ...   26.365608     84.598334  620.586067       0.146766   \n",
       "3        0.203093  ...   24.385385     73.296855  429.675600       0.100060   \n",
       "4        0.195326  ...   19.614305     89.910502  472.323112       0.138135   \n",
       "...           ...  ...         ...           ...         ...            ...   \n",
       "2561     0.168884  ...   23.016513    108.867289  731.638144       0.079317   \n",
       "2562     0.164375  ...   24.919767     67.920361  374.629250       0.129882   \n",
       "2563     0.183905  ...   17.412221     73.221040  421.681446       0.135757   \n",
       "2564     0.203736  ...   26.677691     86.572055  539.508865       0.132254   \n",
       "2565     0.193889  ...   31.513549     82.313581  468.522601       0.163906   \n",
       "\n",
       "      n3_compactness  n3_concavity  n3_concave_points  n3_symmetry  \\\n",
       "0           0.200185      0.205695           0.111592     0.335999   \n",
       "1           0.330466      0.283304           0.088021     0.312882   \n",
       "2           0.118707      0.147900           0.050402     0.291805   \n",
       "3           0.143683      0.177225           0.028111     0.287749   \n",
       "4           0.276127      0.151098           0.074396     0.345258   \n",
       "...              ...           ...                ...          ...   \n",
       "2561        0.410666      0.674672           0.146962     0.241495   \n",
       "2562        0.081497      0.109356           0.028243     0.252432   \n",
       "2563        0.089036      0.070456           0.039851     0.295527   \n",
       "2564        0.194688      0.179610           0.071053     0.330223   \n",
       "2565        0.168648      0.143961           0.073360     0.292210   \n",
       "\n",
       "      n3_fractal_dimension  DIAGNOSIS  \n",
       "0                 0.093477          B  \n",
       "1                 0.096158          B  \n",
       "2                 0.069556          B  \n",
       "3                 0.073174          B  \n",
       "4                 0.095830          B  \n",
       "...                    ...        ...  \n",
       "2561              0.106978          B  \n",
       "2562              0.081462          B  \n",
       "2563              0.070584          B  \n",
       "2564              0.079510          B  \n",
       "2565              0.091949          B  \n",
       "\n",
       "[2566 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tumour_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a function that transforms the diagnosis result from 'B' and 'M' to 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_diagnosis_lg(y):\n",
    "    #taked a vector of diagnosis and return a vector long the same containing +1 instead of 'M' and 0 instead of 'B'\n",
    "    N = len(y)\n",
    "    y_tr = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        if y[i] == 'M':\n",
    "            y_tr[i]+=1\n",
    "        elif y[i] == 'B':\n",
    "            y_tr[i]+=0\n",
    "    return y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the diagnosis from 'M's and 'B's to +-1\n",
    "\n",
    "y_tumour_samples_tr = transform_diagnosis_lg(y_tumour_samples).astype(int)\n",
    "y_tumour_test_tr = transform_diagnosis_lg(y_tumour_test).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use logistic regression to classificate. In particular we will compute the probabulity that a certain 'sample/diagnosis/element' is =1  ( we will do that using: \n",
    "$$\n",
    "P(y=1) = \\frac{1}{1 + e^{-\\beta^T  x \\ + \\beta_0}}\n",
    "$$\n",
    "where $ \\beta$, $\\beta_0$  are parameters that we will find  minimising the cost/loss function), and we will use a threshold $=0.5$. This means that if the probability that we calculated is $\\le 0.5$, then we assign 0, otherwise we predict 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):                 #logistic function\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def predict_log(X, beta, beta_0):\n",
    "    y_log = logistic(X.T @ beta + beta_0)  \n",
    "    return y_log.squeeze()\n",
    "\n",
    "\n",
    "def initialise(d):\n",
    "    \"\"\"    \n",
    "    Argument:\n",
    "    d: size of the beta vector (or number of parameters)\n",
    "  \n",
    "    Returns:\n",
    "    beta: initialised vector of shape (d, 1)\n",
    "    beta_0: initialised scalar (corresponds to the offset)\n",
    "    \"\"\"\n",
    "  \n",
    "    beta = np.zeros(shape=(d, 1), dtype=np.float32)\n",
    "    beta_0 = 0\n",
    "  \n",
    "    assert(beta.shape==(d, 1))\n",
    "    assert(isinstance(beta_0, float) or isinstance(beta_0, int))\n",
    "  \n",
    "    return beta, beta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(X, y, beta, beta_0):\n",
    "    \"\"\"\n",
    "  Arguments:\n",
    "  X: data of size (d, n)\n",
    "  y: true label vector of size (1, n)\n",
    "  beta: parameters, a numpy array of size (d, 1)\n",
    "  beta_0: offset, a scalar\n",
    "\n",
    "  Returns:\n",
    "  cost: negative log-likelihood cost for logistic regression\n",
    "  dbeta: gradient of the loss with respect to beta\n",
    "  dbeta_0: gradient of the loss with respect to beta_0\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    y_log = predict_log(X, beta, beta_0)\n",
    "\n",
    "    # cost function\n",
    "    cost = - (y * np.log(y_log) + (1-y) * np.log(1 - y_log)).mean()\n",
    "\n",
    "    # derivatives\n",
    "    dbeta = (X * (y_log - y)).mean(axis=1).reshape(-1, 1) \n",
    "    dbeta_0 =  (y_log - y).mean() \n",
    "\n",
    "    assert(dbeta.shape==beta.shape)\n",
    "    assert(dbeta_0.dtype==float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape==())\n",
    "  \n",
    "    # store gradients in a dictionary\n",
    "    grads = {\"dbeta\": dbeta, \"dbeta_0\": dbeta_0}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the cost function $$\n",
    "\\mathcal L = - \\frac{1}{n} \\sum_{i=1}^n y^{(i)} \\log(\\hat{y}_{\\text{log}}^{(i)}) + (1-y^{(i)}) \\log (1-\\hat{y}_{\\text{log}}^{(i)}) \\, .\n",
    "$$\n",
    "So in this way we will use gradiend descend to find the optimal values of $\\beta$ and $\\beta_0$. In particular we will use 5000 iterations and a learning rate of 0.005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise(X, y, beta, beta_0, num_iterations=5000, learning_rate=0.005, print_cost=False):\n",
    "    \"\"\"\n",
    "  Arguments:\n",
    "  X: data of size (d, n)\n",
    "  y: true label vector of size (1, n)\n",
    "  beta: parameters, a numpy array of size (d, 1)\n",
    "  beta_0: offset, a scalar\n",
    "  num_iterations: number of iterations gradient descent shall update the parameters\n",
    "  learning_rate: step size in updating procedure\n",
    "  print_cost: whether to print the cost every 100 iterations or not\n",
    "\n",
    "  Returns:\n",
    "  params: dictionary containing the parameters beta and offset beta_0\n",
    "  grads: dictionary containing the gradients\n",
    "  costs: list of all the costs computed during the optimisation (can be used to plot the learning curve).\n",
    "  \"\"\"\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # calculate cost and gradients using the function 'propagate'\n",
    "        grads, cost = propagate(X, y, beta, beta_0)\n",
    "      \n",
    "        # retrieve derivatives from grads dictionary\n",
    "        dbeta = grads[\"dbeta\"]\n",
    "        dbeta_0 = grads[\"dbeta_0\"]\n",
    "\n",
    "        # update parameters\n",
    "        beta = beta -learning_rate*dbeta\n",
    "        beta_0 = beta_0 - learning_rate*dbeta_0\n",
    "\n",
    "        # record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        # print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"cost after iteration %i: %f\" %(i, cost))\n",
    "  \n",
    "    # save parameters and the gradients in dictionaries\n",
    "    params = {\"beta\": beta, \"beta_0\": beta_0}\n",
    "    grads = {\"dbeta\": dbeta, \"dbeta_0\": dbeta_0}\n",
    "  \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is the one used to predict. Notice that we use $\\texttt{.round()}$, instead of the threshold comparison, since the threshold is 0.5, and so these two things are equivalent. In fact $\\texttt{.round()}$ assigns 1 for values $>0.5$ and $0$ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, beta, beta_0, return_y_log = False):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  X_test: test data of size (d, n)\n",
    "  beta: parameters, a numpy array of size (d, 1)\n",
    "  beta_0: offset, a scalar\n",
    "\n",
    "  Returns:\n",
    "  y_pred: vector containing all binary predictions (0/1) for the examples in X_test\n",
    "  \"\"\"\n",
    "  n = X_test.shape[1]\n",
    "  y_pred = np.zeros((1,n))\n",
    "  beta = beta.reshape(X_test.shape[0], 1)\n",
    "  \n",
    "  # compute vector y_log predicting the probabilities\n",
    "  y_log = predict_log(X_test, beta, beta_0)\n",
    "  \n",
    "  y_pred = y_log.round().reshape(1, -1) \n",
    "  \n",
    "  assert(y_pred.shape==(1, n))\n",
    "  \n",
    "  if return_y_log == False:\n",
    "    return y_pred\n",
    "  else:\n",
    "        return y_pred, y_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our final function. In it we define also the accuracy cost, which relies basically on the number of accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005, print_cost=False):\n",
    "  # initialize parameters with zeros\n",
    "  beta, beta_0 = initialise(X_train.shape[0])\n",
    "\n",
    "  # gradient descent\n",
    "  parameters, grads, costs = optimise(X_train, y_train, beta, beta_0, num_iterations, learning_rate, print_cost=print_cost)\n",
    "\n",
    "  # retrieve parameters beta and beta_0 from dictionary \"parameters\"\n",
    "  beta = parameters[\"beta\"]\n",
    "  beta_0 = parameters[\"beta_0\"]\n",
    "\n",
    "  # predict test and train set examples\n",
    "  y_pred_test = predict(X_test, beta, beta_0)\n",
    "  y_pred_train = predict(X_train, beta, beta_0)\n",
    "\n",
    "  # print train/test Errors\n",
    "  print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_pred_train - y_train)) * 100))\n",
    "  print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))\n",
    "\n",
    "  # saving all information\n",
    "  d = {\"costs\": costs, \"y_pred_test\": y_pred_test, \"y_pred_train\": y_pred_train, \"beta\": beta, \"beta_0\": beta_0, \"learning_rate\": learning_rate, \"num_iterations\": num_iterations}\n",
    "  \n",
    "  return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we apply the logistic regression to our problem and get the following accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 94.1153546375682 %\n",
      "test accuracy: 82.5 %\n"
     ]
    }
   ],
   "source": [
    "d = model(X_tumour_samples_lg, y_tumour_samples_tr, X_tumour_test_lg,y_tumour_test_tr, num_iterations=5000, learning_rate=0.005, print_cost=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to apply bagging to the logistic regression classifier. So we will take $R$ random samples with $N'<N$ elements from the data set. We will apply logistic regression to all of them, obtaining so R probability vectors. We average them all and at the end compare the average vector with the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we modify our function 'model' defined in the previous exercise 3.1.1. This function train the logistic model and returns the probability vector for the testing samples. (So a probability vector which gives for each element in the testing sample the probability of being diagnosed 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_modified(X_train, y_train, X_test, y_test, num_iterations=2000, learning_rate=0.005, print_cost=False):\n",
    "    \n",
    "  \"\"\"This function returns the 'not rounded prediction', so before the comparison with the threshold\"\"\"\n",
    "  # initialize parameters with zeros\n",
    "  beta, beta_0 = initialise(X_train.shape[0])\n",
    "\n",
    "  # gradient descent\n",
    "  parameters, grads, costs = optimise(X_train, y_train, beta, beta_0, num_iterations, learning_rate, print_cost=print_cost)\n",
    "\n",
    "  # retrieve parameters beta and beta_0 from dictionary \"parameters\"\n",
    "  beta = parameters[\"beta\"]\n",
    "  beta_0 = parameters[\"beta_0\"]\n",
    "\n",
    "  # predict test and train set examples\n",
    "  y_pred_test, y_log_test = predict(X_test, beta, beta_0,return_y_log = True )\n",
    "  y_pred_train = predict(X_train, beta, beta_0)\n",
    "  \n",
    "  return y_log_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function applies bagging, generating R random sample (with 500 elements taken from the training dataset), trains the logistic model with all of them, calculates the average probability vector for the prediction and compares with a threshold equal to 0.5 (using the function $\\texttt{round()}$. Finally it calculates the accuracy on the testing dataset given as an input.\n",
    "This function relies in part to the functions given in the previous exercise and works with 5000 iterations and a learning rate of 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_logistic(R, X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005, print_cost=False):\n",
    "    \"\"\"R: number of samples we will use for bagging\n",
    "    The other inputs are the same of the previous function\n",
    "    \n",
    "    Output: the average accuracy\"\"\"\n",
    "    \n",
    "    # I initialize my prediction vector for the testing set\n",
    "    y_pred = np.zeros(len(y_test))\n",
    "    \n",
    "    N = X_train.shape[1] #it's going to me the max bound in random.randint\n",
    "    \n",
    "    # I start a loop long R, where R is the number of the samples I am considering\n",
    "    for j in range (R):\n",
    "        k_j = random.randint(0,N,500)    #I choose the indexes in order to collect random samples\n",
    "        \n",
    "        # I calculate y_log using logistic regression (I don't use the threshold yet to obtain the final prediction)\n",
    "        y_log_test = model_modified(X_train[:,k_j], y_train[k_j], X_test, y_test, num_iterations=5000, learning_rate=0.005, print_cost=False)\n",
    "        \n",
    "        #I add my resulting array to the prediction array\n",
    "        y_pred += y_log_test\n",
    "    #I calculate the average\n",
    "    y_pred = y_pred/R     #This is the average vector I get using logistic regression on R random samples\n",
    "    \n",
    "    #NOw I use a threshold equal to 0.5 (I can do that just rounding the element of my vector, such that if they are <=0.5, they are rounded to 0 and viceversa they are rounded to 1 when ther are >0.5)\n",
    "    \n",
    "    y_pred = y_pred.round().reshape(1, -1)\n",
    "    \n",
    "    #Now I can calculate the accuracy on the testing set\n",
    "    test_accuracy = 100 - np.mean(np.abs(y_pred - y_test)) * 100\n",
    "    \n",
    "    return  test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_indexes_tumour_samples = np.split(np.arange(len(y_tumour_samples)-1), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally implement 5 folds cross validation (as in the previous results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score_logistic_bagging(X_train, y_train, folds,number_of_samples):\n",
    "    #performs 5 cross validation and returns the average accuracy\n",
    "  scores = []\n",
    "  for i in range(len(folds)):\n",
    "    val_indexes = folds[i]\n",
    "    train_indexes = list(set(range(y_train.shape[0]-1)) - set(val_indexes))\n",
    "    \n",
    "    X_train_i = X_train[train_indexes, :]\n",
    "    y_train_i = y_train[train_indexes]\n",
    "\n",
    "\n",
    "    X_val_i = X_train[val_indexes, :] \n",
    "    y_val_i = y_train[val_indexes] \n",
    "    \n",
    "    score_lb = bagging_logistic(number_of_samples,X_train_i.T, y_train_i, X_val_i.T,y_val_i, num_iterations=5000, learning_rate=0.005, print_cost=False)\n",
    "\n",
    "    scores.append(score_lb)\n",
    "\n",
    "  # Return the average score\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_number_log_bg(X_train, y_train, folds, numb_range):   #This function is strongly inspired by the weekly notebooks of this module\n",
    "  numb_scores = np.zeros((len(numb_range),))\n",
    "  \n",
    "  for i, numb in enumerate(numb_range):\n",
    "    numb_scores[i] = cross_validation_score_logistic_bagging(X_train, y_train, folds, numb)  #numb is the number of samples\n",
    "    print(f'Hyperparameter number of samples={numb}: {numb_scores[i]:f}')\n",
    "\n",
    "  best_numb_index = np.argmax(numb_scores)  #I choose the one with greater accusacy\n",
    "  return numb_range[best_numb_index]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the following numbers of sets to use in the cross validation. Being the research of an hyperparameter a never ending research and being this algorithm quite slow we will limite ourselves to only 6 possible parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 15, 22, 29, 36, 43])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numb_range = np.arange(8,50,7)\n",
    "numb_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter number of samples=8: 94.074074\n",
      "Hyperparameter number of samples=15: 94.152047\n",
      "Hyperparameter number of samples=22: 94.113060\n",
      "Hyperparameter number of samples=29: 94.152047\n",
      "Hyperparameter number of samples=36: 94.113060\n",
      "Hyperparameter number of samples=43: 94.074074\n",
      "best number of samples: 15\n"
     ]
    }
   ],
   "source": [
    "best_numb = choose_best_number_log_bg(X_tumour_samples.astype(float), y_tumour_samples_tr, folds_indexes_tumour_samples, numb_range)\n",
    "print('best number of samples:', best_numb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our optimal parameter is 15, so the number of samples we will use for the bagging is 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{bagging_logistic}$ returns the accuracy (we still use 0.5 as a threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.5"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_15_ = bagging_logistic(15, X_tumour_samples_lg, y_tumour_samples_tr, X_tumour_test_lg, y_tumour_test_tr, num_iterations=5000, learning_rate=0.005, print_cost=False)\n",
    "precision_15_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final accuracy on the testing data set is $83.5\\%$, which is slightly better than the one we got with the logistic regression without bagging. However we don't see an heavy improvement, which doesn't give us any reason to use bagging with logistic regression (which is slower and has higher computational costs) instead of classic logistic regression. \n",
    "\n",
    "(I may guess (but I am not sure about this ) that this may be the reason bagging is more common with decision trees rather than with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude with some little comments:\n",
    "1. We choose as cardinality N' of the random sub samples 500, but we could take different N' and do a research for the optimal N' parameter.\n",
    "2. The component of randomness in the choice of the elements of each sample could lead to slightly different results in the accuracy, but they can ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XbuLm6qy100"
   },
   "source": [
    "<a name=\"q32\"></a>\n",
    "\n",
    "## 3.2 [^](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize \n",
    "\n",
    "$min_{w_z}||w_z||^2+\\frac{\\lambda}{n}\\sum_i max\\{0,1-(z^{(i)}\\cdot w_z+b)y^{(i)}$. We take the kernel $k(x,y)=e^{-\\frac{||x-y||^2}{\\sigma}}$. So $||w_z||^2 = w_z\\cdot w_x = k(w,w) = 1$, which means that we can just focus on minimizing $\\sum_i max\\{0,1-(z^{(i)}\\cdot w_z+b)y^{(i)}=\\sum_i max\\{0,1-(k(x^{(i)},w)+b)y^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set\n",
    "Tumour_samples = df_tumour_samples.to_numpy()\n",
    "y_tumour_samples = Tumour_samples[:,31] # creating the array with the classifications (B or M)\n",
    "N_tumour_samples = len(y_tumour_samples)\n",
    "X_tumour_samples = Tumour_samples[:,1:31] #creating a matrix storing the input data\n",
    "\n",
    "\n",
    "#We standardise\n",
    "X_tumour_samples_3 = (X_tumour_samples-np.mean(X_tumour_samples))/np.std(X_tumour_samples)\n",
    "\n",
    "#We augment\n",
    "X_tumour_samples_kern = np.hstack([X_tumour_samples_3,1.0 * np.ones((len(y_tumour_samples),1))])\n",
    "\n",
    "#Testing set\n",
    "Tumour_test = df_tumour_test.to_numpy()\n",
    "y_tumour_test = Tumour_test[:,31] # creating the array with the classifications (B or M)\n",
    "N_tumour_test = len(y_tumour_test)\n",
    "X_tumour_test = Tumour_test[:,1:31] #creating a matrix storing the input data\n",
    "\n",
    "\n",
    "#We standardise\n",
    "X_tumour_test_3 = (X_tumour_test-np.mean(X_tumour_test))/np.std(X_tumour_test)\n",
    "\n",
    "\n",
    "#We augment\n",
    "X_tumour_test_kern = np.hstack([X_tumour_test_3,1.0 * np.ones((len(y_tumour_test),1))])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_kern(w,sigma, X, y):\n",
    "    n = X.shape[0]\n",
    "    distances = []\n",
    "    for i in range(n):\n",
    "        distance = 1 - y[i]*(np.exp(-np.linalg.norm(X[i,:]-w)**2/sigma))\n",
    "        if distance <0:\n",
    "            distances.append(0)\n",
    "        else:\n",
    "            distances.append(distance)\n",
    "    hinge = np.sum(distances)   #the sum is our new cost function\n",
    "\n",
    "    # calculate cost\n",
    "    return hinge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the gradient of the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient_kern(w, X_batch, y_batch, sigma):\n",
    "    # if only one example is passed\n",
    "    if type(y_batch) == np.float64:\n",
    "        y_batch = np.asarray([y_batch])\n",
    "        X_batch = np.asarray([X_batch])  # gives multidimensional array\n",
    "    \n",
    "    dw = np.zeros(len(w), dtype = float)\n",
    "    for i in range (X_batch.shape[0]):\n",
    "        if max(0, 1 - y_batch[i]*(np.exp(-np.linalg.norm(X_batch[i,:]-w)**2/sigma)))==0:\n",
    "            dw += 0\n",
    "        else:\n",
    "            for j in range(len(w)):\n",
    "                dw[j] +=  -(2*y_batch[i] *(np.exp(-np.linalg.norm(X_batch[i,:]-w)**2/sigma))*((X_batch[i,j]-w[j])))/sigma\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_kern(X, y,sigma, batch_size=16, max_iterations=50000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=1e5, print_outcome=False):\n",
    "    # initialise zero weights\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    nth = 0\n",
    "    # initialise starting cost as infinity\n",
    "    prev_cost = np.inf\n",
    "    \n",
    "    # stochastic gradient descent\n",
    "    indices = np.arange(len(y))\n",
    "    for iteration in range(1, max_iterations):\n",
    "        # shuffle to prevent repeating update cycles\n",
    "        np.random.shuffle(indices)\n",
    "        batch_idx = indices[:batch_size]\n",
    "        X_b, y_b = X[batch_idx], y[batch_idx]\n",
    "        for xi, yi in zip(X_b, y_b):\n",
    "            ascent = calculate_cost_gradient_kern(weights, xi, yi, sigma) \n",
    "            weights = weights - (learning_rate * ascent)\n",
    "\n",
    "        # convergence check on 2^n'th iteration\n",
    "        if iteration==2**nth or iteration==max_iterations-1:\n",
    "            # compute cost\n",
    "            cost = compute_cost_kern(weights,sigma, X, y)\n",
    "            if print_outcome:\n",
    "                print(\"Iteration is: {}, Cost is: {}\".format(iteration, cost))\n",
    "            # stop criterion\n",
    "            \n",
    "              \n",
    "            prev_cost = cost\n",
    "            nth += 1\n",
    "    \n",
    "    return weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_diagnosis_kern(y):\n",
    "    #taked a vector of diagnosis and return a vector long the same containing +1 instead of 'M' and -1 instead of 'B'\n",
    "    N = len(y)\n",
    "    y_tr = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        if y[i] == 'M':\n",
    "            y_tr[i]+=1.0\n",
    "        elif y[i] == 'B':\n",
    "            y_tr[i]-=1.0\n",
    "    return y_tr\n",
    "\n",
    "\n",
    "\n",
    "def score_kern(w, X, y):\n",
    "    y_preds = np.sign(X @ w)\n",
    "    return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tumour_samples_kern = transform_diagnosis_kern( y_tumour_samples)\n",
    "y_tumour_test_kern = transform_diagnosis_kern( y_tumour_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1, Cost is: 2567.171158292329\n",
      "Iteration is: 2, Cost is: 2567.1711307279625\n",
      "Iteration is: 4, Cost is: 2567.1710978952906\n",
      "Iteration is: 8, Cost is: 2567.1710287898754\n",
      "Iteration is: 16, Cost is: 2567.1708604713986\n",
      "Iteration is: 32, Cost is: 2567.1705358441486\n",
      "Iteration is: 64, Cost is: 2567.1699341389067\n",
      "Iteration is: 128, Cost is: 2567.168639255805\n",
      "Iteration is: 256, Cost is: 2567.166096412904\n",
      "Iteration is: 512, Cost is: 2567.1609034436233\n",
      "Iteration is: 1024, Cost is: 2567.15067108759\n",
      "Iteration is: 2048, Cost is: 2567.130920674679\n",
      "Iteration is: 4096, Cost is: 2567.092852220184\n",
      "Iteration is: 8192, Cost is: 2567.0231354624966\n",
      "Iteration is: 9999, Cost is: 2566.994671307575\n"
     ]
    }
   ],
   "source": [
    "w_kern = sgd_kern(X_tumour_samples_kern, y_tumour_samples_kern,1, 200, 10000, 0.001, 1e-5, regul_strength=2, print_outcome=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00395797  0.00304685 -0.0001982  -0.01438712  0.00473178  0.00473287\n",
      "  0.00473664  0.00473964  0.00472336  0.00473407  0.00471475  0.00460263\n",
      "  0.00456057  0.00337331  0.00473974  0.00473825  0.00473761  0.00474008\n",
      "  0.00473828  0.0047403   0.0038742   0.00256184 -0.00080267 -0.01873447\n",
      "  0.0047278   0.0047246   0.0047267   0.00473716  0.00471415  0.00473218\n",
      " -0.0169119 ]\n"
     ]
    }
   ],
   "source": [
    "print(w_kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8347622759158223"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_kern(w_kern, X_tumour_samples_kern, y_tumour_samples_kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.635"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_kern(w_kern, X_tumour_test_kern, y_tumour_test_kern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise has not been finished, but the way to proceed is just to implement cross validation as in the previous exercises, to find the optimal parameter $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SurnameCID_CW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
